Ultimatix Knowmax learning:
In Knowmax -> Organization units -> Banking and Technology Services -> Services -> Google Business Unit -> GSkool option, there learning materials.

O'reilly learning (Access from ultimatix):
https://learning.oreilly.com/library/view/official-google-cloud/9781119564416/

LinkedIn links for tutorials:
https://www.linkedin.com/learning/google-cloud-platform-for-enterprise-essential-training/enterprise-ready-gcp?u=2154233
https://www.linkedin.com/learning/google-cloud-platform-essential-training-3/hosting-your-application-on-google-cloud-platform?u=2154233

Github repos:
https://github.com/lynnlangit/gcp-essentials
https://github.com/lynnlangit/gcp-ml

https://cloud.google.com/pricing

https://medium.com/google-cloud/gcpcomics/home

https://cloud.google.com/docs

https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations

https://cloud.google.com/terms/services

https://developers.google.com/community/experts/directory

https://cloud.google.com/identity/docs/overview

https://cloud.google.com/resource-manager/docs/cloud-platform-resource-hierarchy

https://cloud.google.com/iam/docs/resource-hierarchy-access-control
https://cloud.google.com/iam/docs/job-functions/networking

https://cloud.google.com/billing/docs/concepts

https://cloud.google.com/files/Lift-and-Shift-onto-Google-Cloud.pdf
https://cloud.google.com/solutions/application-migration
https://cloud.google.com/solutions/migrating-vms-migrate-for-compute-engine-getting-started

https://cloud.google.com/compute/docs/images/image-management-best-practices

https://cloud.google.com/migrate/compute-engine/docs/4.11/concepts/architecture/gcp-reference-architecture

https://cloud.google.com/anthos/docs/concepts/overview
https://cloud.google.com/anthos/docs/tutorials/explore-anthos

https://cloud.google.com/kuberun/docs/setup

https://cloud.google.com/solutions/dr-scenarios-planning-guide

https://cloud.google.com/secret-manager/docs/overview
https://cloud.google.com/solutions/dr-scenarios-for-data
https://cloud.google.com/solutions/bigquery-data-warehouse
https://cloud.google.com/billing/docs/how-to/visualize-data
https://cloud.google.com/bigquery-ml/docs/introduction

https://cloud.google.com/data-fusion/docs

google Cloud dataflow uses apache beam and cloud dataproc uses apache hadoop/spark and cloud composer uses apache airflow.

https://cloud.google.com/solutions/build-a-data-lake-on-gcp

https://airflow.apache.org/docs/apache-airflow/stable/ui.html

https://cloud.google.com/life-sciences/docs/how-tos/variant-transforms
https://cloud.google.com/life-sciences/docs/quickstart
https://cloud.google.com/life-sciences/docs/resources/public-datasets
https://ai.googleblog.com/2017/12/deepvariant-highly-accurate-genomes.html

https://console.cloud.google.com/marketplace
https://cloud.google.com/foundation-toolkit

https://cloud.google.com/build/docs/overview

Difference between serverless and containers: https://www.cloudflare.com/learning/serverless/serverless-vs-containers/

https://codelabs.developers.google.com/codelabs/cloud-builder-gke-continuous-deploy/index.html#0
https://cloud.google.com/build/docs/cloud-builders

https://cloud.google.com/compute/quotas

https://cloud.google.com/products/tools

https://github.com/googlecloudplatform

https://cloud.google.com/sdk/docs/quickstart

https://cloud.google.com/docs/compare/aws

https://cloud.google.com/storage/docs/storage-classes

https://cloud.google.com/datastore/docs/firestore-or-datastore

https://datastudio.google.com/navigation/reporting

https://cloud.google.com/code

https://cloud.google.com/serverless/whitepaper

https://medium.com/google-cloud/about

https://cloud.google.com/appengine/docs/flexible
https://cloud.google.com/appengine/docs/standard

https://cloud.google.com/blog/products/bigquery/bigquery-under-the-hood

https://cloud.google.com/network-connectivity/docs/interconnect/concepts/dedicated-overview
https://cloud.google.com/network-connectivity/docs/interconnect/concepts/partner-overview

https://cloud.google.com/logging/docs/export/configure_export_v2


https://www.youtube.com/watch?v=r7ce2yqdckk
https://www.coursera.org/lecture/logging-monitoring-observability-google-cloud/exporting-and-analyzing-logs-peXBz

https://cloud.google.com/compute/docs/disks/snapshots
https://cloud.google.com/compute/docs/regions-zones/global-regional-zonal-resources#globalresources

https://cloud.google.com/appengine/docs/admin-api/migrating-splitting-traffic

Compute Engine:
https://cloud.google.com/compute/docs/instances/create-start-instance
https://cloud.google.com/compute/docs/instances/create-start-preemptible-instance#handle_preemption
Do not use regional persistent disks for boot disks. In a failover situation, they do not force-attach to a VM.
https://cloud.google.com/compute/docs/disks/detach-reattach-boot-disk
https://cloud.google.com/compute/docs/instances/windows/creating-managing-windows-instances
https://cloud.google.com/compute/docs/instances/windows/creating-passwords-for-windows-instances
https://cloud.google.com/compute/docs/instances/create-vm-from-instance-template
https://cloud.google.com/compute/docs/instances/create-vm-from-similar-instance
https://cloud.google.com/compute/docs/instances/custom-hostname-vm
https://cloud.google.com/compute/docs/instances/ssh#metadata-managed_ssh_connections
https://cloud.google.com/compute/docs/instances/connecting-advanced#provide-key
(Recommended) Enable OS Login.
(Not recommended) Manually add and remove SSH keys by editing project or instance metadata.
https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys#risks
By creating and managing SSH keys, you can let users access a Linux instance through third-party tools.
An SSH key consists of the following files:
1) A public SSH key file that is applied to instance-level metadata or project-wide metadata.
2) A private SSH key file that the user stores on their local devices.
If a user presents their private SSH key, they can use a third-party tool to connect to any instance that is configured with the matching public SSH key file,
even if they aren't a member of your Google Cloud project.
Therefore, you can control which instances a user can access by changing the public SSH key metadata for one or more instances.
https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys#project-wide
Use project-wide public SSH keys to give users general access to a Linux instance.
Project-wide public SSH keys give users access to all of the Linux instances in a project that allow project-wide public SSH keys.
If an instance blocks project-wide public SSH keys,
a user can't use their project-wide public SSH key to connect to the instance unless the same public SSH key is also added to instance metadata.
https://cloud.google.com/compute/docs/instances/managing-instance-access#connect
Grant one of the following instance access roles.
1. roles/compute.osLogin, which doesn't grant administrator permissions
2. roles/compute.osAdminLogin, which grants administrator permissions
https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys
https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys#edit-ssh-metadata
https://cloud.google.com/compute/docs/disks/add-persistent-disk
https://cloud.google.com/compute/docs/disks/working-with-persistent-disks#disk_type
You can change the type of your persistent disk using snapshots.
For example, to change your standard persistent disk to an SSD persistent disk, use the following process:
1) Create a snapshot of your standard persistent disk.
2) Create a new persistent disk based on the snapshot. Include the --type flag and specify pd-ssd.
https://cloud.google.com/compute/docs/disks/regional-persistent-disk#restrictions
Restrictions
1) You cannot use regional persistent disks as boot disks.
2) You can create a regional persistent disk from a snapshot but not an image.
https://cloud.google.com/compute/docs/disks/regional-persistent-disk#use_multi_instances
https://cloud.google.com/compute/docs/disks/regional-persistent-disk#migrate-repd
https://cloud.google.com/compute/docs/disks/create-disk-from-source
You can create persistent disks from the following data sources:
1) Existing disks: Clone an existing persistent disk. Use this option if you need an instantly attachable copy of an existing non-boot persistent disk.
2) Snapshots: Create a non-boot disk from a source snapshot. Use this option to restore data from a persistent disk that you've backed up using snapshots.
3) Images: Create a boot disk from a source image. Use this option to create a boot disk for a new VM or to create a standalone boot persistent disk.
https://cloud.google.com/compute/docs/disks/create-disk-from-source#clone
Restrictions
1) The zone, region, and disk type of the clone must be the same as that of the source disk.
2) You cannot create a zonal disk clone from a regional disk. You cannot create a regional disk clone from a zonal disk.
https://cloud.google.com/compute/docs/disks/create-snapshots
You can create snapshots from disks even while they are attached to running instances.
Snapshots are global resources, so you can use them to restore data to a new disk or instance within the same project.
You can also share snapshots across projects.
https://cloud.google.com/compute/docs/disks/create-snapshots#restore-snapshots
Compute Engine uses incremental snapshots so that each snapshot contains only the data that has changed since the previous snapshot.
https://cloud.google.com/compute/docs/disks/scheduled-snapshots
Restrictions:
1) A persistent disk can have only one snapshot schedule attached to it at a time.
2) You cannot delete a snapshot schedule if it is attached to a disk. You must detach the schedule from all disks, then delete the schedule.
https://cloud.google.com/compute/docs/disks/scheduled-snapshots#deletion_rule
https://cloud.google.com/compute/docs/disks/scheduled-snapshots#change_snapshot_schedule
https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images
Create the image
You can create disk images from the following sources:
1) A persistent disk, even while that disk is attached to a VM
2) A snapshot of a persistent disk
3) Another image in your project
https://cloud.google.com/compute/docs/instances/startup-scripts/linux
https://cloud.google.com/compute/docs/shutdownscript
Create and run shutdown scripts that execute commands right before a virtual machine (VM) instance is stopped or restarted.
Shutdown scripts have a limited amount of time to finish running before the instance stops:
1. On-demand instances: 90 seconds after you stop or delete an instance
2. Preemptible instances: 30 seconds after instance preemption begins
https://cloud.google.com/compute/docs/shutdownscript#shutdown_actions
The shutdown script won't run if the instance is reset using instances().reset.
https://cloud.google.com/compute/docs/instances/stop-start-instance
You cannot stop and restart a VM with a local SSD attached.
https://cloud.google.com/compute/docs/instances/deleting-instance
https://cloud.google.com/compute/docs/instances/update-instance-properties
https://cloud.google.com/compute/docs/instances/moving-instance-across-zones
Move your instance manually using the following steps:
1. Create snapshots of persistent disks attached to the original instance.
2. Create copies of the persistent disks in the destination zone.
3. For external and internal IP addresses:
    a. If you are moving an instance across zones within the same region, and you want to preserve its ephemeral IP address,
    temporarily promote the ephemeral IP address that is assigned to the instance to a static IP address
    and then assign it to the new VM instance you create in the destination zone.
    b. If you are moving an instance across regions, you must pick a different IP address for the VM instance.
4. Create and boot up a new instance in the destination zone. If you are moving across regions, you must also pick a new subnetwork for the new instance.
5. Attach the new persistent disks to your new instance.
6. Assign an external IP address to the new instance. If necessary, demote the address back to an ephemeral external IP address.
7. Delete the snapshots, original disks, and original instance.
https://cloud.google.com/compute/docs/instances/moving-instance-across-zones#limitations
https://cloud.google.com/compute/docs/instances/copy-vm-between-projects
https://cloud.google.com/compute/docs/instances/changing-machine-type-of-stopped-instance
https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances
A virtual machine instance can only have one service account identity.
https://cloud.google.com/compute/docs/instances/setting-instance-scheduling-options
https://cloud.google.com/compute/docs/instances/setting-instance-scheduling-options#settingoptions
https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances
You can create regional MIGs or zonal MIGs.
You must select which zones are associated with a regional MIG when you create the regional MIG.
After you choose specific zones during creation, you cannot change or update the zones later.
1.  To select more than three zones within a region, you must explicitly specify the individual zones.
    For example, to select all four zones within a region, you must provide all four zones explicitly in your request.
    If you do not, Compute Engine selects three zones by default.
2.  To select two or fewer zones in a region, you must explicitly specify the individual zones.
    Even if the region only contains two zones, you must still explicitly specify the zones in your request.
https://cloud.google.com/compute/docs/instance-groups/regional-migs#autoscaling_a_regional_mig
An autoscaling policy is applied to the group as a whole.
For example, if you enable autoscaler to target 66% CPU utilization,
the autoscaler tracks all instances in the group to maintain an average 66% utilization across all instances in all zones.
To add more instances to a MIG, you can:
1. Manually set the size of the MIG.
2. Use autoscaling for stateless applications.
https://cloud.google.com/compute/docs/instance-groups/autohealing-instances-in-migs
An autohealing policy relies on an application-based health check to verify that an application is responding as expected.
Checking that an application responds is more precise than simply verifying that a VM is in a RUNNING state.
If the autohealer determines that an application isn't responding, the MIG automatically recreates that VM.
You can set a maximum of one autohealing policy per MIG.
https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address
https://cloud.google.com/compute/docs/autoscaler/
Autoscaling only works with zonal and regional managed instance groups (MIGs). Unmanaged instance groups are not supported.
You cannot use autoscaling if your MIG has stateful configuration.
Autoscaling works independently from autohealing.
https://cloud.google.com/compute/docs/autoscaler#autoscaling_policy
https://cloud.google.com/compute/docs/autoscaler/multiple-signals#overview
https://cloud.google.com/compute/docs/autoscaler/scaling-cpu
https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#starting_a_basic_rolling_update
https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#max_surge
https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#max_unavailable
If you do not want any unavailable machines during an update, set the maxUnavailable value to 0 and the maxSurge value to greater than 0.
With these settings, Compute Engine removes each old machine only after its replacement new machine is created and running.
https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#canary_updates
https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#rolling_back_an_update
There is no explicit command for rolling back an update to a previous version, but if you decide to roll back an update
(either a fully committed update or a canary update), you can do so by making a new update request and passing in the instance template that you want to roll back to.
https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#performing_a_rolling_replace_or_restart
https://cloud.google.com/compute/docs/startupscript
probably, you can only create boot disk from an image and not a snapshot. please verify.
Difference between image and snapshot:
https://stackoverflow.com/a/29634892
https://www.quora.com/What-is-the-difference-between-an-image-and-a-snapshot-on-Google-Cloud
https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images
The VM images for Compute Engine and Amazon Elastic Compute Cloud (EC2) don't include the Logging agent,so you must complete these steps to install it on those instances.
The agent runs under both Linux and Windows.
If your VMs are running in Google Kubernetes Engine or App Engine, the agent is already included in the VM image.
https://cloud.google.com/logging/docs/agent/logging/installation


Load Balancers:
https://cloud.google.com/load-balancing/docs/choosing-load-balancer (Choosing a load balancer)
External TCP/UDP network load balancer & Internal TCP/UDP loadbalancer are passthrough.
All others are proxy-based.
Internal HTTP(S) and External TCP/UDP network load balancers are regional load balancers.
All others are global.
https://cloud.google.com/load-balancing/docs/https (External HTTP(S) load balancing)
https://cloud.google.com/load-balancing/docs/l7-internal (Internal HTTP(S) load balancing)
Only clients that are located in the same region as the load balancer can access this IP address. Internal client requests stay internal to your network and region.
https://cloud.google.com/load-balancing/docs/ssl (External SSl (TCP traffic with SSL offload) proxy load balancing)
https://cloud.google.com/load-balancing/docs/tcp (External TCP proxy load balancing)
https://cloud.google.com/load-balancing/docs/network/networklb-backend-service (External TCP/UDP network load balancers)
With the Premium Tier, a load balancer can be configured as a global load balancing service.
With Standard Tier, a load balancer handles load balancing regionally.
External IP addresses can be global or regional.
You can also use HTTP(S) Load Balancing with Cloud Storage buckets.
https://cloud.google.com/load-balancing/docs/https#load_balancing_using_multiple_backend_types
External HTTP(S) Load Balancing supports the following backend types:
1. Instance groups
2. Zonal network endpoint groups (NEGs)
3. Serverless NEGs: One or more App Engine, Cloud Run, or Cloud Functions services
4. Internet NEGs, for endpoints that are outside of Google Cloud (also known as custom origins)
5. Buckets in Cloud Storage

Cloud Monitoring:
https://cloud.google.com/monitoring/quickstart-lamp
https://cloud.google.com/monitoring/settings/multiple-projects
Using the Monitoring agent is optional but recommended.
Monitoring can access some instance metrics without the Monitoring agent, including CPU utilization, some disk traffic metrics,
network traffic, and uptime information
https://cloud.google.com/monitoring/agent/monitoring

Kubernetes:
https://cloud.google.com/kubernetes-engine/docs
https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster
1.  A single-zone cluster has a single control plane running in one zone. This control plane manages workloads on nodes running in the same zone.
2.  A multi-zonal cluster's nodes run in multiple zones, but it has only a single replica of the control plane.
    If you need higher availability for the control plane, consider creating a regional cluster instead.
    In a regional cluster, the control plane is replicated across multiple zones in a region.
For private clusters:
Internal IP addresses for nodes come from the primary IP address range of the subnet you choose for the cluster.
Pod IP addresses and Service IP addresses come from two subnet secondary IP address ranges of that same subnet.
https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-admin-overview#node_configuration
You can create one or more node pools; node pools are groups of nodes within your cluster that share a common configuration.
Your cluster must have at least one node pool, and a node pool called default is created when you create the cluster.
https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-usage-metering
GKE usage metering tracks information about the resource requests and actual resource usage of your cluster's workloads.
Currently, GKE usage metering tracks information about CPU, GPU, TPU, memory, storage, and optionally network egress.
You can differentiate resource usage using Kubernetes Namespaces, labels, or a combination of both.
Data is stored in BigQuery, where you can query it directly or export it for analysis using external tools such as Google Data Studio.
GKE usage metering is not enabled by default. You have to enable it on a particular cluster (existing or new).
https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl
If you run multiple clusters within your Google Cloud project, you need to choose which cluster kubectl talks to.
You can set a default cluster for kubectl by setting the current context in Kubernetes' kubeconfig file.
Additionally, you can run kubectl commands against a specific cluster using the --cluster flag.
https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl#cluster_endpoint
https://cloud.google.com/kubernetes-engine/docs/how-to/upgrading-a-cluster
By default, automatic upgrades are enabled for Google Kubernetes Engine (GKE) clusters and node pools.
Control planes are upgraded on a regular basis, and cannot be disabled.
However, you can apply maintenance windows and exclusions to temporarily suspend upgrades for control planes and nodes.
https://cloud.google.com/kubernetes-engine/docs/how-to/node-pools#deploy
Kubernetes uses the Deployment controller to deploy stateless applications
Kubernetes uses the StatefulSets to deploy stateful applications
https://cloud.google.com/kubernetes-engine/docs/how-to/stateless-apps#update
https://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips#cluster_sizing
Pod IP address ranges, and subnet secondary IP address ranges in general,
are accessible from on-premises networks connected with Cloud VPN or Cloud Interconnect using Cloud Routers.
When you create a VPC-native cluster, you specify a subnet in a VPC network. The cluster uses three unique subnet IP address ranges:
1. It uses the subnet's primary IP address range for all node IP addresses.
2. It uses one secondary IP address range for all Pod IP addresses.
3. It uses another secondary IP address range for all Service (cluster IP) addresses.
https://cloud.google.com/kubernetes-engine/docs/how-to/exposing-apps
A service of type LoadBalancer is an external TCP/UDP network loadbalancer.
A kubernetes ingress object creates an external HTTP(S) Load Balancer.
https://cloud.google.com/kubernetes-engine/docs/how-to/load-balance-ingress
Kubernetes RBAC(role-based access control) is a core component of Kubernetes and lets you create and grant roles (sets of permissions)
for any object or type of object within the cluster.
https://cloud.google.com/kubernetes-engine/docs/concepts/pod
By default, Pods run on nodes in the default node pool for the cluster. You can configure the node pool a Pod selects explicitly or implicitly:
1.  You can explicitly force a Pod to deploy to a specific node pool by setting a nodeSelector in the Pod manifest.
    This forces a Pod to run only on Nodes in that node pool.
2.  You can specify resource requests for the containers you run.
    The Pod will only run on nodes that satisfy the resource requests.
    For instance, if the Pod definition includes a container that requires four CPUs, the Service will not select Pods running on Nodes with two CPUs.
https://cloud.google.com/kubernetes-engine/docs/concepts/daemonset
DaemonSets attempt to adhere to a one-Pod-per-node model, either across the entire cluster or a subset of nodes.
As you add nodes to a node pool, DaemonSets automatically add Pods to the new nodes as needed.
Examples of such tasks include storage daemons like ceph, log collection daemons like fluent-bit, and node monitoring daemons like collectd.
https://cloud.google.com/kubernetes-engine/docs/concepts/configmap
ConfigMaps bind configuration files, command-line arguments, environment variables, port numbers, and other configuration artifacts to your Pods' containers
and system components at runtime.
ConfigMaps enable you to separate your configurations from your Pods and components, which helps keep your workloads portable.
This makes their configurations easier to change and manage, and prevents hardcoding configuration data to Pod specifications.
ConfigMaps are useful for storing and sharing non-sensitive, unencrypted configuration information.
To use sensitive information in your clusters, you must use Secrets.
https://cloud.google.com/kubernetes-engine/docs/concepts/volumes#types_of_volumes
https://cloud.google.com/kubernetes-engine/docs/concepts/types-of-clusters
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler
https://cloud.google.com/kubernetes-engine/docs/concepts/deployment
https://cloud.google.com/kubernetes-engine/docs/how-to/scaling-apps#autoscaling-deployments
https://cloud.google.com/kubernetes-engine/docs/concepts/service
https://kubernetes.io/docs/concepts/services-networking/service/
https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials
https://cloud.google.com/stackdriver/docs/solutions/gke
https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs
https://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine
https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools


BigQuery:
https://cloud.google.com/bigquery/docs/estimate-costs#bq
https://cloud.google.com/bigquery/docs/bq-command-line-tool#setting_default_values_for_command-line_flags
You can set default values for command-line flags by including them in the bq command-line tool's configuration file, .bigqueryrc
https://cloud.google.com/bigquery/docs/managing-jobs#bq
Jobs are actions that BigQuery runs on your behalf to load data, export data, query data, or copy data.
https://cloud.google.com/bigquery/docs/datasets-intro#dataset_limitations
https://cloud.google.com/bigquery/docs/locations#move-dataset
The BigQuery Data Transfer Service transfers (copies) data from a source to a destination dataset in BigQuery.
https://cloud.google.com/bigquery/docs/datasets#bq
https://cloud.google.com/bigquery/docs/copying-datasets
Dataset copying uses features of the BigQuery Data Transfer Service.
https://cloud.google.com/bigquery/docs/dataset-access-controls
https://cloud.google.com/bigquery/docs/dataset-access-controls#granting_access_to_a_dataset
https://cloud.google.com/bigquery/docs/listing-datasets
https://cloud.google.com/bigquery/docs/dataset-metadata#information_schema
https://cloud.google.com/bigquery/docs/updating-datasets
https://cloud.google.com/bigquery/docs/updating-datasets#table-expiration
https://cloud.google.com/bigquery/docs/managing-datasets
https://cloud.google.com/bigquery/docs/tables
https://cloud.google.com/bigquery/docs/tables#getting_table_information_using_information_schema_beta
https://cloud.google.com/bigquery/docs/table-access-controls#bq
https://cloud.google.com/bigquery/docs/managing-tables
https://cloud.google.com/bigquery/docs/exporting-data#bq
You can undelete a table within seven days of deletion, including explicit deletions and implicit deletions due to table expiration.
You cannot export table data to a local file, to Sheets, or to Drive. The only supported export location is Cloud Storage.
You cannot change the location of a dataset after it is created, but you can make a copy of the dataset.
https://cloud.google.com/bigquery/docs/partitioned-tables
By dividing a large table into smaller partitions, you can improve query performance,
and you can control costs by reducing the number of bytes read by a query.
You can get information about partitioned tables using the INFORMATION_SCHEMA.PARTITIONS view.
https://cloud.google.com/bigquery/docs/views
https://cloud.google.com/bigquery/docs/view-metadata#views_view
https://cloud.google.com/bigquery/docs/adding-labels
https://cloud.google.com/bigquery/docs/loading-data#loading_data_from_other_google_services
https://cloud.google.com/bigquery/docs/loading-data#alternatives_to_loading_data
https://cloud.google.com/bigquery/docs/batch-loading-data#data-locations
Exception: If your dataset is in the US multi-regional location, you can load data from a Cloud Storage bucket in any regional or multi-regional location.
https://cloud.google.com/bigquery/docs/query-overview#types_of_queries
After you load your data into BigQuery, you can query the data in your tables. BigQuery supports two types of queries:
1) Interactive queries
2) Batch queries
By default, BigQuery runs interactive queries, which means that the query is executed as soon as possible.
BigQuery also offers batch queries.
BigQuery queues each batch query on your behalf and starts the query as soon as idle resources are available, usually within a few minutes.
https://cloud.google.com/bigquery/docs/query-overview#query_jobs
https://cloud.google.com/bigquery/docs/running-queries#batch
https://cloud.google.com/bigquery/docs/dry-run-queries
https://cloud.google.com/bigquery/docs/custom-quotas
https://cloud.google.com/bigquery/docs/custom-quotas#example
https://cloud.google.com/bigquery/docs/monitoring#information_schema_views
https://cloud.google.com/bigquery/docs/best-practices-costs
https://cloud.google.com/bigquery/docs/best-practices-costs#preview-data
In the bq command-line tool, use the bq head command and specify the number of rows to preview.
https://cloud.google.com/bigquery/docs/best-practices-costs#limit_query_costs_by_restricting_the_number_of_bytes_billed
You can limit the number of bytes billed for a query using the maximum bytes billed setting.
When you set maximum bytes billed, the number of bytes that the query will read is estimated before the query execution.
If the number of estimated bytes is beyond the limit, then the query fails without incurring a charge.

IAM:
https://cloud.google.com/iam/docs/creating-custom-roles
https://cloud.google.com/sdk/gcloud/reference/iam
https://cloud.google.com/iam/docs/granting-changing-revoking-access
Cloud Storage IAM roles:
https://cloud.google.com/storage/docs/access-control/iam-roles#standard-roles

GCLOUD SDK AUTHORIZATION:
https://cloud.google.com/sdk/docs/authorizing
https://cloud.google.com/sdk/docs/initializing
https://cloud.google.com/sdk/docs/configurations

Google cloud storage:
https://cloud.google.com/storage/docs/creating-buckets
https://cloud.google.com/storage/docs/storage-classes#comparison_of_storage_classes
You can change the storage class of an existing object either by rewriting the object or by using Object Lifecycle Management.
Changing the default storage class of a bucket does not affect any of the objects that already exist in the bucket.
Archive Storage also has higher costs for data access and operations.
https://cloud.google.com/storage/docs/domain-name-verification
https://cloud.google.com/storage/docs/key-terms#geo-redundant
https://cloud.google.com/storage/docs/moving-buckets
Moving data between locations incurs network usage costs.
In addition, moving data between buckets may incur retrieval and early deletion fees,
if the data being moved are Nearline Storage, Coldline Storage, or Archive Storage objects.
The gsutil command-line tool uses resumable uploads in the gsutil cp and gsutil rsync commands when uploading data to Cloud Storage.
https://cloud.google.com/storage/docs/metadata#content-type
If the Content-Type is not specified by the uploader and cannot be determined, it is set to application/octet-stream or application/x-www-form-urlencoded.
https://cloud.google.com/storage/docs/access-control
https://cloud.google.com/storage/docs/changing-storage-classes#gsutil
This guide involves rewriting data, which is a Class A operation and which may incur retrieval and early deletion charges
if the data was originally stored as Nearline Storage, Coldline Storage, or Archive Storage.
https://cloud.google.com/storage/docs/lifecycle
For buckets in a region, the new storage class cannot be Multi-Regional Storage.
For buckets in a multi-region or dual-region, the new storage class cannot be Regional Storage.
https://cloud.google.com/storage/docs/object-versioning
Cloud Storage retains a noncurrent object version each time you replace or delete a live object version,
as long as you do not specify the generation number of the live version.
Noncurrent versions retain the name of the object, but are uniquely identified by their generation number.
https://cloud.google.com/storage/docs/object-versioning#considerations
1. Object Versioning cannot be enabled on a bucket that currently has a retention policy.
2. There is no default limit on the number of object versions you can have.
    Each noncurrent version of an object is charged at the same rate as the live version of the object.
    If you enable versioning, consider using Object Lifecycle Management, which can remove the oldest versions of an object as newer versions become noncurrent.
https://cloud.google.com/storage/docs/object-versioning#example
https://cloud.google.com/storage/docs/using-object-versioning
https://cloud.google.com/storage/docs/bucket-lock
Retention policies and Object Versioning are mutually exclusive features in Cloud Storage: for a given bucket, only one of these can be enabled at a time.
https://cloud.google.com/storage/docs/access-control/making-data-public
https://cloud.google.com/storage/docs/reporting-changes
https://cloud.google.com/storage/docs/audit-logging
https://cloud.google.com/storage/docs/lifecycle
If multiple rules have their conditions satisfied simultaneously for a single object,
Cloud Storage performs the action associated with only one of the rules, based on the following considerations:
1. The Delete action takes precedence over any SetStorageClass action.
2. The SetStorageClass action that switches the object to the storage class with the lowest at-rest storage pricing takes precedence.
For buckets in a region, the new storage class cannot be Multi-Regional Storage.
For buckets in a multi-region or dual-region, the new storage class cannot be Regional Storage.
https://cloud.google.com/storage/docs/lifecycle#islive
The IsLive condition is typically only used in conjunction with Object Versioning.
When set to false, this condition is satisfied for any noncurrent version of an object.
When set to true, this condition is satisfied for the live version of an object.
https://cloud.google.com/storage/docs/lifecycle#numberofnewerversions
The NumberOfNewerVersions condition is typically only used in conjunction with Object Versioning.
If the value of this condition is set to N, an object version satisfies the condition when there are at least N versions (including the live version) newer than it.
For a live object version, the number of newer versions is considered to be 0.
For the most recent noncurrent version, the number of newer versions is 1 (or 0 if there is no live object version), and so on.
https://cloud.google.com/storage/docs/lifecycle#setstorageclass-cost
Unlike changing an object's storage class manually(by using rewrite), using SetStorageClass does not rewrite an object.
This gives Object Lifecycle Management certain pricing advantages:
1. There are no retrieval fees or early deletion fees associated with the storage class change,
    even when the object is originally set to Nearline Storage or Coldline Storage.
https://cloud.google.com/storage/docs/lifecycle#tracking
To track the lifecycle management actions that Cloud Storage takes, use one of the following options:
1. Use Cloud Storage usage logs. This feature logs both the action and who performed the action.
2. Enable Pub/Sub Notifications for Cloud Storage for your bucket. This feature sends notifications to a Pub/Sub topic of your choice when specified actions occur.
    Note that this feature does not record who performed the actions.
https://cloud.google.com/storage/docs/lifecycle#tracking
Cloud Audit Logs does not log actions taken by the Object Lifecycle Management feature.
Pub/Sub notifications for cloud storage tracks actions taken by Object Lifecycle Management.
https://cloud.google.com/storage/docs/managing-lifecycles
Lifecycle configurations are managed at the bucket level and apply to all current and future objects in the bucket.
https://cloud.google.com/storage/docs/access-control/iam
An IAM policy applied to your project defines the actions that users can take on all objects or buckets within your project.
An IAM policy applied to a single bucket defines the actions that users can take on that specific bucket and objects within it.
https://cloud.google.com/storage/docs/access-control/iam#project-level_roles_vs_bucket-level_roles
https://cloud.google.com/storage/docs/access-control/using-iam-permissions#gsutil_2
https://cloud.google.com/storage/docs/access-control/using-iam-permissions#conditions-iam
To set IAM Conditions on a bucket, you must first enable uniform bucket-level access on that bucket.
https://cloud.google.com/storage/docs/uniform-bucket-level-access
When you enable uniform bucket-level access on a bucket, Access Control Lists (ACLs) are disabled,
and only bucket-level Identity and Access Management (IAM) permissions grant access to that bucket and the objects it contains.
To support the ability to disable uniform bucket-level access and revert to using ACLs, Cloud Storage saves existing ACLs for 90 days.
If you disable uniform bucket-level access during this time:
1. Objects regain their saved ACLs.
https://cloud.google.com/storage/docs/encryption
https://cloud.google.com/storage/docs/encryption/using-customer-managed-keys
https://cloud.google.com/storage/docs/encryption/customer-supplied-keys#restrictions
https://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#storage-generate-encryption-key-java
https://cloud.google.com/storage/docs/pubsub-notifications
https://cloud.google.com/storage/docs/pubsub-notifications#events
https://cloud.google.com/storage/docs/access-control/signed-urls
Signed URLs contain authentication information in their query string, allowing users without credentials to perform specific actions on a resource.
When you generate a signed URL, you specify a user or service account which must have sufficient permission to make the request that the signed URL will make.
https://cloud.google.com/storage/docs/access-control/signing-urls-with-helpers
https://cloud.google.com/storage/docs/gsutil/commands/signurl#usage
https://cloud.google.com/storage/docs/caching
To use Cloud CDN, you must use external HTTP(S) Load Balancing with your Cloud Storage buckets as a backend.
Cloud CDN uses the Cache-Control metadata set on your objects to determine how they should be cached.
https://cloud.google.com/storage/docs/hosting-static-website
Note the IP address associated with the load balancer: for example, 30.90.80.100.
To point your domain to your load balancer, create an "A" record using your domain registration service.
https://cloud.google.com/storage/docs/hosting-static-website-http
To connect your domain to Cloud Storage, create a CNAME record through your domain registration service.

Storage Transfer Service:
https://cloud.google.com/storage-transfer/docs/overview
https://cloud.google.com/storage-transfer/docs/create-manage-transfer-console#amazon-s3
https://cloud.google.com/storage-transfer/docs/configure-access

Service Account:
https://cloud.google.com/iam/docs/service-accounts
https://www.preveil.com/blog/public-and-private-key/ (Explains the public and private key concept)

VPC:
https://cloud.google.com/vpc/docs/vpc
https://cloud.google.com/vpc/docs/vpc#specifications
You can create more than one subnet per region.
A network must have at least one subnet before you can use it.
https://cloud.google.com/vpc/docs/vpc#subnet-ranges
When an auto mode VPC network is created, one subnet from each region is automatically created within it. 
These automatically created subnets use a set of predefined IP ranges that fit within the 10.128.0.0/9 CIDR block.
You can switch a VPC network from auto mode to custom mode. This is a one-way conversion. 
Custom mode VPC networks cannot be changed to auto mode VPC networks.
https://cloud.google.com/vpc/docs/vpc#default-network
You can disable the creation of default networks by creating an organization policy with the compute.skipDefaultNetworkCreation constraint.
Because the subnets of every auto mode VPC network use the same predefined range of IP addresses, 
you cannot connect auto mode VPC networks to one another.
Google recommends that you use custom mode VPC networks in production.
https://cloud.google.com/vpc/docs/vpc#firewall_rules
Every VPC network has two implied firewall rules. One implied rule allows most egress traffic, and the other denies all ingress traffic.
You cannot delete the implied rules, but you can override them with your own.
To monitor which firewall rule allowed or denied a particular connection, see Firewall Rules Logging.
https://cloud.google.com/vpc/docs/vpc#internet_access_reqs
https://cloud.google.com/vpc/docs/vpc#app-engine-comm (App Engine does not reside in VPC)
https://cloud.google.com/vpc/docs/shared-vpc
Shared VPC allows an organization to connect resources from multiple projects to a common Virtual Private Cloud (VPC) network, 
so that they can communicate with each other securely and efficiently using internal IPs from that network.
Shared VPC connects projects within the same organization. Participating host and service projects cannot belong to different organizations.
https://cloud.google.com/vpc/docs/vpc-peering
Google Cloud VPC Network Peering allows internal IP address connectivity across two Virtual Private Cloud (VPC) networks 
regardless of whether they belong to the same project or the same organization.
Shared VPC and VPC peering is a way to establish direct connection 
between resources residing on separate VPC networks using internal IP address.
Cloud VPN is another way to achieve the same,
but Shared VPC/VPC peering is more suitable for this type of scenario and comes with low network latency and low cost.
Peering traffic (traffic flowing between peered networks) has the same latency, throughput, and availability 
as private traffic in the same network.
https://cloud.google.com/vpc/docs/alias-ip
https://cloud.google.com/vpc/docs/firewall-rules-logging
You enable Firewall Rules Logging individually for each firewall rule whose connections you need to log.
Firewall Rules Logging only records TCP and UDP connections.
Changes to firewall rules can be viewed in VPC audit logs.
You cannot enable Firewall Rules Logging for the implied deny ingress and implied allow egress rules.
https://cloud.google.com/vpc/docs/flow-logs
VPC Flow Logs interacts with firewall rules in the following ways:
1. Egress packets are sampled before egress firewall rules. 
   Even if an egress firewall rule denies outbound packets, those packets can be sampled by VPC Flow Logs.
2. Ingress packets are sampled after ingress firewall rules. 
   If an ingress firewall rule denies inbound packets, those packets are not sampled by VPC Flow Logs.
https://cloud.google.com/vpc/docs/serverless-vpc-access
Serverless VPC Access enables you to connect from a serverless environment on Google Cloud directly to your VPC network.
This connection makes it possible for your serverless environment to access resources in your VPC network via internal IP addresses.
With Serverless VPC Access, you create a connector in your Google Cloud project and attach it to a VPC network.
You then configure your serverless services (such as Cloud Run services, App Engine apps, or Cloud Functions) 
to use the connector for internal network traffic.
The following Google services support Serverless VPC Access connectors:
1. Cloud Run
2. App Engine standard environment (All runtimes except PHP 5)
3. Cloud Functions
https://cloud.google.com/network-connectivity/docs/router/concepts/overview
Cloud Router isn't supported for Direct Peering or Carrier Peering connections.
When you extend your on-premises network to Google Cloud, 
use Cloud Router to dynamically exchange routes between your Google Cloud networks and your on-premises network. 
Cloud Router peers with your on-premises VPN gateway or router.
The routers exchange topology information through BGP.
Without Cloud Router, you can use only static routes to configure your Cloud VPN tunnels.
https://cloud.google.com/network-connectivity/docs/router/concepts/overview#regional-dyn-routing
https://cloud.google.com/network-connectivity/docs/router/concepts/overview#global_dynamic_routing_example
With global dynamic routing, Cloud Router has visibility to resources in all regions.
For example, if you have VMs in one region, they can reach a Cloud VPN tunnel in another region automatically without maintaining static routes.
Note: We recommend creating two Cloud Routers in each region for an Interconnect connection. 
      Creating two Cloud Routers is required for 99.99% availability.
https://cloud.google.com/network-connectivity/docs/vpn/concepts/classic-topologies
https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview
Traffic traveling between the two networks is encrypted by one VPN gateway and then decrypted by the other VPN gateway. 
This action protects your data as it travels over the internet.
https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview#ha-vpn
HA VPN is a high-availability (HA) Cloud VPN solution that lets you securely connect your on-premises network to your VPC network 
through an IPsec VPN connection in a single region.
To achieve high availability when both VPN gateways are located in VPC networks, 
you must use two HA VPN gateways, and both of them must be located in the same region.
https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview#comparison_table
https://cloud.google.com/vpc/docs/configure-private-google-access
https://cloud.google.com/vpc/docs/using-flow-logs
https://www.coursera.org/lecture/logging-monitoring-observability-google-cloud/vpc-flow-logs-XWUmD (Vpc flow logs)
https://cloud.google.com/vpc/docs/using-firewall-rules-logging (Firewall logs)
https://cloud.google.com/vpc/docs/audit-logging
https://cloud.google.com/vpc/docs/configure-serverless-vpc-access
(Serverless VPC Access enables you to connect from a serverless environment on Google Cloud (Cloud Run (fully managed), Cloud Functions,
or the App Engine standard environment) directly to your VPC network.
This connection makes it possible for your serverless environment to access Compute Engine VM instances, Memorystore instances,
and any other resources with an internal IP address.)

Firewall:
https://cloud.google.com/vpc/docs/firewalls
Firewall configuration is associated with a VPC network. 
This means that you cannot share firewall rules among VPC networks, 
including networks connected by VPC Network Peering or by using Cloud VPN tunnels.
VPC firewall rules are stateful:
When a connection is allowed through the firewall in either direction, return traffic matching this connection is also allowed.
https://cloud.google.com/vpc/docs/firewalls#default_firewall_rules
Internet access is allowed if no other firewall rules deny outbound traffic 
and if the instance has an external IP address or uses a Cloud NAT instance.
The implied rules cannot be removed.
Deny rules take precedence over allow rules of the same priority.
Implied firewall rules are present in all VPC networks, regardless of how the networks are created, 
and whether they are auto mode or custom mode VPC networks.
The default network has the same implied rules.
https://cloud.google.com/vpc/docs/firewalls#more_rules_default_vpc
The default network is pre-populated with firewall rules that allow incoming connections to instances. (Note present in other networks)
default-allow-internal: Allows ingress connections for all protocols and ports among instances in the network.
https://cloud.google.com/vpc/docs/firewalls#direction_of_the_rule
Consider an example connection between two VMs in the same network. 
Traffic from VM1 to VM2 can be controlled by using either of these firewall rules:
1. An ingress rule with a target of VM2 and a source of VM1.
2. An egress rule with a target of VM1 and a destination of VM2.
https://cloud.google.com/vpc/docs/firewalls#priority_order_for_firewall_rules
 If you do not specify a priority when creating a rule, it is assigned a priority of 1000.
https://cloud.google.com/vpc/docs/firewalls#enforcement
You can change whether a firewall rule is enforced by setting its state to enabled or disabled. 
Disabling a rule is useful for troubleshooting or to grant temporary access to instances. 
It's much easier to disable a rule, test, and then re-enable it, than it is to delete and re-create the rule.
https://cloud.google.com/vpc/docs/vpc#app-engine-comm (Communications and access for App Engine)
Since app Engine standard environment instances do not run inside VPC network, VPC firewall rules do not apply to them.
https://cloud.google.com/vpc/docs/vpc#internet_access_reqs (Internet access requirements)
https://cloud.google.com/vpc/docs/firewall-rules-logging


BigTable:
Bigtable uses apache Hbase. Bigtable is not a relational database. It does not support SQL queries, joins, or multi-row transactions.
https://cloud.google.com/bigtable/docs/overview
https://cloud.google.com/bigtable/docs/quickstart-cbt
https://cloud.google.com/bigtable/docs/creating-instance
For a paticular bigtable instance, clusters can be in any region where Bigtable is available, as long as each cluster is in a different zone.
https://cloud.google.com/bigtable/docs/instances-clusters-nodes
A table belongs to an instance, not to a cluster or node.
A cluster is located in a single zone.
Behind the scenes, Bigtable splits all of the data in a table into separate tablets.
Tablets are stored on disk, separate from the nodes but in the same zone as the nodes. A tablet is associated with a single node.
https://cloud.google.com/bigtable/docs/access-control#overview
In Bigtable, you cannot grant access to the following types of members:
1. allAuthenticatedUsers
2. allUsers
https://cloud.google.com/bigtable/docs/audit-logging
https://cloud.google.com/bigtable/docs/keyvis-overview
The Key Visualizer tool for Bigtable provides scans that show the usage patterns for each table in a cluster.
Key Visualizer helps you check whether your schema design and usage patterns are causing undesirable results, such as hotspots on specific rows.
Each table has only one index, the row key. There are no secondary indices. Each row key must be unique.
https://cloud.google.com/bigtable/docs/creating-managing-labels#common-uses
https://cloud.google.com/bigtable/docs/modifying-instance
In the Cloud Console, monitoring data is available in the following places:
1. Bigtable monitoring
2. Bigtable instance overview
3. Google Cloud's operations suite Cloud Monitoring
4. Key Visualizer
https://cloud.google.com/bigtable/docs/backups#storage
A table backup is a cluster-level resource.
Even if a table is in an instance with multiple clusters (meaning the cluster is using replication),
a backup is created and stored on only one cluster in that instance.
https://cloud.google.com/bigtable/docs/managing-backups
https://cloud.google.com/bigtable/docs/import-export
https://cloud.google.com/bigtable/docs/managing-tables
https://cloud.google.com/bigtable/docs/managing-tables#multiple-versions
https://cloud.google.com/bigtable/docs/failovers
https://cloud.google.com/bigtable/docs/managing-failovers
https://cloud.google.com/bigtable/docs/audit-logging

Cloud Logging:
https://cloud.google.com/logging/docs/routing/overview
https://cloud.google.com/logging/docs/export
https://cloud.google.com/logging/docs/export/configure_export_v2
https://cloud.google.com/logging/docs/export/aggregated_sinks (Aggregated sink)


Cloud Audit logs:
Google Cloud services write audit logs to help you answer the questions, "Who did what, where, and when?" 
Your Cloud projects contain only the audit logs for resources that are directly within the project. 
Other entities, such as folders, organizations, and Cloud Billing accounts, contain the audit logs for the entity itself.
https://cloud.google.com/logging/docs/audit
https://www.youtube.com/watch?v=iR8GjOwTOrQ
Data Access audit logsexcept for BigQueryare disabled by default.
https://cloud.google.com/logging/docs/audit/configure-data-access

App Engine:
APP ENGINE STANDARD DOES NOT USE VPC NETWORKS. SEE: https://www.youtube.com/watch?v=T0P6qsTuh1s
APP ENGINE FLEXIBLE USES VPC.
https://cloud.google.com/appengine/docs/standard
https://cloud.google.com/appengine/docs/flexible
https://cloud.google.com/appengine/docs/the-appengine-environments
https://cloud.google.com/appengine/docs/standard/java11/an-overview-of-app-engine
https://cloud.google.com/appengine/docs/standard/java11/configuration-files
You must deploy the initial version of your app to the default service before you can create and deploy additional services to your app.
https://cloud.google.com/appengine/docs/standard/java11/building-app
https://cloud.google.com/appengine/docs/standard/java11/building-app/deploying-web-service
You can specify the name of your service in the app.yaml file. If the name is omitted, it is treated as default. 
The first service you deploy must be the default service.
You can update your service at any time by running the gcloud app deploy command again. 
Each time you deploy, a new version is created and traffic is automatically routed to the latest version.
https://cloud.google.com/appengine/docs/standard/java11/config/appref
https://cloud.google.com/appengine/docs/standard/java11/reference/dispatch-yaml
An app engine app can have only one dispatch.yaml file.
https://cloud.google.com/appengine/docs/standard/java11/labeling-resources
https://cloud.google.com/appengine/docs/standard/java11/how-requests-are-routed
https://cloud.google.com/appengine/docs/standard/java11/application-security#ingress_controls
 If you use Cloud Load Balancing with your App Engine app, we recommend that you use ingress controls to ensure that your app receives only internal
 and Cloud Load Balancing traffic.
 Otherwise, users can use your app's appspot URL to bypass the load balancer.
 https://cloud.google.com/appengine/docs/standard/java11/understanding-firewalls
 For App Engine, the App Engine firewall only applies to incoming traffic routed to your app or service.
https://cloud.google.com/appengine/docs/standard/java11/access-control#auth-cloud-implicit-java
By default, the app's environment contains credentials from the default App Engine service account.
This service account is created by Google when you create an App Engine app and is given full permissions to manage and use all Cloud services in a GCP project.
https://cloud.google.com/appengine/docs/standard/java11/service-account
By default, the App Engine default service account has the Editor role in the project.
https://cloud.google.com/appengine/docs/standard/java11/audit-logging#audit_log_format
App Engine doesn't write Data Access audit logs or System Event audit logs.
Admin Activity audit logs and System Event audit logs are free.
Data Access audit logs and Policy Denied audit logs are chargeable.
https://cloud.google.com/appengine/docs/standard/java11/testing-and-deploying-your-app#other_deployment_options
https://cloud.google.com/appengine/docs/standard/java11/testing-and-deploying-your-app#testing-on-app-engine
Deploy your new version, but prevent traffic from being automatically routed to the new version:
mvn appengine:deploy -Dapp.deploy.projectId=PROJECT_ID -Dapp.deploy.promote=False
https://cloud.google.com/appengine/docs/standard/java11/writing-application-logs
https://cloud.google.com/appengine/docs/standard/java11/storage-options
https://cloud.google.com/appengine/docs/standard/java11/using-cloud-datastore
Datastore mode automatically creates single-property indexes for use with simple types of queries.
For complex queries that include multiple properties, you'll need to configure composite indexes in your app's index.yaml file.
https://cloud.google.com/appengine/docs/standard/java11/using-cloud-storage
App Engine creates a default bucket when you create an app.
This bucket provides the first 5GB of storage for free and includes a free quota for Cloud Storage I/O operations.
https://cloud.google.com/appengine/docs/standard/java11/serving-static-files
To serve static files for Java 11 in the standard environment, you define the handlers in your app.yaml file using either the static_dir or static_files elements.
https://cloud.google.com/appengine/docs/standard/java11/how-instances-are-managed#loading_requests
https://cloud.google.com/appengine/docs/standard/java11/how-instances-are-managed#warmup_requests
https://cloud.google.com/appengine/docs/standard/java11/migrating-traffic
https://cloud.google.com/appengine/docs/standard/java11/splitting-traffic
When you have specified two or more versions for splitting, you must choose whether to split traffic by using either an IP address or HTTP cookie. 
It's easier to set up an IP address split, but a cookie split is more precise.
https://cloud.google.com/sdk/gcloud/reference/app/services/set-traffic
https://cloud.google.com/appengine/docs/standard/java11/scheduling-jobs-with-cron-yaml
https://cloud.google.com/appengine/docs/standard/java11/connecting-vpc
To use Serverless VPC Access, you must first create a Serverless VPC Access connector to handle communication to your VPC network.
https://cloud.google.com/vpc/docs/vpc#app-engine-comm (App Engine does not reside in VPC)


Cloud SQL:
https://cloud.google.com/sql/docs/mysql/create-instance
https://cloud.google.com/sql/docs/mysql/clone-instance
https://cloud.google.com/sql/docs/mysql/monitor-instance
https://cloud.google.com/sql/docs/mysql/delete-instance
You cannot delete an instance that has any replicas. You must delete all replicas first.
https://cloud.google.com/sql/docs/mysql/connect-overview
https://cloud.google.com/sql/docs/mysql/configure-ip
https://cloud.google.com/sql/docs/mysql/configure-private-ip
https://cloud.google.com/sql/docs/mysql/authorize-networks
Even if your instance has only a public IP address, you can connect to it securely by using the Cloud SQL Auth proxy.
All traffic between the Cloud SQL Auth proxy and your Cloud SQL instance is encrypted.
If you don't use the proxy, and you are connecting your client from your own public IP address,
you need to add your client's public address as an authorized network.
Your client application's IP address or address range must be configured as authorized networks for the following conditions:
1. Your client application is connecting directly to a Cloud SQL instance on its public IP address.
2. Your client application is connecting directly to a Cloud SQL instance on its private IP address, and your client's IP address is a non-RFC 1918 address
https://cloud.google.com/sql/docs/mysql/connect-admin-proxy
https://cloud.google.com/sql/docs/mysql/connect-admin-ip
https://cloud.google.com/sql/docs/mysql/connect-app-engine-standard
https://cloud.google.com/sql/docs/mysql/connect-kubernetes-engine
https://cloud.google.com/sql/docs/mysql/configure-ha
https://cloud.google.com/sql/docs/mysql/backup-recovery/backing-up
https://cloud.google.com/sql/docs/mysql/backup-recovery/restoring
https://cloud.google.com/sql/docs/mysql/backup-recovery/pitr
https://cloud.google.com/sql/docs/mysql/replication/create-replica
https://cloud.google.com/sql/docs/mysql/replication/cross-region-replicas
There are two common scenarios for promoting cross-region replicas:
1. Regional migration: Perform a planned migration of a database to a different region.
2. Disaster recovery: Fail over a database to another region in the event that the primary's region becomes unavailable.
Note: Promoting a replica is done manually and intentionally.
It is not the same as high availability,
where a standby instance (which is not a replica) automatically becomes the primary in case of a failure or zonal outage.
https://cloud.google.com/sql/docs/mysql/import-export/importing
https://cloud.google.com/sql/docs/mysql/import-export/exporting
https://cloud.google.com/sql/docs/mysql/create-manage-databases
https://cloud.google.com/sql/docs/mysql/create-manage-users
Additional material: https://cloud.google.com/sql/docs/mysql/high-availability
Additional material: https://cloud.google.com/sql/docs/mysql/replication
