Ultimatix Knowmax learning:
In Knowmax -> Organization units -> Banking and Technology Services -> Services -> Google Business Unit -> GSkool option, there learning materials.

O'reilly learning (Access from ultimatix):
https://learning.oreilly.com/library/view/official-google-cloud/9781119564416/

LinkedIn links for tutorials:
https://www.linkedin.com/learning/google-cloud-platform-for-enterprise-essential-training/enterprise-ready-gcp?u=2154233
https://www.linkedin.com/learning/google-cloud-platform-essential-training-3/hosting-your-application-on-google-cloud-platform?u=2154233

Github repos:
https://github.com/lynnlangit/gcp-essentials
https://github.com/lynnlangit/gcp-ml

https://cloud.google.com/pricing

https://medium.com/google-cloud/gcpcomics/home

https://cloud.google.com/docs

https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations

https://cloud.google.com/terms/services

https://developers.google.com/community/experts/directory

https://cloud.google.com/identity/docs/overview

https://cloud.google.com/resource-manager/docs/cloud-platform-resource-hierarchy

https://cloud.google.com/iam/docs/resource-hierarchy-access-control
https://cloud.google.com/iam/docs/job-functions/networking

https://cloud.google.com/billing/docs/concepts

https://cloud.google.com/files/Lift-and-Shift-onto-Google-Cloud.pdf
https://cloud.google.com/solutions/application-migration
https://cloud.google.com/solutions/migrating-vms-migrate-for-compute-engine-getting-started

https://cloud.google.com/compute/docs/images/image-management-best-practices

https://cloud.google.com/migrate/compute-engine/docs/4.11/concepts/architecture/gcp-reference-architecture

https://cloud.google.com/anthos/docs/concepts/overview
https://cloud.google.com/anthos/docs/tutorials/explore-anthos

https://cloud.google.com/kuberun/docs/setup

https://cloud.google.com/solutions/dr-scenarios-planning-guide

https://cloud.google.com/secret-manager/docs/overview
https://cloud.google.com/solutions/dr-scenarios-for-data
https://cloud.google.com/solutions/bigquery-data-warehouse
https://cloud.google.com/billing/docs/how-to/visualize-data
https://cloud.google.com/bigquery-ml/docs/introduction

https://cloud.google.com/data-fusion/docs

google Cloud dataflow uses apache beam and cloud dataproc uses apache hadoop/spark and cloud composer uses apache airflow.

https://cloud.google.com/solutions/build-a-data-lake-on-gcp

https://airflow.apache.org/docs/apache-airflow/stable/ui.html

https://cloud.google.com/life-sciences/docs/how-tos/variant-transforms
https://cloud.google.com/life-sciences/docs/quickstart
https://cloud.google.com/life-sciences/docs/resources/public-datasets
https://ai.googleblog.com/2017/12/deepvariant-highly-accurate-genomes.html

https://console.cloud.google.com/marketplace
https://cloud.google.com/foundation-toolkit

https://cloud.google.com/build/docs/overview

Difference between serverless and containers: https://www.cloudflare.com/learning/serverless/serverless-vs-containers/

https://codelabs.developers.google.com/codelabs/cloud-builder-gke-continuous-deploy/index.html#0
https://cloud.google.com/build/docs/cloud-builders

https://cloud.google.com/compute/quotas

https://cloud.google.com/products/tools

https://github.com/googlecloudplatform

https://cloud.google.com/sdk/docs/quickstart

https://cloud.google.com/docs/compare/aws

https://cloud.google.com/storage/docs/storage-classes

https://cloud.google.com/datastore/docs/firestore-or-datastore

https://datastudio.google.com/navigation/reporting

https://cloud.google.com/code

https://cloud.google.com/serverless/whitepaper

https://medium.com/google-cloud/about

https://cloud.google.com/appengine/docs/flexible
https://cloud.google.com/appengine/docs/standard

https://cloud.google.com/blog/products/bigquery/bigquery-under-the-hood

https://cloud.google.com/network-connectivity/docs/interconnect/concepts/dedicated-overview
https://cloud.google.com/network-connectivity/docs/interconnect/concepts/partner-overview

https://cloud.google.com/logging/docs/export/configure_export_v2


https://www.youtube.com/watch?v=r7ce2yqdckk
https://www.coursera.org/lecture/logging-monitoring-observability-google-cloud/exporting-and-analyzing-logs-peXBz

https://cloud.google.com/compute/docs/disks/snapshots
https://cloud.google.com/compute/docs/regions-zones/global-regional-zonal-resources#globalresources

https://cloud.google.com/appengine/docs/admin-api/migrating-splitting-traffic

Compute Engine:
https://cloud.google.com/compute/docs/instances/create-start-instance
https://cloud.google.com/compute/docs/instances/create-start-preemptible-instance#handle_preemption
Do not use regional persistent disks for boot disks. In a failover situation, they do not force-attach to a VM.
https://cloud.google.com/compute/docs/disks/detach-reattach-boot-disk
https://cloud.google.com/compute/docs/instances/windows/creating-managing-windows-instances
https://cloud.google.com/compute/docs/instances/windows/creating-passwords-for-windows-instances
https://cloud.google.com/compute/docs/instances/create-vm-from-instance-template
https://cloud.google.com/compute/docs/instances/create-vm-from-similar-instance
https://cloud.google.com/compute/docs/instances/custom-hostname-vm
https://cloud.google.com/compute/docs/instances/ssh#metadata-managed_ssh_connections
https://cloud.google.com/compute/docs/instances/connecting-advanced#provide-key
https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys#risks
By creating and managing SSH keys, you can let users access a Linux instance through third-party tools.
An SSH key consists of the following files:
1) A public SSH key file that is applied to instance-level metadata or project-wide metadata.
2) A private SSH key file that the user stores on their local devices.
If a user presents their private SSH key, they can use a third-party tool to connect to any instance that is configured with the matching public SSH key file,
even if they aren't a member of your Google Cloud project.
Therefore, you can control which instances a user can access by changing the public SSH key metadata for one or more instances.
https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys
https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys#edit-ssh-metadata
https://cloud.google.com/compute/docs/disks/add-persistent-disk
https://cloud.google.com/compute/docs/disks/working-with-persistent-disks#disk_type
You can change the type of your persistent disk using snapshots. For example, to change your standard persistent disk to an SSD persistent disk, use the following process:
1) Create a snapshot of your standard persistent disk.
2) Create a new persistent disk based on the snapshot. Include the --type flag and specify pd-ssd.
https://cloud.google.com/compute/docs/disks/regional-persistent-disk#restrictions
Restrictions
1) You cannot use regional persistent disks as boot disks.
2) You can create a regional persistent disk from a snapshot but not an image.
https://cloud.google.com/compute/docs/disks/regional-persistent-disk#use_multi_instances
https://cloud.google.com/compute/docs/disks/regional-persistent-disk#migrate-repd
https://cloud.google.com/compute/docs/disks/create-disk-from-source
You can create persistent disks from the following data sources:
1) Existing disks: Clone an existing persistent disk. Use this option if you need an instantly attachable copy of an existing non-boot persistent disk.
2) Snapshots: Create a non-boot disk from a source snapshot. Use this option to restore data from a persistent disk that you've backed up using snapshots.
3) Images: Create a boot disk from a source image. Use this option to create a boot disk for a new VM or to create a standalone boot persistent disk.
https://cloud.google.com/compute/docs/disks/create-disk-from-source#clone
Restrictions
1) The zone, region, and disk type of the clone must be the same as that of the source disk.
2) You cannot create a zonal disk clone from a regional disk. You cannot create a regional disk clone from a zonal disk.
https://cloud.google.com/compute/docs/disks/create-snapshots
You can create snapshots from disks even while they are attached to running instances.
Snapshots are global resources, so you can use them to restore data to a new disk or instance within the same project.
You can also share snapshots across projects.
https://cloud.google.com/compute/docs/disks/create-snapshots#restore-snapshots
Compute Engine uses incremental snapshots so that each snapshot contains only the data that has changed since the previous snapshot.
https://cloud.google.com/compute/docs/disks/scheduled-snapshots
Restrictions:
1) A persistent disk can have only one snapshot schedule attached to it at a time.
2) You cannot delete a snapshot schedule if it is attached to a disk. You must detach the schedule from all disks, then delete the schedule.
https://cloud.google.com/compute/docs/disks/scheduled-snapshots#deletion_rule
https://cloud.google.com/compute/docs/disks/scheduled-snapshots#change_snapshot_schedule
https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images
Create the image
You can create disk images from the following sources:
1) A persistent disk, even while that disk is attached to a VM
2) A snapshot of a persistent disk
3) Another image in your project
https://cloud.google.com/compute/docs/instances/startup-scripts/linux
https://cloud.google.com/compute/docs/shutdownscript
Create and run shutdown scripts that execute commands right before a virtual machine (VM) instance is stopped or restarted.
Shutdown scripts have a limited amount of time to finish running before the instance stops:
1. On-demand instances: 90 seconds after you stop or delete an instance
2. Preemptible instances: 30 seconds after instance preemption begins
https://cloud.google.com/compute/docs/shutdownscript#shutdown_actions
The shutdown script won't run if the instance is reset using instances().reset.
https://cloud.google.com/compute/docs/instances/stop-start-instance
You cannot stop and restart a VM with a local SSD attached.
https://cloud.google.com/compute/docs/instances/deleting-instance
https://cloud.google.com/compute/docs/instances/update-instance-properties
https://cloud.google.com/compute/docs/instances/moving-instance-across-zones
Move your instance manually using the following steps:
1. Create snapshots of persistent disks attached to the original instance.
2. Create copies of the persistent disks in the destination zone.
3. For external and internal IP addresses:
    a. If you are moving an instance across zones within the same region, and you want to preserve its ephemeral IP address,
    temporarily promote the ephemeral IP address that is assigned to the instance to a static IP address
    and then assign it to the new VM instance you create in the destination zone.
    b. If you are moving an instance across regions, you must pick a different IP address for the VM instance.
4. Create and boot up a new instance in the destination zone. If you are moving across regions, you must also pick a new subnetwork for the new instance.
5. Attach the new persistent disks to your new instance.
6. Assign an external IP address to the new instance. If necessary, demote the address back to an ephemeral external IP address.
7. Delete the snapshots, original disks, and original instance.
https://cloud.google.com/compute/docs/instances/moving-instance-across-zones#limitations
https://cloud.google.com/compute/docs/instances/copy-vm-between-projects
https://cloud.google.com/compute/docs/instances/changing-machine-type-of-stopped-instance
https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances
A virtual machine instance can only have one service account identity.
https://cloud.google.com/compute/docs/instances/setting-instance-scheduling-options
https://cloud.google.com/compute/docs/instances/setting-instance-scheduling-options#settingoptions
https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances
You can create regional MIGs or zonal MIGs.
You must select which zones are associated with a regional MIG when you create the regional MIG.
After you choose specific zones during creation, you cannot change or update the zones later.
1.  To select more than three zones within a region, you must explicitly specify the individual zones.
    For example, to select all four zones within a region, you must provide all four zones explicitly in your request.
    If you do not, Compute Engine selects three zones by default.
2.  To select two or fewer zones in a region, you must explicitly specify the individual zones.
    Even if the region only contains two zones, you must still explicitly specify the zones in your request.
https://cloud.google.com/compute/docs/instance-groups/regional-migs#autoscaling_a_regional_mig
An autoscaling policy is applied to the group as a whole.
For example, if you enable autoscaler to target 66% CPU utilization,
the autoscaler tracks all instances in the group to maintain an average 66% utilization across all instances in all zones.
To add more instances to a MIG, you can:
1. Manually set the size of the MIG.
2. Use autoscaling for stateless applications.
https://cloud.google.com/compute/docs/instance-groups/autohealing-instances-in-migs
An autohealing policy relies on an application-based health check to verify that an application is responding as expected.
Checking that an application responds is more precise than simply verifying that a VM is in a RUNNING state.
If the autohealer determines that an application isn't responding, the MIG automatically recreates that VM.
You can set a maximum of one autohealing policy per MIG.
https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address
https://cloud.google.com/compute/docs/autoscaler/
Autoscaling only works with zonal and regional managed instance groups (MIGs). Unmanaged instance groups are not supported.
You cannot use autoscaling if your MIG has stateful configuration.
Autoscaling works independently from autohealing.
https://cloud.google.com/compute/docs/autoscaler#autoscaling_policy
https://cloud.google.com/compute/docs/autoscaler/multiple-signals#overview
https://cloud.google.com/compute/docs/autoscaler/scaling-cpu
https://cloud.google.com/compute/docs/startupscript
probably, you can only create boot disk from an image and not a snapshot. please verify.
Difference between image and snapshot:
https://stackoverflow.com/a/29634892
https://www.quora.com/What-is-the-difference-between-an-image-and-a-snapshot-on-Google-Cloud
https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images


Load Balancers:
https://cloud.google.com/load-balancing/docs/choosing-load-balancer (Choosing a load balancer)
https://cloud.google.com/load-balancing/docs/https (External HTTP(S) load balancing)
https://cloud.google.com/load-balancing/docs/l7-internal (Internal HTTP(S) load balancing)
https://cloud.google.com/load-balancing/docs/ssl (External SSl (TCP traffic with SSL offload) proxy load balancing)
https://cloud.google.com/load-balancing/docs/tcp (External TCP proxy load balancing)
https://cloud.google.com/load-balancing/docs/network/networklb-backend-service (External TCP/UDP network load balancers)
With the Premium Tier, a load balancer can be configured as a global load balancing service.
With Standard Tier, a load balancer handles load balancing regionally.

Cloud Monitoring:
https://cloud.google.com/monitoring/quickstart-lamp
https://cloud.google.com/monitoring/settings/multiple-projects
Using the Monitoring agent is optional but recommended.
Monitoring can access some instance metrics without the Monitoring agent, including CPU utilization, some disk traffic metrics,
network traffic, and uptime information
https://cloud.google.com/monitoring/agent/monitoring

Kubernetes:
https://cloud.google.com/kubernetes-engine/docs
https://cloud.google.com/kubernetes-engine/docs/concepts/types-of-clusters
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler
https://cloud.google.com/kubernetes-engine/docs/concepts/deployment
https://cloud.google.com/kubernetes-engine/docs/how-to/scaling-apps#autoscaling-deployments
https://cloud.google.com/kubernetes-engine/docs/concepts/service
https://kubernetes.io/docs/concepts/services-networking/service/
https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials
https://cloud.google.com/stackdriver/docs/solutions/gke
https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs
https://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine
https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools
https://cloud.google.com/kubernetes-engine/docs/how-to/node-pools
##############SEE ingress rules on gke which is an HTTP(S) load balancer. "LoadBalancer" service is a TCP/UDP network load balancer#############

BigQuery:
https://cloud.google.com/bigquery/docs/estimate-costs#bq
https://cloud.google.com/bigquery/docs/bq-command-line-tool#setting_default_values_for_command-line_flags
You can set default values for command-line flags by including them in the bq command-line tool's configuration file, .bigqueryrc
https://cloud.google.com/bigquery/docs/managing-jobs#bq
Jobs are actions that BigQuery runs on your behalf to load data, export data, query data, or copy data.
https://cloud.google.com/bigquery/docs/datasets-intro#dataset_limitations
https://cloud.google.com/bigquery/docs/locations#move-dataset
The BigQuery Data Transfer Service transfers (copies) data from a source to a destination dataset in BigQuery.
https://cloud.google.com/bigquery/docs/datasets#bq
https://cloud.google.com/bigquery/docs/copying-datasets
Dataset copying uses features of the BigQuery Data Transfer Service.
https://cloud.google.com/bigquery/docs/dataset-access-controls
https://cloud.google.com/bigquery/docs/dataset-access-controls#granting_access_to_a_dataset
https://cloud.google.com/bigquery/docs/listing-datasets
https://cloud.google.com/bigquery/docs/dataset-metadata#information_schema
https://cloud.google.com/bigquery/docs/updating-datasets
https://cloud.google.com/bigquery/docs/updating-datasets#table-expiration
https://cloud.google.com/bigquery/docs/managing-datasets
https://cloud.google.com/bigquery/docs/tables
https://cloud.google.com/bigquery/docs/tables#getting_table_information_using_information_schema_beta
https://cloud.google.com/bigquery/docs/table-access-controls#bq
https://cloud.google.com/bigquery/docs/managing-tables
https://cloud.google.com/bigquery/docs/exporting-data#bq
You can undelete a table within seven days of deletion, including explicit deletions and implicit deletions due to table expiration.
You cannot export table data to a local file, to Sheets, or to Drive. The only supported export location is Cloud Storage.
You cannot change the location of a dataset after it is created, but you can make a copy of the dataset.
https://cloud.google.com/bigquery/docs/partitioned-tables
By dividing a large table into smaller partitions, you can improve query performance,
and you can control costs by reducing the number of bytes read by a query.
You can get information about partitioned tables using the INFORMATION_SCHEMA.PARTITIONS view.
https://cloud.google.com/bigquery/docs/views
https://cloud.google.com/bigquery/docs/view-metadata#views_view
https://cloud.google.com/bigquery/docs/adding-labels
https://cloud.google.com/bigquery/docs/loading-data#loading_data_from_other_google_services
https://cloud.google.com/bigquery/docs/loading-data#alternatives_to_loading_data
https://cloud.google.com/bigquery/docs/batch-loading-data#data-locations
Exception: If your dataset is in the US multi-regional location, you can load data from a Cloud Storage bucket in any regional or multi-regional location.
https://cloud.google.com/bigquery/docs/query-overview#types_of_queries
After you load your data into BigQuery, you can query the data in your tables. BigQuery supports two types of queries:
1) Interactive queries
2) Batch queries
By default, BigQuery runs interactive queries, which means that the query is executed as soon as possible.
BigQuery also offers batch queries.
BigQuery queues each batch query on your behalf and starts the query as soon as idle resources are available, usually within a few minutes.
https://cloud.google.com/bigquery/docs/query-overview#query_jobs
https://cloud.google.com/bigquery/docs/running-queries#batch
https://cloud.google.com/bigquery/docs/dry-run-queries
https://cloud.google.com/bigquery/docs/custom-quotas
https://cloud.google.com/bigquery/docs/custom-quotas#example
https://cloud.google.com/bigquery/docs/monitoring#information_schema_views
https://cloud.google.com/bigquery/docs/best-practices-costs
https://cloud.google.com/bigquery/docs/best-practices-costs#preview-data
In the bq command-line tool, use the bq head command and specify the number of rows to preview.
https://cloud.google.com/bigquery/docs/best-practices-costs#limit_query_costs_by_restricting_the_number_of_bytes_billed
You can limit the number of bytes billed for a query using the maximum bytes billed setting.
When you set maximum bytes billed, the number of bytes that the query will read is estimated before the query execution.
If the number of estimated bytes is beyond the limit, then the query fails without incurring a charge.

IAM:
https://cloud.google.com/iam/docs/creating-custom-roles
https://cloud.google.com/sdk/gcloud/reference/iam
https://cloud.google.com/iam/docs/granting-changing-revoking-access

GCLOUD SDK AUTHORIZATION:
https://cloud.google.com/sdk/docs/authorizing
https://cloud.google.com/sdk/docs/initializing
https://cloud.google.com/sdk/docs/configurations

Google cloud storage:
https://cloud.google.com/storage/docs/storage-classes#comparison_of_storage_classes
https://cloud.google.com/storage/docs/access-control
https://cloud.google.com/storage/docs/access-control/signing-urls-with-helpers
https://cloud.google.com/storage/docs/gsutil/commands/signurl#usage
https://cloud.google.com/storage/docs/changing-storage-classes#gsutil
https://cloud.google.com/storage/docs/lifecycle
For buckets in a region, the new storage class cannot be Multi-Regional Storage.
For buckets in a multi-region or dual-region, the new storage class cannot be Regional Storage.
https://cloud.google.com/storage/docs/managing-lifecycles#gsutil_1
https://cloud.google.com/storage/docs/moving-buckets
Changing the default storage class of a bucket does not affect any of the objects that already exist in the bucket.
https://cloud.google.com/storage/docs/creating-buckets
https://cloud.google.com/storage/docs/uniform-bucket-level-access
https://cloud.google.com/storage/docs/object-versioning
https://cloud.google.com/storage/docs/using-object-versioning
https://cloud.google.com/storage/docs/access-control/making-data-public
https://cloud.google.com/storage/docs/pubsub-notifications
https://cloud.google.com/storage/docs/pubsub-notifications#events
https://cloud.google.com/storage/docs/reporting-changes
https://cloud.google.com/storage/docs/audit-logging
https://cloud.google.com/storage/docs/lifecycle#tracking
Cloud Audit Logs does not log actions taken by the Object Lifecycle Management feature.
Pub/Sub notifications for cloud storage tracks actions taken by Object Lifecycle Management.

Storage Transfer Service:
https://cloud.google.com/storage-transfer/docs/overview
https://cloud.google.com/storage-transfer/docs/create-manage-transfer-console#amazon-s3
https://cloud.google.com/storage-transfer/docs/configure-access

Service Account:
https://cloud.google.com/iam/docs/service-accounts
https://www.preveil.com/blog/public-and-private-key/ (Explains the public and private key concept)

VPC:
https://cloud.google.com/vpc/docs/vpc
https://cloud.google.com/network-connectivity/docs/router/concepts/overview
https://cloud.google.com/network-connectivity/docs/vpn/concepts/classic-topologies
https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview
https://cloud.google.com/vpc/docs/configure-private-google-access
https://cloud.google.com/vpc/docs/using-flow-logs
https://www.coursera.org/lecture/logging-monitoring-observability-google-cloud/vpc-flow-logs-XWUmD (Vpc flow logs)
https://cloud.google.com/vpc/docs/using-firewall-rules-logging (Firewall logs)
https://cloud.google.com/vpc/docs/audit-logging
https://cloud.google.com/vpc/docs/configure-serverless-vpc-access
(Serverless VPC Access enables you to connect from a serverless environment on Google Cloud (Cloud Run (fully managed), Cloud Functions,
or the App Engine standard environment) directly to your VPC network.
This connection makes it possible for your serverless environment to access Compute Engine VM instances, Memorystore instances,
and any other resources with an internal IP address.)

Firewall:
https://cloud.google.com/vpc/docs/firewalls
https://cloud.google.com/vpc/docs/vpc#app-engine-comm (Communications and access for App Engine)
Since app Engine standard environment instances do not run inside VPC network, VPC firewall rules do not apply to them.
https://cloud.google.com/vpc/docs/vpc#internet_access_reqs (Internet access requirements)

BigTable:
Bigtable uses apache Hbase. Bigtable is not a relational database. It does not support SQL queries, joins, or multi-row transactions.
https://cloud.google.com/bigtable/docs/overview
https://cloud.google.com/bigtable/docs/quickstart-cbt
https://cloud.google.com/bigtable/docs/creating-instance
For a paticular bigtable instance, clusters can be in any region where Bigtable is available, as long as each cluster is in a different zone.
https://cloud.google.com/bigtable/docs/instances-clusters-nodes
A table belongs to an instance, not to a cluster or node.
A cluster is located in a single zone.
Behind the scenes, Bigtable splits all of the data in a table into separate tablets.
Tablets are stored on disk, separate from the nodes but in the same zone as the nodes. A tablet is associated with a single node.
https://cloud.google.com/bigtable/docs/access-control#overview
In Bigtable, you cannot grant access to the following types of members:
1. allAuthenticatedUsers
2. allUsers
https://cloud.google.com/bigtable/docs/audit-logging
https://cloud.google.com/bigtable/docs/keyvis-overview
The Key Visualizer tool for Bigtable provides scans that show the usage patterns for each table in a cluster.
Key Visualizer helps you check whether your schema design and usage patterns are causing undesirable results, such as hotspots on specific rows.
Each table has only one index, the row key. There are no secondary indices. Each row key must be unique.
https://cloud.google.com/bigtable/docs/creating-managing-labels#common-uses
https://cloud.google.com/bigtable/docs/modifying-instance
In the Cloud Console, monitoring data is available in the following places:
1. Bigtable monitoring
2. Bigtable instance overview
3. Google Cloud's operations suite Cloud Monitoring
4. Key Visualizer
https://cloud.google.com/bigtable/docs/backups#storage
A table backup is a cluster-level resource.
Even if a table is in an instance with multiple clusters (meaning the cluster is using replication),
a backup is created and stored on only one cluster in that instance.
https://cloud.google.com/bigtable/docs/managing-backups
https://cloud.google.com/bigtable/docs/import-export
https://cloud.google.com/bigtable/docs/managing-tables
https://cloud.google.com/bigtable/docs/managing-tables#multiple-versions
https://cloud.google.com/bigtable/docs/failovers
https://cloud.google.com/bigtable/docs/managing-failovers
https://cloud.google.com/bigtable/docs/audit-logging

Cloud Logging:
https://cloud.google.com/logging/docs/routing/overview
https://cloud.google.com/logging/docs/export
https://cloud.google.com/logging/docs/export/configure_export_v2
https://cloud.google.com/logging/docs/export/aggregated_sinks (Aggregated sink)


Cloud Audit logs:
https://cloud.google.com/logging/docs/audit
https://www.youtube.com/watch?v=iR8GjOwTOrQ
Data Access audit logs—except for BigQuery—are disabled by default.
https://cloud.google.com/logging/docs/audit/configure-data-access

App Engine:
https://cloud.google.com/appengine/docs/standard
https://cloud.google.com/appengine/docs/flexible
https://cloud.google.com/appengine/docs/the-appengine-environments
https://cloud.google.com/appengine/docs/standard/java11/an-overview-of-app-engine
https://cloud.google.com/appengine/docs/standard/java11/configuration-files
You must deploy the initial version of your app to the default service before you can create and deploy additional services to your app.
https://cloud.google.com/appengine/docs/standard/java11/building-app
https://cloud.google.com/appengine/docs/standard/java11/config/appref
https://cloud.google.com/appengine/docs/standard/java11/reference/dispatch-yaml
An app engine app can have only one dispatch.yaml file.
https://cloud.google.com/appengine/docs/standard/java11/labeling-resources
https://cloud.google.com/appengine/docs/standard/java11/how-requests-are-routed
https://cloud.google.com/appengine/docs/standard/java11/application-security#ingress_controls
 If you use Cloud Load Balancing with your App Engine app, we recommend that you use ingress controls to ensure that your app receives only internal
 and Cloud Load Balancing traffic.
 Otherwise, users can use your app's appspot URL to bypass the load balancer.
https://cloud.google.com/appengine/docs/standard/java11/access-control#auth-cloud-implicit-java
By default, the app's environment contains credentials from the default App Engine service account.
This service account is created by Google when you create an App Engine app and is given full permissions to manage and use all Cloud services in a GCP project.
https://cloud.google.com/appengine/docs/standard/java11/service-account
By default, the App Engine default service account has the Editor role in the project.
https://cloud.google.com/appengine/docs/standard/java11/audit-logging#audit_log_format
App Engine doesn't write Data Access audit logs or System Event audit logs.
Admin Activity audit logs and System Event audit logs are free.
Data Access audit logs and Policy Denied audit logs are chargeable.
https://cloud.google.com/appengine/docs/standard/java11/testing-and-deploying-your-app#other_deployment_options
https://cloud.google.com/appengine/docs/standard/java11/testing-and-deploying-your-app#testing-on-app-engine
https://cloud.google.com/appengine/docs/standard/java11/writing-application-logs
https://cloud.google.com/appengine/docs/standard/java11/storage-options
https://cloud.google.com/appengine/docs/standard/java11/using-cloud-datastore
Datastore mode automatically creates single-property indexes for use with simple types of queries.
For complex queries that include multiple properties, you'll need to configure composite indexes in your app's index.yaml file.
https://cloud.google.com/appengine/docs/standard/java11/migrating-traffic
https://cloud.google.com/appengine/docs/standard/java11/splitting-traffic
https://cloud.google.com/sdk/gcloud/reference/app/services/set-traffic
https://cloud.google.com/appengine/docs/standard/java11/scheduling-jobs-with-cron-yaml
https://cloud.google.com/appengine/docs/standard/java11/connecting-vpc

Cloud SQL:
https://cloud.google.com/sql/docs/mysql/create-instance
https://cloud.google.com/sql/docs/mysql/clone-instance
https://cloud.google.com/sql/docs/mysql/monitor-instance
https://cloud.google.com/sql/docs/mysql/delete-instance
You cannot delete an instance that has any replicas. You must delete all replicas first.
https://cloud.google.com/sql/docs/mysql/connect-overview
https://cloud.google.com/sql/docs/mysql/configure-ip
https://cloud.google.com/sql/docs/mysql/configure-private-ip
https://cloud.google.com/sql/docs/mysql/authorize-networks
Even if your instance has only a public IP address, you can connect to it securely by using the Cloud SQL Auth proxy.
All traffic between the Cloud SQL Auth proxy and your Cloud SQL instance is encrypted.
If you don't use the proxy, and you are connecting your client from your own public IP address,
you need to add your client's public address as an authorized network.
Your client application's IP address or address range must be configured as authorized networks for the following conditions:
1. Your client application is connecting directly to a Cloud SQL instance on its public IP address.
2. Your client application is connecting directly to a Cloud SQL instance on its private IP address, and your client's IP address is a non-RFC 1918 address
https://cloud.google.com/sql/docs/mysql/connect-admin-proxy
https://cloud.google.com/sql/docs/mysql/connect-admin-ip
https://cloud.google.com/sql/docs/mysql/connect-app-engine-standard
https://cloud.google.com/sql/docs/mysql/connect-kubernetes-engine
https://cloud.google.com/sql/docs/mysql/configure-ha
https://cloud.google.com/sql/docs/mysql/backup-recovery/backing-up
https://cloud.google.com/sql/docs/mysql/backup-recovery/restoring
https://cloud.google.com/sql/docs/mysql/backup-recovery/pitr
https://cloud.google.com/sql/docs/mysql/replication/create-replica
https://cloud.google.com/sql/docs/mysql/replication/cross-region-replicas
There are two common scenarios for promoting cross-region replicas:
1. Regional migration: Perform a planned migration of a database to a different region.
2. Disaster recovery: Fail over a database to another region in the event that the primary's region becomes unavailable.
Note: Promoting a replica is done manually and intentionally.
It is not the same as high availability,
where a standby instance (which is not a replica) automatically becomes the primary in case of a failure or zonal outage.
https://cloud.google.com/sql/docs/mysql/import-export/importing
https://cloud.google.com/sql/docs/mysql/import-export/exporting
https://cloud.google.com/sql/docs/mysql/create-manage-databases
https://cloud.google.com/sql/docs/mysql/create-manage-users
Additional material: https://cloud.google.com/sql/docs/mysql/high-availability
Additional material: https://cloud.google.com/sql/docs/mysql/replication
