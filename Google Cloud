Ultimatix Knowmax learning:
In Knowmax -> Organization units -> Banking and Technology Services -> Services -> Google Business Unit -> GSkool option, there learning materials.

O'reilly learning (Access from ultimatix):
https://learning.oreilly.com/library/view/official-google-cloud/9781119564416/

LinkedIn links for tutorials:
https://www.linkedin.com/learning/google-cloud-platform-for-enterprise-essential-training/enterprise-ready-gcp?u=2154233
https://www.linkedin.com/learning/google-cloud-platform-essential-training-3/hosting-your-application-on-google-cloud-platform?u=2154233

Github repos:
https://github.com/lynnlangit/gcp-essentials
https://github.com/lynnlangit/gcp-ml

https://cloud.google.com/pricing

https://medium.com/google-cloud/gcpcomics/home

https://cloud.google.com/docs

https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations

https://cloud.google.com/terms/services

https://developers.google.com/community/experts/directory

https://cloud.google.com/identity/docs/overview

https://cloud.google.com/resource-manager/docs/cloud-platform-resource-hierarchy

https://cloud.google.com/iam/docs/resource-hierarchy-access-control
https://cloud.google.com/iam/docs/job-functions/networking

https://cloud.google.com/billing/docs/concepts

https://cloud.google.com/files/Lift-and-Shift-onto-Google-Cloud.pdf
https://cloud.google.com/solutions/application-migration
https://cloud.google.com/solutions/migrating-vms-migrate-for-compute-engine-getting-started

https://cloud.google.com/compute/docs/images/image-management-best-practices

https://cloud.google.com/migrate/compute-engine/docs/4.11/concepts/architecture/gcp-reference-architecture

https://cloud.google.com/anthos/docs/concepts/overview
https://cloud.google.com/anthos/docs/tutorials/explore-anthos

https://cloud.google.com/kuberun/docs/setup

https://cloud.google.com/solutions/dr-scenarios-planning-guide

https://cloud.google.com/secret-manager/docs/overview
https://cloud.google.com/solutions/dr-scenarios-for-data
https://cloud.google.com/solutions/bigquery-data-warehouse
https://cloud.google.com/billing/docs/how-to/visualize-data
https://cloud.google.com/bigquery-ml/docs/introduction

https://cloud.google.com/data-fusion/docs

google Cloud dataflow uses apache beam and cloud dataproc uses apache hadoop/spark and cloud composer uses apache airflow.

https://cloud.google.com/solutions/build-a-data-lake-on-gcp

https://airflow.apache.org/docs/apache-airflow/stable/ui.html

https://cloud.google.com/life-sciences/docs/how-tos/variant-transforms
https://cloud.google.com/life-sciences/docs/quickstart
https://cloud.google.com/life-sciences/docs/resources/public-datasets
https://ai.googleblog.com/2017/12/deepvariant-highly-accurate-genomes.html

https://console.cloud.google.com/marketplace
https://cloud.google.com/foundation-toolkit

https://cloud.google.com/build/docs/overview

Difference between serverless and containers: https://www.cloudflare.com/learning/serverless/serverless-vs-containers/

https://codelabs.developers.google.com/codelabs/cloud-builder-gke-continuous-deploy/index.html#0
https://cloud.google.com/build/docs/cloud-builders

https://cloud.google.com/compute/quotas

https://cloud.google.com/products/tools

https://github.com/googlecloudplatform

https://cloud.google.com/sdk/docs/quickstart

https://cloud.google.com/docs/compare/aws

https://cloud.google.com/storage/docs/storage-classes

https://cloud.google.com/datastore/docs/firestore-or-datastore

https://datastudio.google.com/navigation/reporting

https://cloud.google.com/code

https://cloud.google.com/serverless/whitepaper

https://medium.com/google-cloud/about

https://cloud.google.com/appengine/docs/flexible
https://cloud.google.com/appengine/docs/standard

https://cloud.google.com/blog/products/bigquery/bigquery-under-the-hood

https://cloud.google.com/network-connectivity/docs/interconnect/concepts/dedicated-overview
https://cloud.google.com/network-connectivity/docs/interconnect/concepts/partner-overview

https://cloud.google.com/logging/docs/export/configure_export_v2


https://www.youtube.com/watch?v=r7ce2yqdckk
https://www.coursera.org/lecture/logging-monitoring-observability-google-cloud/exporting-and-analyzing-logs-peXBz

https://cloud.google.com/compute/docs/disks/snapshots
https://cloud.google.com/compute/docs/regions-zones/global-regional-zonal-resources#globalresources

https://cloud.google.com/appengine/docs/admin-api/migrating-splitting-traffic

Compute Engine:
https://cloud.google.com/compute/docs/instances/create-start-instance
https://cloud.google.com/compute/docs/instances/create-start-preemptible-instance#handle_preemption
Do not use regional persistent disks for boot disks. In a failover situation, they do not force-attach to a VM.
https://cloud.google.com/compute/docs/disks/detach-reattach-boot-disk
https://cloud.google.com/compute/docs/instances/windows/creating-managing-windows-instances
Windows instances experience a longer startup time because of the sysprep process. 
The Cloud Console might show that the instance is running even if the sysprep process is not yet complete. 
To check if your instance has successfully started and is ready to be used, check the serial port output with the following command:
gcloud compute instances get-serial-port-output [INSTANCE_NAME]
https://cloud.google.com/compute/docs/instances/windows/creating-passwords-for-windows-instances
https://cloud.google.com/compute/docs/instances/create-vm-from-instance-template
https://cloud.google.com/compute/docs/instances/create-vm-from-similar-instance
https://cloud.google.com/compute/docs/instances/custom-hostname-vm
https://cloud.google.com/compute/docs/instances/ssh#metadata-managed_ssh_connections
https://cloud.google.com/compute/docs/instances/connecting-advanced#provide-key
(Recommended) Enable OS Login.
(Not recommended) Manually add and remove SSH keys by editing project or instance metadata.
https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys#risks
By creating and managing SSH keys, you can let users access a Linux instance through third-party tools.
An SSH key consists of the following files:
1) A public SSH key file that is applied to instance-level metadata or project-wide metadata.
2) A private SSH key file that the user stores on their local devices.
If a user presents their private SSH key, they can use a third-party tool to connect to any instance that is configured with the matching public SSH key file,
even if they aren't a member of your Google Cloud project.
Therefore, you can control which instances a user can access by changing the public SSH key metadata for one or more instances.
https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys#project-wide
Use project-wide public SSH keys to give users general access to a Linux instance.
Project-wide public SSH keys give users access to all of the Linux instances in a project that allow project-wide public SSH keys.
If an instance blocks project-wide public SSH keys,
a user can't use their project-wide public SSH key to connect to the instance unless the same public SSH key is also added to instance metadata.
https://cloud.google.com/compute/docs/instances/managing-instance-access#connect
Grant one of the following instance access roles.
1. roles/compute.osLogin, which doesn't grant administrator permissions
2. roles/compute.osAdminLogin, which grants administrator permissions
https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys
https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys#edit-ssh-metadata
https://cloud.google.com/compute/docs/instances/connecting-advanced#sshbetweeninstances
https://cloud.google.com/compute/docs/disks/add-persistent-disk
https://cloud.google.com/compute/docs/disks/repd-failover
https://cloud.google.com/compute/docs/disks/working-with-persistent-disks#resize_partitions
1)Boot disk: VMs using public images automatically resize the root partition and file system after you've resized the boot disk on the VM and restarted the VM.
2)Non-boot disk: After resizing the disk, you must extend the file system on the disk to use the added space.
https://cloud.google.com/compute/docs/disks/working-with-persistent-disks#disk_type
You can change the type of your persistent disk using snapshots.
For example, to change your standard persistent disk to an SSD persistent disk, use the following process:
1) Create a snapshot of your standard persistent disk.
2) Create a new persistent disk based on the snapshot. Include the --type flag and specify pd-ssd.
https://cloud.google.com/compute/docs/disks/regional-persistent-disk#restrictions
Restrictions
1) You cannot use regional persistent disks as boot disks.
2) You can create a regional persistent disk from a snapshot but not an image.
https://cloud.google.com/compute/docs/disks/regional-persistent-disk#resize_repd
You can resize disks at any time, regardless of whether the disk is attached to a running VM.
You can attach a non-boot persistent disk to more than one VM in read-only mode, which lets you share static data between multiple VMs.
https://cloud.google.com/compute/docs/disks/regional-persistent-disk#use_multi_instances
https://cloud.google.com/compute/docs/disks/regional-persistent-disk#migrate-repd
https://cloud.google.com/compute/docs/disks/create-disk-from-source
You can create persistent disks from the following data sources:
1) Existing disks: Clone an existing persistent disk. Use this option if you need an instantly attachable copy of an existing non-boot persistent disk.
2) Snapshots: Create a non-boot disk from a source snapshot. Use this option to restore data from a persistent disk that you've backed up using snapshots.
3) Images: Create a boot disk from a source image. Use this option to create a boot disk for a new VM or to create a standalone boot persistent disk.
https://cloud.google.com/compute/docs/disks/create-disk-from-source#clone
Restrictions
1) The zone, region, and disk type of the clone must be the same as that of the source disk.
2) You cannot create a zonal disk clone from a regional disk.
https://cloud.google.com/compute/docs/disks/create-snapshots
You can create snapshots from disks even while they are attached to running instances.
Snapshots are global resources, so you can use them to restore data to a new disk or instance within the same project.
You can also share snapshots across projects.
https://cloud.google.com/compute/docs/disks/create-snapshots#restore-snapshots
Compute Engine uses incremental snapshots so that each snapshot contains only the data that has changed since the previous snapshot.
https://cloud.google.com/compute/docs/disks/scheduled-snapshots
Restrictions:
1) A persistent disk can have only one snapshot schedule attached to it at a time.
2) You cannot delete a snapshot schedule if it is attached to a disk. You must detach the schedule from all disks, then delete the schedule.
https://cloud.google.com/compute/docs/disks/scheduled-snapshots#deletion_rule
https://cloud.google.com/compute/docs/disks/scheduled-snapshots#change_snapshot_schedule
https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images
Custom images are ideal for situations where you have created and modified a persistent boot disk 
or specific image to a certain state and need to save that state for creating VMs.
Create the image
You can create disk images from the following sources:
1) A persistent disk, even while that disk is attached to a VM
2) A snapshot of a persistent disk
3) Another image in your project
https://cloud.google.com/compute/docs/instances/startup-scripts/linux
Startup scripts can apply to all VMs in a project or to a single VM.
Startup scripts specified by VM-level metadata override startup scripts specified by project-level metadata.
https://cloud.google.com/compute/docs/shutdownscript
Create and run shutdown scripts that execute commands right before a virtual machine (VM) instance is stopped or restarted.
Shutdown scripts have a limited amount of time to finish running before the instance stops:
1. On-demand instances: 90 seconds after you stop or delete an instance
2. Preemptible instances: 30 seconds after instance preemption begins
https://cloud.google.com/compute/docs/shutdownscript#shutdown_actions
The shutdown script won't run if the instance is reset using instances().reset.
https://cloud.google.com/compute/docs/instances/stop-start-instance
A stopped VM retains its persistent disks, its internal IPs, and its MAC addresses. However, the VM shuts down the guest OS and loses its application state.
If you need to retain the guest OS and application state, suspend the VM instead.
You cannot stop and restart a VM with a local SSD attached. The data residing in local ssd, survives a VM reset but not VM stop or terminate.
Performing a reset on your VM is similar to doing a hard reset on your computer, where you might press a reset button or press and hold the power button.
Resetting a VM forcibly wipes the memory contents of the machine and resets the VM to its initial state.
The VM does not perform a clean shutdown of the guest OS.
Snapshot is not available for local SSDs.
https://cloud.google.com/compute/docs/instances/deleting-instance
https://cloud.google.com/compute/docs/instances/update-instance-properties
https://cloud.google.com/compute/docs/instances/moving-instance-across-zones
External IP addresses are regional resources.
Move your instance manually using the following steps:
1. Create snapshots of persistent disks attached to the original instance.
2. Create copies of the persistent disks in the destination zone.
3. For external and internal IP addresses:
    a. If you are moving an instance across zones within the same region, and you want to preserve its ephemeral IP address,
    temporarily promote the ephemeral IP address that is assigned to the instance to a static IP address
    and then assign it to the new VM instance you create in the destination zone.
    b. If you are moving an instance across regions, you must pick a different IP address for the VM instance.
4. Create and boot up a new instance in the destination zone. If you are moving across regions, you must also pick a new subnetwork for the new instance.
5. Attach the new persistent disks to your new instance.
6. Assign an external IP address to the new instance. If necessary, demote the address back to an ephemeral external IP address.
7. Delete the snapshots, original disks, and original instance.
https://cloud.google.com/compute/docs/instances/moving-instance-across-zones#limitations
https://cloud.google.com/compute/docs/instances/copy-vm-between-projects
https://cloud.google.com/compute/docs/instances/changing-machine-type-of-stopped-instance
If your VM does not have a local SSD and is not part of a managed instance group (MIG), you can change the machine type of your VM after stopping it.
https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances
A virtual machine instance can only have one service account identity.
https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#clientlib
Client libraries can use Application Default Credentials to authenticate with Google APIs and send requests to those APIs.
https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#best_practices
The default compute engine service account is added as project editor IAM role.
In general, Google recommends that each instance that needs to call a Google API should run as a service account with the minimum permissions necessary for that instance 
to do its job. In practice, this means you should configure service accounts for your instances with the following process:
1. Create a new service account rather than using the Compute Engine default service account.
2. Grant IAM roles to that service account for only the resources that it needs.
3. Configure the instance to run as that service account.
https://cloud.google.com/docs/authentication/production#automatically
Google Cloud Client Libraries use a library called Application Default Credentials (ADC) to automatically find your service account credentials.
ADC looks for service account credentials in the following order:
1)If the environment variable GOOGLE_APPLICATION_CREDENTIALS is set, ADC uses the service account key or configuration file that the variable points to.
2)If the environment variable GOOGLE_APPLICATION_CREDENTIALS isn't set, ADC uses the service account that is attached to the resource that is running your code.
This service account might be a default service account provided by Compute Engine, Google Kubernetes Engine, App Engine, Cloud Run, or Cloud Functions.
It might also be a user-managed service account that you created.
3)If ADC can't use any of the above credentials, an error occurs.
https://cloud.google.com/compute/docs/instances/setting-instance-scheduling-options
You can't change the availability policies for a preemptible VM.
When there is a maintenance event, the preemptible VM stops and it does not migrate. You must manually restart the preempted VM.
https://cloud.google.com/compute/docs/instances/setting-instance-scheduling-options#settingoptions
https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances
You can create regional MIGs or zonal MIGs.
You must select which zones are associated with a regional MIG when you create the regional MIG.
After you choose specific zones during creation, you cannot change or update the zones later.
1.  To select more than three zones within a region, you must explicitly specify the individual zones.
    For example, to select all four zones within a region, you must provide all four zones explicitly in your request.
    If you do not, Compute Engine selects three zones by default.
2.  To select two or fewer zones in a region, you must explicitly specify the individual zones.
    Even if the region only contains two zones, you must still explicitly specify the zones in your request.
https://cloud.google.com/compute/docs/instance-groups/regional-migs#autoscaling_a_regional_mig
An autoscaling policy is applied to the group as a whole.
For example, if you enable autoscaler to target 66% CPU utilization,
the autoscaler tracks all instances in the group to maintain an average 66% utilization across all instances in all zones.
To add more instances to a MIG, you can:
1. Manually set the size of the MIG.
2. Use autoscaling for stateless applications.
The autoscaler only adds instances to a zone up to 1/n of the specified maximum for the group, where n is the number of provisioned zones.
For example, if you are using the default of 3 zones, and if 15 is the maxNumReplicas configured for autoscaling,
the autoscaler can only add up to 1/3 * 15 = 5 instances per zone for the group.
If one zone fails, the autoscaler only scales out to 2/3 of the maxNumReplicas in the remaining two zones combined.
https://cloud.google.com/compute/docs/instance-groups/autohealing-instances-in-migs
An autohealing policy relies on an application-based health check to verify that an application is responding as expected.
Checking that an application responds is more precise than simply verifying that a VM is in a RUNNING state.
If the autohealer determines that an application isn't responding, the MIG automatically recreates that VM.
You can set a maximum of one autohealing policy per MIG.
https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address
https://cloud.google.com/compute/docs/autoscaler/
Autoscaling only works with zonal and regional managed instance groups (MIGs). Unmanaged instance groups are not supported.
You cannot use autoscaling if your MIG has stateful configuration.
Autoscaling works independently from autohealing.
https://cloud.google.com/compute/docs/autoscaler#autoscaling_policy
https://cloud.google.com/compute/docs/autoscaler/multiple-signals#overview
https://cloud.google.com/compute/docs/autoscaler/scaling-cpu
https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#starting_a_basic_rolling_update
https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#max_surge
https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#max_unavailable
If you do not want any unavailable machines during an update, set the maxUnavailable value to 0 and the maxSurge value to greater than 0.
With these settings, Compute Engine removes each old machine only after its replacement new machine is created and running.
https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#canary_updates
https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#rolling_back_an_update
There is no explicit command for rolling back an update to a previous version, but if you decide to roll back an update
(either a fully committed update or a canary update), you can do so by making a new update request and passing in the instance template that you want to roll back to.
https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#performing_a_rolling_replace_or_restart
https://cloud.google.com/compute/docs/startupscript
probably, you can only create boot disk from an image and not a snapshot. please verify.
Difference between image and snapshot:
https://stackoverflow.com/a/29634892
https://www.quora.com/What-is-the-difference-between-an-image-and-a-snapshot-on-Google-Cloud
https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images
The VM images for Compute Engine and Amazon Elastic Compute Cloud (EC2) don't include the Logging agent,so you must complete these steps to install it on those instances.
The agent runs under both Linux and Windows.
If your VMs are running in Google Kubernetes Engine or App Engine, the agent is already included in the VM image.
https://cloud.google.com/logging/docs/agent/logging/installation
Note: Compute engine instance does not know the external IP address, but only knows the internal IP address.
External to internal IP address mapping is managed separately.
Note: If an instance is stopped, any ephemeral external IP addresses assigned to the instance are released back into the general Compute Engine pool
and become available for use by other projects.
When a stopped instance is started again, a new ephemeral external IP address is assigned to the instance.
Note: When an instance is stopped, you are not charged for the CPU or memory, rather you are charged for any attached disks or IPs.
Note: Static regional external IP addresses are regional resources that are used by instances that are in the same region as the address.
Although the regional external IP address is accessed by clients anywhere in the globe.


Load Balancers:
https://cloud.google.com/load-balancing/docs/choosing-load-balancer (Choosing a load balancer)
External TCP/UDP network load balancer & Internal TCP/UDP loadbalancer are passthrough.
All others are proxy-based.
Internal HTTP(S), Internal TCP/UDP and External TCP/UDP network load balancers are regional load balancers.
All others are global.
Global HTTP(S), SSL proxy and TCP proxy load balancers can receive IPv4/IPv6 traffic.
Whereas Network TCP/UDP and Internal load balancers can receive only IPv4 traffic.
https://cloud.google.com/load-balancing/docs/https (External HTTP(S) load balancing)
https://cloud.google.com/load-balancing/docs/https#component
https://cloud.google.com/load-balancing/docs/https#source_ip_addresses_for_client_packets
https://cloud.google.com/load-balancing/docs/https/ext-https-lb-simple#update_dns
https://cloud.google.com/load-balancing/docs/l7-internal (Internal HTTP(S) load balancing)
Only clients that are located in the same region as the load balancer can access this IP address. 
Internal client requests stay internal to your network and region.
https://cloud.google.com/kubernetes-engine/docs/how-to/internal-load-balance-ingress
An Ingress for Internal HTTP(S) Load Balancing requires the following annotation:
annotations:
    kubernetes.io/ingress.class: "gce-internal"
https://cloud.google.com/load-balancing/docs/ssl (External SSl (TCP traffic with SSL offload) proxy load balancing)
https://cloud.google.com/load-balancing/docs/tcp (External TCP proxy load balancing)
https://cloud.google.com/load-balancing/docs/network/networklb-backend-service (External TCP/UDP network load balancers)
With the Premium Tier, a load balancer can be configured as a global load balancing service.
With Standard Tier, a load balancer handles load balancing regionally.
External IP addresses can be global or regional.
You can also use HTTP(S) Load Balancing with Cloud Storage buckets.
https://cloud.google.com/load-balancing/docs/https#load_balancing_using_multiple_backend_types
External HTTP(S) Load Balancing supports the following backend types:
1. Instance groups
2. Zonal network endpoint groups (NEGs)
3. Serverless NEGs: One or more App Engine, Cloud Run, or Cloud Functions services
4. Internet NEGs, for endpoints that are outside of Google Cloud (also known as custom origins)
5. Buckets in Cloud Storage
https://cloud.google.com/load-balancing/docs/https/ext-load-balancer-backend-buckets
Provide public access to a cloud storage bucket to use it as a backend for an external HTTP(S) load balancer.
Backend buckets are only supported with global external HTTP(S) load balancers and global external HTTP(S) load balancer (classic).
They aren't supported by regional external HTTP(S) load balancer or any other load balancer type.
https://cloud.google.com/load-balancing/docs/negs/serverless-neg-concepts

CLOUD CDN:
https://cloud.google.com/cdn/docs/overview

Cloud Monitoring:
https://cloud.google.com/monitoring/quickstart-lamp
https://cloud.google.com/monitoring/settings/multiple-projects
Using the Monitoring agent is optional but recommended.
Monitoring can access some instance metrics without the Monitoring agent, including CPU utilization, some disk traffic metrics,
network traffic, and uptime information
https://cloud.google.com/monitoring/agent/monitoring

Kubernetes:
REVIEW THIS QWIKLABS COURSE FOR AN IN-DEPTH KNOWLEDGE ON KUBERNETES AND GKE: https://googlecourses.qwiklabs.com/course_templates/34
https://cloud.google.com/kubernetes-engine/docs
https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster
1.  A single-zone cluster has a single control plane running in one zone. This control plane manages workloads on nodes running in the same zone.
2.  A multi-zonal cluster's nodes run in multiple zones, but it has only a single replica of the control plane.
    If you need higher availability for the control plane, consider creating a regional cluster instead.
    In a regional cluster, the control plane is replicated across multiple zones in a region.
For private clusters:
Internal IP addresses for nodes come from the primary IP address range of the subnet you choose for the cluster.
Pod IP addresses and Service IP addresses come from two subnet secondary IP address ranges of that same subnet.
https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-admin-overview#node_configuration
You can create one or more node pools; node pools are groups of nodes within your cluster that share a common configuration.
Your cluster must have at least one node pool, and a node pool called default is created when you create the cluster.
https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-usage-metering
GKE usage metering tracks information about the resource requests and actual resource usage of your cluster's workloads.
Currently, GKE usage metering tracks information about CPU, GPU, TPU, memory, storage, and optionally network egress.
You can differentiate resource usage using Kubernetes Namespaces, labels, or a combination of both.
Data is stored in BigQuery, where you can query it directly or export it for analysis using external tools such as Google Data Studio.
GKE usage metering is not enabled by default. You have to enable it on a particular cluster (existing or new).
https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl
If you run multiple clusters within your Google Cloud project, you need to choose which cluster kubectl talks to.
You can set a default cluster for kubectl by setting the current context in Kubernetes' kubeconfig file.
Additionally, you can run kubectl commands against a specific cluster using the --cluster flag.
https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl#cluster_endpoint
https://cloud.google.com/kubernetes-engine/docs/how-to/upgrading-a-cluster
By default, automatic upgrades are enabled for Google Kubernetes Engine (GKE) clusters and node pools.
Control planes are upgraded on a regular basis, and cannot be disabled.
However, you can apply maintenance windows and exclusions to temporarily suspend upgrades for control planes and nodes.
https://cloud.google.com/kubernetes-engine/docs/how-to/node-pools#deploy
Kubernetes uses the Deployment controller to deploy stateless applications
Kubernetes uses the StatefulSets to deploy stateful applications
https://cloud.google.com/kubernetes-engine/docs/how-to/stateless-apps#update
https://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips#cluster_sizing
Pod IP address ranges, and subnet secondary IP address ranges in general,
are accessible from on-premises networks connected with Cloud VPN or Cloud Interconnect using Cloud Routers.
When you create a VPC-native cluster, you specify a subnet in a VPC network. The cluster uses three unique subnet IP address ranges:
1. It uses the subnet's primary IP address range for all node IP addresses.
2. It uses one secondary IP address range for all Pod IP addresses.
3. It uses another secondary IP address range for all Service (cluster IP) addresses.
https://cloud.google.com/kubernetes-engine/docs/how-to/exposing-apps
A service of type LoadBalancer is an external TCP/UDP network loadbalancer.
A kubernetes ingress object creates an external HTTP(S) Load Balancer.
https://cloud.google.com/kubernetes-engine/docs/how-to/load-balance-ingress
An Ingress for Internal HTTP(S) Load Balancing requires the following annotation:
annotations:
    kubernetes.io/ingress.class: "gce-internal"
https://cloud.google.com/kubernetes-engine/docs/how-to/internal-load-balance-ingress
Kubernetes RBAC(role-based access control) is a core component of Kubernetes and lets you create and grant roles (sets of permissions)
for any object or type of object within the cluster.
https://cloud.google.com/kubernetes-engine/docs/how-to/internal-load-balancing
https://cloud.google.com/kubernetes-engine/docs/concepts/pod
By default, Pods run on nodes in the default node pool for the cluster. You can configure the node pool a Pod selects explicitly or implicitly:
1.  You can explicitly force a Pod to deploy to a specific node pool by setting a nodeSelector in the Pod manifest.
    This forces a Pod to run only on Nodes in that node pool.
2.  You can specify resource requests for the containers you run.
    The Pod will only run on nodes that satisfy the resource requests.
    For instance, if the Pod definition includes a container that requires four CPUs, the Service will not select Pods running on Nodes with two CPUs.
https://cloud.google.com/kubernetes-engine/docs/concepts/daemonset
DaemonSets attempt to adhere to a one-Pod-per-node model, either across the entire cluster or a subset of nodes.
As you add nodes to a node pool, DaemonSets automatically add Pods to the new nodes as needed.
Examples of such tasks include storage daemons like ceph, log collection daemons like fluent-bit, and node monitoring daemons like collectd.
https://cloud.google.com/kubernetes-engine/docs/concepts/configmap
ConfigMaps bind configuration files, command-line arguments, environment variables, port numbers, and other configuration artifacts to your Pods' containers
and system components at runtime.
ConfigMaps enable you to separate your configurations from your Pods and components, which helps keep your workloads portable.
This makes their configurations easier to change and manage, and prevents hardcoding configuration data to Pod specifications.
ConfigMaps are useful for storing and sharing non-sensitive, unencrypted configuration information.
To use sensitive information in your clusters, you must use Secrets.
https://cloud.google.com/kubernetes-engine/docs/concepts/volumes#types_of_volumes
https://cloud.google.com/kubernetes-engine/docs/concepts/types-of-clusters
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler
https://cloud.google.com/kubernetes-engine/docs/concepts/deployment
https://cloud.google.com/kubernetes-engine/docs/how-to/scaling-apps#autoscaling-deployments
https://cloud.google.com/kubernetes-engine/docs/concepts/service
https://kubernetes.io/docs/concepts/services-networking/service/
https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials
https://cloud.google.com/stackdriver/docs/solutions/gke
https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs
Kubernetes containers collect logs for your workloads, written to STDOUT and STDERR.
https://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine
https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools
Note: ClusterIP service creates a static common IP for the backend pods and clusterIP can only be accessed within the cluster and not externally.


BigQuery:
https://cloud.google.com/bigquery/docs/estimate-costs#bq
https://cloud.google.com/bigquery/docs/bq-command-line-tool#setting_default_values_for_command-line_flags
You can set default values for command-line flags by including them in the bq command-line tool's configuration file, .bigqueryrc
https://cloud.google.com/bigquery/docs/jobs-overview
https://cloud.google.com/bigquery/docs/managing-jobs#bq
Jobs are actions that BigQuery runs on your behalf to load data, export data, query data, or copy data.
https://cloud.google.com/bigquery/docs/datasets-intro#dataset_limitations
https://cloud.google.com/bigquery/docs/locations#move-dataset
You specify a location for storing your BigQuery data when you create a dataset. After you create the dataset, the location cannot be changed.
BigQuery processes queries in the same location as the dataset that contains the tables you're querying.
The BigQuery Data Transfer Service transfers (copies) data from a source to a destination dataset in BigQuery.
https://cloud.google.com/bigquery-transfer/docs/locations
Transfers from Cloud Storage to BigQuery require that the Cloud Storage bucket be colocated with the BigQuery destination dataset.
https://cloud.google.com/bigquery/docs/datasets#bq
https://cloud.google.com/bigquery/docs/copying-datasets
Dataset copying uses features of the BigQuery Data Transfer Service.
Datasets can be copied from region to region, single region to multi-region, multi-region to single region, or multi-region to multi-region.
https://cloud.google.com/bigquery/docs/dataset-access-controls
https://cloud.google.com/bigquery/docs/dataset-access-controls#granting_access_to_a_dataset
https://cloud.google.com/bigquery/docs/listing-datasets
https://cloud.google.com/bigquery/docs/dataset-metadata#information_schema
INFORMATION_SCHEMA is a series of views that provide access to metadata about datasets, routines, tables, views, jobs, reservations, and streaming data.
https://cloud.google.com/bigquery/docs/updating-datasets
If you do not set a default table expiration at the dataset level, and you do not set a table expiration when the table is created,
the table never expires and you must delete the table manually.
If both options are set, the default partition expiration overrides the default table expiration.
https://cloud.google.com/bigquery/docs/updating-datasets#table-expiration
https://cloud.google.com/bigquery/docs/managing-datasets
https://cloud.google.com/bigquery/docs/tables
When you load data into BigQuery, you can load data into a new table or partition, you can append data to an existing table or partition, or you can overwrite a table or partition.
You do not need to create an empty table before loading data into it. You can create the new table and load your data at the same time.
https://cloud.google.com/bigquery/docs/tables#table-acl
You can control access to bigquery in column and row level.
https://cloud.google.com/bigquery/docs/tables#getting_table_information_using_information_schema_beta
You can query the INFORMATION_SCHEMA.TABLES and INFORMATION_SCHEMA.TABLE_OPTIONS views to retrieve metadata about tables and views in a project.
https://cloud.google.com/bigquery/docs/table-access-controls#bq
https://cloud.google.com/bigquery/docs/managing-tables
https://cloud.google.com/bigquery/docs/exporting-data#bq
You can undelete a table within seven days of deletion, including explicit deletions and implicit deletions due to table expiration.
You cannot export table data to a local file, to Sheets, or to Drive. The only supported export location is Cloud Storage.
You cannot change the location of a dataset after it is created, but you can make a copy of the dataset.
https://cloud.google.com/bigquery/docs/partitioned-tables
By dividing a large table into smaller partitions, you can improve query performance,
and you can control costs by reducing the number of bytes read by a query.
You can get information about partitioned tables using the INFORMATION_SCHEMA.PARTITIONS view.
https://cloud.google.com/bigquery/docs/views
https://cloud.google.com/bigquery/docs/view-metadata#views_view
https://cloud.google.com/bigquery/docs/adding-labels
https://cloud.google.com/bigquery/docs/loading-data#loading_data_from_other_google_services
https://cloud.google.com/bigquery/docs/loading-data#alternatives_to_loading_data
https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json
https://cloud.google.com/bigquery/docs/batch-loading-data#data-locations
Exception: If your dataset is in the US multi-regional location, you can load data from a Cloud Storage bucket in any regional or multi-regional location.
https://cloud.google.com/bigquery/docs/query-overview#types_of_queries
After you load your data into BigQuery, you can query the data in your tables. BigQuery supports two types of queries:
1) Interactive queries
2) Batch queries
By default, BigQuery runs interactive queries, which means that the query is executed as soon as possible.
BigQuery also offers batch queries.
BigQuery queues each batch query on your behalf and starts the query as soon as idle resources are available, usually within a few minutes.
In BigQuery, compute is decoupled from storage, and they are designed to work together to organize the data to make queries efficient over huge datasets.
https://cloud.google.com/bigquery/docs/query-overview#query_jobs
https://cloud.google.com/bigquery/docs/running-queries#batch
https://cloud.google.com/bigquery/docs/dry-run-queries
https://cloud.google.com/bigquery/docs/custom-quotas
https://cloud.google.com/bigquery/docs/custom-quotas#example
https://cloud.google.com/bigquery/docs/monitoring#information_schema_views
https://cloud.google.com/bigquery/docs/best-practices-costs
Don't run queries to explore or preview table data.
If you are experimenting with or exploring your data, you can use table preview options to view data for free and without affecting quotas.
Use clustering and partitioning to reduce the amount of data scanned.
For non-clustered tables, not use a LIMIT clause as a method of cost control.
For non-clustered tables, applying a LIMIT clause to a query does not affect the amount of data that is read.
You are billed for reading all bytes in the entire table as indicated by the query, even though the query returns only a subset.
With a clustered table, a LIMIT clause can reduce the number of bytes scanned.
https://cloud.google.com/bigquery/docs/best-practices-costs#preview-data
In the bq command-line tool, use the bq head command and specify the number of rows to preview.
https://cloud.google.com/bigquery/docs/best-practices-costs#limit_query_costs_by_restricting_the_number_of_bytes_billed
You can limit the number of bytes billed for a query using the maximum bytes billed setting.
When you set maximum bytes billed, the number of bytes that the query will read is estimated before the query execution.
If the number of estimated bytes is beyond the limit, then the query fails without incurring a charge.

IAM:
https://cloud.google.com/iam/docs/creating-custom-roles
Custom roles can only be used at the project or organization levels. They can’t be used at the folder level.
https://cloud.google.com/iam/docs/creating-managing-service-account-keys
https://cloud.google.com/iam/docs/granting-changing-revoking-access
Cloud Storage IAM roles:
https://cloud.google.com/storage/docs/access-control/iam-roles#standard-roles

GCLOUD SDK AUTHORIZATION:
https://cloud.google.com/sdk/docs/authorizing
https://cloud.google.com/sdk/docs/initializing
https://cloud.google.com/sdk/docs/configurations
https://cloud.google.com/sdk/docs/properties
You can also set properties via environment variables. Each property has a corresponding environment variable that can be used to set it. 
The name of the environment variable follows the CLOUDSDK_SECTION_NAME_PROPERTY_NAME pattern. 
For example, you can set the core/project and compute/zone properties as follows:
CLOUDSDK_CORE_PROJECT=[YOUR_PROJECT_NAME]
CLOUDSDK_COMPUTE_ZONE=[YOUR_ZONE_NAME]

Google cloud storage:
https://cloud.google.com/storage/docs/creating-buckets
https://cloud.google.com/storage/docs/storage-classes#comparison_of_storage_classes
You can change the storage class of an existing object either by rewriting the object or by using Object Lifecycle Management.
Changing the default storage class of a bucket does not affect any of the objects that already exist in the bucket.
Archive Storage also has higher costs for data access and operations.
https://cloud.google.com/storage/docs/domain-name-verification
https://cloud.google.com/storage/docs/key-terms#geo-redundant
https://cloud.google.com/storage/docs/moving-buckets
Moving data between locations incurs network usage costs.
In addition, moving data between buckets may incur retrieval and early deletion fees,
if the data being moved are Nearline Storage, Coldline Storage, or Archive Storage objects.
The gsutil command-line tool uses resumable uploads in the gsutil cp and gsutil rsync commands when uploading data to Cloud Storage.
https://cloud.google.com/storage/docs/metadata#content-type
If the Content-Type is not specified by the uploader and cannot be determined, it is set to application/octet-stream or application/x-www-form-urlencoded.
https://cloud.google.com/storage/docs/access-control
If you use IAM and ACLs on the same resource, Cloud Storage grants the broader permission set on the resource.
For example, if your IAM permissions only allow a few users to access my-object, but your ACLs make my-object public,
then my-object is exposed to the public.
In general, IAM cannot detect permissions granted by ACLs, and ACLs cannot detect permissions granted by IAM.
https://cloud.google.com/storage/docs/changing-storage-classes#gsutil
This guide involves rewriting data, which is a Class A operation and which may incur retrieval and early deletion charges
if the data was originally stored as Nearline Storage, Coldline Storage, or Archive Storage.
https://cloud.google.com/storage/docs/lifecycle
If multiple rules have their conditions satisfied simultaneously for a single object,
Cloud Storage performs the action associated with only one of the rules, based on the following considerations:
1. The Delete action takes precedence over any SetStorageClass action.
2. The SetStorageClass action that switches the object to the storage class with the lowest at-rest storage pricing takes precedence.
For buckets in a region, the new storage class cannot be Multi-Regional Storage.
For buckets in a multi-region or dual-region, the new storage class cannot be Regional Storage.
https://cloud.google.com/storage/docs/lifecycle#setstorageclass-cost
For object lifecycle management, there are no retrieval fees or early deletion fees associated with the storage class change,
even when the object is originally set to Nearline Storage or Coldline Storage.
https://cloud.google.com/storage/docs/object-versioning
Cloud Storage retains a noncurrent object version each time you replace or delete a live object version,
as long as you do not specify the generation number of the live version.
Noncurrent versions retain the name of the object, but are uniquely identified by their generation number.
https://cloud.google.com/storage/docs/object-versioning#considerations
1. Object Versioning cannot be enabled on a bucket that currently has a retention policy.
2. There is no default limit on the number of object versions you can have.
    Each noncurrent version of an object is charged at the same rate as the live version of the object.
    If you enable versioning, consider using Object Lifecycle Management, which can remove the oldest versions of an object as newer versions become noncurrent.
https://cloud.google.com/storage/docs/object-versioning#example
https://cloud.google.com/storage/docs/using-object-versioning
https://cloud.google.com/storage/docs/using-versioned-objects
https://cloud.google.com/storage/docs/bucket-lock
Retention policies and Object Versioning are mutually exclusive features in Cloud Storage: for a given bucket, only one of these can be enabled at a time.
https://cloud.google.com/storage/docs/access-control/making-data-public
To make individual objects readable to everyone on the public internet:
gsutil acl ch -u AllUsers:R gs://BUCKET_NAME/OBJECT_NAME
To make all objects in a bucket readable to everyone on the public internet:
gsutil iam ch allUsers:objectViewer gs://BUCKET_NAME
https://cloud.google.com/storage/docs/reporting-changes
https://cloud.google.com/storage/docs/audit-logging
Cloud Audit Logs does not track access to public objects.
Cloud Audit Logs does not log actions taken by the Object Lifecycle Management feature.
To track the lifecycle management actions that Cloud Storage takes, use one of the following options:
1. Use Cloud Storage usage logs.
2. Enable Pub/Sub Notifications for Cloud Storage for your bucket.
https://cloud.google.com/storage/docs/lifecycle#islive
The IsLive condition is typically only used in conjunction with Object Versioning.
When set to false, this condition is satisfied for any noncurrent version of an object.
When set to true, this condition is satisfied for the live version of an object.
https://cloud.google.com/storage/docs/lifecycle#numberofnewerversions
The NumberOfNewerVersions condition is typically only used in conjunction with Object Versioning.
If the value of this condition is set to N, an object version satisfies the condition when there are at least N versions (including the live version) newer than it.
For a live object version, the number of newer versions is considered to be 0.
For the most recent noncurrent version, the number of newer versions is 1 (or 0 if there is no live object version), and so on.
https://cloud.google.com/storage/docs/lifecycle#setstorageclass-cost
Unlike changing an object's storage class manually(by using rewrite), using SetStorageClass does not rewrite an object.
This gives Object Lifecycle Management certain pricing advantages:
1. There are no retrieval fees or early deletion fees associated with the storage class change,
    even when the object is originally set to Nearline Storage or Coldline Storage.
https://cloud.google.com/storage/docs/lifecycle#tracking
To track the lifecycle management actions that Cloud Storage takes, use one of the following options:
1. Use Cloud Storage usage logs. This feature logs both the action and who performed the action.
2. Enable Pub/Sub Notifications for Cloud Storage for your bucket. This feature sends notifications to a Pub/Sub topic of your choice when specified actions occur.
    Note that this feature does not record who performed the actions.
https://cloud.google.com/storage/docs/lifecycle#tracking
Cloud Audit Logs does not log actions taken by the Object Lifecycle Management feature.
Pub/Sub notifications for cloud storage tracks actions taken by Object Lifecycle Management.
https://cloud.google.com/storage/docs/managing-lifecycles
Lifecycle configurations are managed at the bucket level and apply to all current and future objects in the bucket.
https://cloud.google.com/storage/docs/access-control/iam
An IAM policy applied to your project defines the actions that users can take on all objects or buckets within your project.
An IAM policy applied to a single bucket defines the actions that users can take on that specific bucket and objects within it.
https://cloud.google.com/storage/docs/access-control/iam#project-level_roles_vs_bucket-level_roles
https://cloud.google.com/storage/docs/access-control/using-iam-permissions#gsutil_2
https://cloud.google.com/storage/docs/access-control/using-iam-permissions#conditions-iam
To set IAM Conditions on a bucket, you must first enable uniform bucket-level access on that bucket.
https://cloud.google.com/storage/docs/uniform-bucket-level-access
When you enable uniform bucket-level access on a bucket, Access Control Lists (ACLs) are disabled,
and only bucket-level Identity and Access Management (IAM) permissions grant access to that bucket and the objects it contains.
To support the ability to disable uniform bucket-level access and revert to using ACLs, Cloud Storage saves existing ACLs for 90 days.
If you disable uniform bucket-level access during this time:
1. Objects regain their saved ACLs.
https://cloud.google.com/storage/docs/encryption
https://cloud.google.com/storage/docs/encryption/using-customer-managed-keys
https://cloud.google.com/storage/docs/encryption/customer-supplied-keys#restrictions
https://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#storage-generate-encryption-key-java
https://cloud.google.com/storage/docs/pubsub-notifications
https://cloud.google.com/storage/docs/pubsub-notifications#events
https://cloud.google.com/storage/docs/access-control/signed-urls
Signed URLs contain authentication information in their query string, allowing users without credentials to perform specific actions on a resource.
When you generate a signed URL, you specify a user or service account which must have sufficient permission to make the request that the signed URL will make.
https://cloud.google.com/storage/docs/access-control/signing-urls-with-helpers
https://cloud.google.com/storage/docs/gsutil/commands/signurl#usage
https://cloud.google.com/storage/docs/caching
To use Cloud CDN, you must use external HTTP(S) Load Balancing with your Cloud Storage buckets as a backend.
Cloud CDN uses the Cache-Control metadata set on your objects to determine how they should be cached.
https://cloud.google.com/storage/docs/hosting-static-website
Note the IP address associated with the load balancer: for example, 30.90.80.100.
To point your domain to your load balancer, create an "A" record using your domain registration service.
https://cloud.google.com/storage/docs/hosting-static-website-http
To connect your domain to Cloud Storage, create a CNAME record through your domain registration service.

Storage Transfer Service:
https://cloud.google.com/storage-transfer/docs/overview
https://cloud.google.com/storage-transfer/docs/overview#gsutil
https://cloud.google.com/storage-transfer/docs/create-manage-transfer-console#amazon-s3
https://cloud.google.com/storage-transfer/docs/configure-access

Service Account:
https://cloud.google.com/iam/docs/service-accounts
https://www.preveil.com/blog/public-and-private-key/ (Explains the public and private key concept)
https://cloud.google.com/docs/authentication/production#automatically
If your application runs inside a Google Cloud environment, and you have attached a service account to that environment, 
your application can retrieve credentials for the service account. 
The application can then use the credentials to call Google Cloud APIs.
You can attach service accounts to resources for many different Google Cloud services, including Compute Engine, 
Google Kubernetes Engine, App Engine, Cloud Run, and Cloud Functions. 
We recommend using this strategy because it is more convenient and secure than manually passing credentials.
Google Cloud Client Libraries use a library called Application Default Credentials (ADC) to automatically find your service account credentials.

VPC:
Google Cloud Virtual Private Cloud (VPC) provides networking functionality to Compute Engine virtual machine (VM) instances,
Kubernetes Engine containers, and the App Engine flexible environment.
In other words, without a VPC network, you cannot create VM instances, containers, or App Engine applications.
Therefore, each Google Cloud project has a default network to get you started.
https://cloud.google.com/vpc/docs/vpc
https://cloud.google.com/vpc/docs/vpc#specifications
You can create more than one subnet per region.
A network must have at least one subnet before you can use it.
https://cloud.google.com/vpc/docs/vpc#subnet-ranges
When an auto mode VPC network is created, one subnet from each region is automatically created within it. 
These automatically created subnets use a set of predefined IP ranges that fit within the 10.128.0.0/9 CIDR block.
You can switch a VPC network from auto mode to custom mode. This is a one-way conversion. 
Custom mode VPC networks cannot be changed to auto mode VPC networks.
https://cloud.google.com/vpc/docs/vpc#default-network
You can disable the creation of default networks by creating an organization policy with the compute.skipDefaultNetworkCreation constraint.
Because the subnets of every auto mode VPC network use the same predefined range of IP addresses, 
you cannot connect auto mode VPC networks to one another.
Google recommends that you use custom mode VPC networks in production.
https://cloud.google.com/vpc/docs/vpc#system-generated-routes
If your VPC network is connected to an on-premises network by using Cloud VPN or Cloud Interconnect,
check that subnet ranges do not conflict with on-premises IP addresses.
https://cloud.google.com/vpc/docs/vpc#firewall_rules
Every VPC network has two implied firewall rules. One implied rule allows most egress traffic, and the other denies all ingress traffic.
You cannot delete the implied rules, but you can override them with your own.
To monitor which firewall rule allowed or denied a particular connection, see Firewall Rules Logging.
https://cloud.google.com/vpc/docs/vpc#intra_vpc_reqs
Except for the default network, you must explicitly create higher priority ingress firewall rules to allow instances to communicate with one another.
The default network includes several firewall rules in addition to the implied ones, including the default-allow-internal rule,
which permits instance-to-instance communication within the network.
The default network also comes with ingress rules allowing protocols such as RDP and SSH.
https://cloud.google.com/vpc/docs/vpc#internet_access_reqs
One of the following must be true:
1)The instance must have an external IP address. An external IP address can be assigned to an instance when it is created or after it has been created.
2)The instance must be able to use Cloud NAT or an instance-based proxy that is the target for a static 0.0.0.0/0 route.
https://cloud.google.com/vpc/docs/vpc#app-engine-comm (App Engine does not reside in VPC)
https://cloud.google.com/vpc/docs/shared-vpc
Shared VPC allows an organization to connect resources from multiple projects to a common Virtual Private Cloud (VPC) network, 
so that they can communicate with each other securely and efficiently using internal IPs from that network.
Shared VPC connects projects within the same organization. Participating host and service projects cannot belong to different organizations.
https://cloud.google.com/vpc/docs/vpc-peering
Google Cloud VPC Network Peering allows internal IP address connectivity across two Virtual Private Cloud (VPC) networks 
regardless of whether they belong to the same project or the same organization.
A given VPC network can peer with multiple VPC networks.
A subnet CIDR range in one peered VPC network cannot overlap with a static route in another peered network.
At the time of peering, Google Cloud checks to see if there are any subnet IP ranges that overlap subnet IP ranges in the other network.
If there is any overlap, peering is not established.
Shared VPC and VPC peering is a way to establish direct connection between resources residing on separate VPC networks using internal IP address.
Cloud VPN is another way to achieve the same, but Shared VPC/VPC peering is more suitable for this type of scenario and comes with low network latency and low cost.
Peering traffic (traffic flowing between peered networks) has the same latency, throughput, and availability as private traffic in the same network.
https://cloud.google.com/vpc/docs/alias-ip
If you have only one service running on a VM, you can reference it using the interface's primary IP address.
If you have multiple services running on a VM, you may want to assign each one a different internal IP address.
You can do this with Alias IP ranges.
Subnet overlap checks across peered networks ensure that primary and secondary ranges do not overlap with any peered ranges.
https://cloud.google.com/vpc/docs/firewall-rules-logging
You enable Firewall Rules Logging individually for each firewall rule whose connections you need to log.
Firewall Rules Logging only records TCP and UDP connections.
Changes to firewall rules can be viewed in VPC audit logs.
You cannot enable Firewall Rules Logging for the implied deny ingress and implied allow egress rules.
If the incoming request is allowed by an ingress rule, established responses cannot be blocked by any kind of egress rule.
https://cloud.google.com/vpc/docs/flow-logs
You enable or disable VPC Flow Logs per subnet.
VPC Flow Logs interacts with firewall rules in the following ways:
1. Egress packets are sampled before egress firewall rules. 
   Even if an egress firewall rule denies outbound packets, those packets can be sampled by VPC Flow Logs.
2. Ingress packets are sampled after ingress firewall rules. 
   If an ingress firewall rule denies inbound packets, those packets are not sampled by VPC Flow Logs.
https://cloud.google.com/vpc/docs/serverless-vpc-access
Serverless VPC Access enables you to connect from a serverless environment on Google Cloud directly to your VPC network.
This connection makes it possible for your serverless environment to access resources in your VPC network via internal IP addresses.
With Serverless VPC Access, you create a connector in your Google Cloud project and attach it to a VPC network.
You then configure your serverless services (such as Cloud Run services, App Engine apps, or Cloud Functions) 
to use the connector for internal network traffic.
The following Google services support Serverless VPC Access connectors:
1. Cloud Run
2. App Engine standard environment (All runtimes except PHP 5)
3. Cloud Functions
https://cloud.google.com/network-connectivity/docs/router/concepts/overview
Cloud Router isn't supported for Direct Peering or Carrier Peering connections.
When you extend your on-premises network to Google Cloud, 
use Cloud Router to dynamically exchange routes between your Google Cloud networks and your on-premises network. 
Cloud Router peers with your on-premises VPN gateway or router.
The routers exchange topology information through BGP.
Without Cloud Router, you can use only static routes to configure your Cloud VPN tunnels.
https://cloud.google.com/network-connectivity/docs/router/concepts/overview#regional-dyn-routing
https://cloud.google.com/network-connectivity/docs/router/concepts/overview#global_dynamic_routing_example
With global dynamic routing, Cloud Router has visibility to resources in all regions.
For example, if you have VMs in one region, they can reach a Cloud VPN tunnel in another region automatically without maintaining static routes.
Note: We recommend creating two Cloud Routers in each region for an Interconnect connection. 
      Creating two Cloud Routers is required for 99.99% availability.
https://cloud.google.com/network-connectivity/docs/vpn/concepts/classic-topologies
https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview
Traffic traveling between the two networks is encrypted by one VPN gateway and then decrypted by the other VPN gateway. 
This action protects your data as it travels over the internet.
https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview#ha-vpn
HA VPN is a high-availability (HA) Cloud VPN solution that lets you securely connect your on-premises network to your VPC network 
through an IPsec VPN connection in a single region.
When you create an HA VPN gateway, Google Cloud automatically chooses two external IP addresses, one for each of its fixed number of two interfaces.
To achieve high availability when both VPN gateways are located in VPC networks, 
you must use two HA VPN gateways, and both of them must be located in the same region.
In order to establish VPN connection between two separate networks, two VPN gateways (one for each network), and two VPN tunnels need to be established.
If both tunnels are not established, traffic might flow from one network to another but not vice-versa.
The traffic reach the remote server, but the response can't be returned.
Each vpn gateway needs an external IP.
https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview#comparison_table
https://cloud.google.com/vpc/docs/configure-private-google-access
Private google access can be enabled on a per subnet basis.
A VM with an external IP address assigned to its network interface doesn't need Private Google Access to connect to Google APIs and services.
The firewall configuration of your VPC network must allow access from VMs to the IP addresses used by Google APIs and services.
The implied allow egress rule satisfies this requirement.
https://cloud.google.com/vpc/docs/using-flow-logs
https://www.coursera.org/lecture/logging-monitoring-observability-google-cloud/vpc-flow-logs-XWUmD (Vpc flow logs)
https://cloud.google.com/vpc/docs/using-firewall-rules-logging (Firewall logs)
https://cloud.google.com/vpc/docs/audit-logging
https://cloud.google.com/vpc/docs/configure-serverless-vpc-access
(Serverless VPC Access enables you to connect from a serverless environment on Google Cloud (Cloud Run (fully managed), Cloud Functions,
or the App Engine standard environment) directly to your VPC network.
This connection makes it possible for your serverless environment to access Compute Engine VM instances, Memorystore instances,
and any other resources with an internal IP address.)
https://cloud.google.com/vpc/docs/configure-private-services-access
Note: If a VM, with only internal IP address (and no external IP address), wants to access google APIs and services (like google cloud storage),
then "private google access" has to be enabled in the corresponding subnet for those VMs.
This is not required for VMs with external IPs.
Note: Dedicated interconnect, partner interconnect and cloud vpn helps on-premise network to connect to the
"private" IP addresses of the gcp resources in gcp network. Whereas direct and carrier peering connects on premise network with the "public" IP addresses
of the gcp resources in gcp network by creating a direct connection with google PoP (Point of Presence).
Interconnect(including cloud VPN) comes with SLA whereas peering doesn't have any SLA.
Note: VPC peering can establish connection between two vpc networks regardless of whether those two networks reside
in the same or different project or organization.
Shared VPC cannot have a single vpc network that spans across multiple organizations.
Shared VPC can only span projects in the same organization.
Note(Cloud NAT): Cloud NAT lets your VM instances and container pods communicate with the internet using a shared, public IP address.
A cloud NAT gateway is region and VPC network specific.
If you have VM instances in multiple regions, you’ll need to create a NAT gateway for each region.

Firewall:
https://cloud.google.com/vpc/docs/firewalls
Firewall configuration is associated with a VPC network. 
This means that you cannot share firewall rules among VPC networks, 
including networks connected by VPC Network Peering or by using Cloud VPN tunnels.
Every network has two implied firewall rules (whether they are auto mode or custom mode VPC networks) that permit outgoing connections and block incoming connections.
These rules are not shown in the Cloud Console.
Firewall rules that you create can override these implied rules.
VPC firewall rules are stateful:
When a connection is allowed through the firewall in either direction, return traffic matching this connection is also allowed.
https://cloud.google.com/vpc/docs/firewalls#default_firewall_rules
Internet access is allowed if no other firewall rules deny outbound traffic and if the instance has an external IP address or uses a Cloud NAT instance.
The implied rules cannot be removed.
Deny rules take precedence over allow rules of the same priority.
Implied firewall rules are present in all VPC networks, regardless of how the networks are created, 
and whether they are auto mode or custom mode VPC networks.
The default network has the same implied rules.
https://cloud.google.com/vpc/docs/firewalls#more_rules_default_vpc
The default network is pre-populated with firewall rules that allow incoming connections to instances. (Not present in other networks)
default-allow-internal: Allows ingress connections for all protocols and ports among instances in the network.
default-allow-ssh: Allows ingress connections on TCP destination port 22 from any source to any instance in the network.
default-allow-rdp: Allows ingress connections on TCP destination port 3389 from any source to any instance in the network.
            This rule has a priority of 65534, and it enables connections to instances running the Microsoft Remote Desktop Protocol (RDP).
default-allow-icmp: Allows ingress ICMP traffic from any source to any instance in the network.
            This rule has a priority of 65534, and it enables tools such as ping.
https://cloud.google.com/vpc/docs/firewalls#alwaysallowed
Google Cloud runs a local metadata server alongside each instance at 169.254.169.254.
This server is essential to the operation of the instance, so the instance can access it regardless of any firewall rules that you configure.
https://cloud.google.com/vpc/docs/firewalls#direction_of_the_rule
The direction of a firewall rule can be either ingress or egress.
The direction is always defined from the perspective of the VM that the firewall rule applies to (the target).
Consider an example connection between two VMs in the same network. 
Traffic from VM1 to VM2 can be controlled by using either of these firewall rules:
1. An ingress rule with a target of VM2 and a source of VM1.
2. An egress rule with a target of VM1 and a destination of VM2.
https://cloud.google.com/vpc/docs/firewalls#priority_order_for_firewall_rules
Lower integers indicate higher priorities. If you do not specify a priority when creating a rule, it is assigned a priority of 1000.
https://cloud.google.com/vpc/docs/firewalls#enforcement
You can change whether a firewall rule is enforced by setting its state to enabled or disabled. 
Disabling a rule is useful for troubleshooting or to grant temporary access to instances. 
It's much easier to disable a rule, test, and then re-enable it, than it is to delete and re-create the rule.
https://cloud.google.com/vpc/docs/firewalls#ingress_cases
https://cloud.google.com/vpc/docs/firewalls#egress_cases
https://cloud.google.com/vpc/docs/vpc#app-engine-comm (Communications and access for App Engine)
Since app Engine standard environment instances do not run inside VPC network, VPC firewall rules do not apply to them.
https://cloud.google.com/vpc/docs/vpc#internet_access_reqs (Internet access requirements)
https://cloud.google.com/vpc/docs/firewall-rules-logging
You enable Firewall Rules Logging individually for each firewall rule whose connections you need to log.
Firewall Rules Logging only records TCP and UDP connections.
Changes to firewall rules can be viewed in VPC audit logs.
You cannot enable Firewall Rules Logging for the implied deny ingress and implied allow egress rules.
If the incoming request is allowed by an ingress rule, established responses cannot be blocked by any kind of egress rule.


BigTable:
Bigtable uses apache Hbase. Bigtable is not a relational database. It does not support SQL queries, joins, or multi-row transactions.
https://cloud.google.com/bigtable/docs/overview
You can manage security at the project, instance, and table levels. Bigtable does not support row-level, column-level, or cell-level security restrictions.
https://cloud.google.com/bigtable/docs/quickstart-cbt
https://cloud.google.com/bigtable/docs/replication-overview
https://cloud.google.com/bigtable/docs/creating-instance
For a particular bigtable instance, clusters can be in any region where Bigtable is available, as long as each cluster is in a different zone.
An instance's clusters must each be in unique zones. You can create an additional cluster in any zone where Bigtable is available.
For example, if the first cluster is in us-east1-b, you can choose a different zone in the same region, such as us-east1-c,
or a zone in a separate region, such as europe-west2-a.
https://cloud.google.com/bigtable/docs/instances-clusters-nodes
A table belongs to an instance, not to a cluster or node.
If you have an instance with more than one cluster, you are using replication. This means you can't assign a table to an individual cluster.
You also can't make each cluster store a different set of data in the same table.
A cluster is located in a single zone. An instance's clusters must each be in unique zones.
Behind the scenes, Bigtable splits all of the data in a table into separate tablets.
Tablets are stored on disk, separate from the nodes but in the same zone as the nodes. A tablet is associated with a single node.
https://cloud.google.com/bigtable/docs/access-control#overview
In Bigtable, you cannot grant access to the following types of members:
1. allAuthenticatedUsers
2. allUsers
https://cloud.google.com/bigtable/docs/audit-logging
https://cloud.google.com/bigtable/docs/keyvis-overview
The Key Visualizer tool for Bigtable provides scans that show the usage patterns for each table in a cluster.
Key Visualizer helps you check whether your schema design and usage patterns are causing undesirable results, such as hotspots on specific rows.
Each table has only one index, the row key. There are no secondary indices. Each row key must be unique.
https://cloud.google.com/bigtable/docs/creating-managing-labels#common-uses
https://cloud.google.com/bigtable/docs/modifying-instance
An instance can have up to 4 clusters.
In the Cloud Console, monitoring data is available in the following places:
1. Bigtable monitoring
2. Bigtable instance overview
3. Google Cloud's operations suite Cloud Monitoring
4. Key Visualizer
https://cloud.google.com/bigtable/docs/backups#storage
A table backup is a cluster-level resource.
Even if a table is in an instance with multiple clusters (meaning the cluster is using replication),
a backup is created and stored on only one cluster in that instance.
https://cloud.google.com/bigtable/docs/managing-backups
https://cloud.google.com/bigtable/docs/import-export
https://cloud.google.com/bigtable/docs/managing-tables
A .cbtrc file is a configuration file for google cloud bigtable which contains values like project id, bigtable instance name etc.
https://cloud.google.com/bigtable/docs/managing-tables#multiple-versions
https://cloud.google.com/bigtable/docs/failovers
https://cloud.google.com/bigtable/docs/managing-failovers
If an app profile routes all requests to a single cluster, you can perform a manual failover.
If an app profile uses multi-cluster routing, failovers are automatic and you do not need to take any action.
https://cloud.google.com/bigtable/docs/managing-failovers#automatic
With Bigtable, automatic failovers truly are automatic.
If an app profile uses multi-cluster routing, and the nearest cluster to the application server becomes unhealthy, you don't need to take any action.
Bigtable automatically fails over, even if the cluster is only briefly unhealthy,
and uses the nearest healthy cluster to handle requests until the unhealthy cluster has recovered.


Cloud Logging:
https://cloud.google.com/logging/docs/agent/logging/installation
The VM images for Compute Engine and Amazon Elastic Compute Cloud (EC2) don't include the Logging agent, 
so you must complete these steps to install it on those instances.
If your VMs are running in Google Kubernetes Engine or App Engine, the agent is already included in the VM image.
The Ops Agent collects both metrics and logs by default.
https://cloud.google.com/logging/docs/routing/overview
Depending on the log sink's configuration, every log entry received by Cloud Logging falls into one or more of these categories:
1. Stored in Cloud Logging and not routed elsewhere
2. Stored in Cloud Logging and routed to a supported destination
3. Not stored in Cloud Logging but routed to a supported destination
4. Neither stored in Cloud Logging nor routed elsewhere. These logs are excluded entirely.
You can create sinks at the Google Cloud project level. To set up sinks at the organization or folder levels, use aggregated sinks.
Inclusion filter: Selects which log entries to route through this sink. For inclusion filter examples, see Sample queries.
Exclusion filter: Selects which log entries to explicitly exclude from routing, even if the log entries match the sink's inclusion filter.
				  Note that a sink can contain multiple exclusion filters.
				  If any log entry matches any of the filters, the log entry is excluded from routing.
The sink's exclusion filters override any of its defined inclusion filters.
If a log matches any exclusion filter in the sink, then it doesn't match the sink regardless of any inclusion filters defined.
The log entry isn't routed to that sink's destination.
https://cloud.google.com/logging/docs/routing/overview#buckets
Cloud log buckets are different storage entities than the similarly named Cloud Storage buckets.
Cloud Logging provides every Cloud project with the _Required and _Default log buckets.
You can disable the logs that are routed to the _Default log bucket. You can't change routing rules for the _Required bucket.
https://cloud.google.com/logging/docs/storage#required-bucket
Cloud Logging automatically routes the following types of logs to the _Required bucket:
1. Admin Activity audit logs
2. System Event audit logs
3. Access Transparency logs
Cloud Logging retains the logs in this bucket for 400 days; you can't change this retention period.
You can't modify or delete the _Required bucket. You can't disable the _Required sink, which routes logs to the _Required bucket.
https://cloud.google.com/logging/docs/storage#default-bucket
Any log entry that isn't ingested by the _Required bucket is routed by the _Default sink to the _Default bucket, 
unless you disable or otherwise edit the _Default sink.
Logs held in the _Default bucket are retained for 30 days, unless you configure custom retention for the bucket.
https://cloud.google.com/logging/docs/routing/overview#user-buckets
By applying sinks to your user-defined log buckets, you can route any subset of your logs to any log bucket,
letting you choose which Cloud project your logs are stored in and which other logs are stored with them.
For any log generated in Project-A, you can configure a sink to route that log to user-defined buckets in Project-A or Project-B.
You can configure custom retention for the bucket.
https://cloud.google.com/logging/docs/logs-views
Consider a scenario in which you store all of your organization's logs in a central project.
Because log buckets can contain logs from multiple projects, you might want to control which projects different users can view logs from.
Using custom log views, you can give one user access to logs only from a single project, while you give another user access to logs from all the projects.
https://cloud.google.com/logging/docs/export
https://cloud.google.com/logging/docs/export/configure_export_v2
You can't disable the _Required sink; neither ingestion pricing nor storage pricing applies to the logs data stored in the _Required log bucket.
You can disable the _Default sink to stop logs from being ingested into the _Default bucket. You can also disable any user-defined sinks.
When you create a sink, Logging creates a new service account for the sink, called a unique writer identity.
Your sink destination must permit this service account to write log entries.
Log entries going to log buckets, BigQuery, or Pub/Sub are streamed immediately. 
Log entries going to Cloud Storage are batched and sent out approximately every hour.
https://cloud.google.com/logging/docs/export/aggregated_sinks (Aggregated sink)
To use aggregated sinks, create a sink in a Google Cloud organization or folder and set the sink's includeChildren parameter to True. 
That sink can then route log entries from the organization or folder, plus (recursively) from any contained folders, billing accounts, or projects.
Note: You can create aggregated sinks for Google Cloud folders and organizations.
Because neither Cloud projects nor billing accounts contain child resources, you can't create aggregated sinks for those.
Note: Sinks for folders, billing accounts, and organizations can't be created from Google Cloud Console. 
You can use Google Cloud Console to create project-level (non-aggregated) sinks only.


Cloud Audit logs:
Google Cloud services write audit logs to help you answer the questions, "Who did what, where, and when?" 
Your Cloud projects contain only the audit logs for resources that are directly within the project. 
Other entities, such as folders, organizations, and Cloud Billing accounts, contain the audit logs for the entity itself.
Admin Activity audit logs are always written; you can't configure or disable them and there is no charge.
Data Access audit logs-- except for BigQuery Data Access audit logs-- are disabled by default because audit logs can be quite large.
System Event audit logs are always written; you can't configure or disable them and there is no charge.
Policy Denied audit logs are generated by default and your Cloud project is charged for the logs storage.
https://cloud.google.com/logging/docs/audit
Cloud Logging _Required buckets ingest and store Admin Activity audit logs and System Event audit logs.
You can't configure _Required buckets or any logs data in it.
The _Default buckets, by default, ingest and store any enabled Data Access audit logs as well as Policy Denied audit logs.
To prevent Data Access audit logs from being stored in the _Default buckets, you can disable them.
To prevent any Policy Denied audit logs from being stored in the _Default buckets, you can exclude them by modifying their sinks' filters.
https://www.youtube.com/watch?v=iR8GjOwTOrQ
Data Access audit logs—except for BigQuery—are disabled by default.
https://cloud.google.com/logging/docs/audit/configure-data-access

App Engine:
APP ENGINE STANDARD DOES NOT USE VPC NETWORKS.
APP ENGINE FLEXIBLE USES VPC and run in VM instances.
https://cloud.google.com/appengine/docs/standard
https://cloud.google.com/appengine/docs/flexible
https://cloud.google.com/appengine/docs/the-appengine-environments
https://cloud.google.com/appengine/docs/standard/java11/an-overview-of-app-engine
https://cloud.google.com/appengine/docs/standard/java11/configuration-files
You must deploy the initial version of your app to the default service before you can create and deploy additional services to your app.
https://cloud.google.com/appengine/docs/standard/java11/building-app
https://cloud.google.com/appengine/docs/standard/java11/building-app/deploying-web-service
You can specify the name of your service in the app.yaml file. If the name is omitted, it is treated as default. 
The first service you deploy must be the default service.
You can update your service at any time by running the gcloud app deploy command again. 
Each time you deploy, a new version is created and traffic is automatically routed to the latest version.
https://cloud.google.com/appengine/docs/standard/java11/config/appref
https://cloud.google.com/appengine/docs/standard/java11/reference/dispatch-yaml
An app engine app can have only one dispatch.yaml file.
https://cloud.google.com/appengine/docs/standard/java11/labeling-resources
https://cloud.google.com/appengine/docs/standard/java11/how-requests-are-routed
https://cloud.google.com/appengine/docs/standard/java11/application-security#ingress_controls
 If you use Cloud Load Balancing with your App Engine app, we recommend that you use ingress controls to ensure that your app receives only internal
 and Cloud Load Balancing traffic.
 Otherwise, users can use your app's appspot URL to bypass the load balancer.
https://cloud.google.com/appengine/docs/standard/java11/understanding-firewalls
 For App Engine, the App Engine firewall only applies to incoming traffic routed to your app or service.
https://cloud.google.com/appengine/docs/standard/java11/access-control#auth-cloud-implicit-java
By default, the app's environment contains credentials from the default App Engine service account.
This service account is created by Google when you create an App Engine app and is given full permissions to manage and use all Cloud services in a GCP project.
https://cloud.google.com/appengine/docs/standard/java11/service-account
By default, the App Engine default service account has the Editor role in the project.
https://cloud.google.com/appengine/docs/standard/java11/audit-logging#audit_log_format
App Engine doesn't write Data Access audit logs or System Event audit logs.
Admin Activity audit logs and System Event audit logs are free.
Data Access audit logs and Policy Denied audit logs are chargeable.
https://cloud.google.com/appengine/docs/standard/java11/testing-and-deploying-your-app#other_deployment_options
https://cloud.google.com/appengine/docs/standard/java11/testing-and-deploying-your-app#testing-on-app-engine
Deploy your new version, but prevent traffic from being automatically routed to the new version:
mvn appengine:deploy -Dapp.deploy.projectId=PROJECT_ID -Dapp.deploy.promote=False
https://cloud.google.com/appengine/docs/standard/java11/writing-application-logs
https://cloud.google.com/appengine/docs/standard/java11/storage-options
https://cloud.google.com/appengine/docs/standard/java11/using-cloud-datastore
Datastore mode automatically creates single-property indexes for use with simple types of queries.
For complex queries that include multiple properties, you'll need to configure composite indexes in your app's index.yaml file.
https://cloud.google.com/appengine/docs/standard/java11/using-cloud-storage
App Engine creates a default bucket when you create an app.
This bucket provides the first 5GB of storage for free and includes a free quota for Cloud Storage I/O operations.
https://cloud.google.com/appengine/docs/standard/java11/serving-static-files
To serve static files for Java 11 in the standard environment, you define the handlers in your app.yaml file using either the static_dir or static_files elements.
https://cloud.google.com/appengine/docs/standard/java11/how-instances-are-managed#scaling_types
https://cloud.google.com/appengine/docs/standard/java11/how-instances-are-managed#loading_requests
https://cloud.google.com/appengine/docs/standard/java11/how-instances-are-managed#warmup_requests
https://cloud.google.com/appengine/docs/standard/java11/migrating-traffic
By default, warmup requests are enabled and all traffic is gradually migrated to a version.
You can also choose to disable warmup requests if you want the traffic immediately migrated to a version.
Deploying a new version with the same name as an existing version causes an immediate traffic migration.
Gradual traffic migration traffic between versions running in the flexible environment is not supported.
You must migrate traffic immediately to versions that are running in the flexible environment.
When warmup requests are enabled, traffic is migrated gradually by first sending a warmup request to new instances before those instances receive any user requests.
1)To migrate traffic immediately:
gcloud app services set-traffic [MY_SERVICE] --splits [MY_VERSION]=1
2)To gradually migrate traffic, you include the optional --migrate flag:
gcloud app services set-traffic [MY_SERVICE] --splits [MY_VERSION]=1 --migrate
https://cloud.google.com/appengine/docs/standard/java11/splitting-traffic
When you have specified two or more versions for splitting, you must choose whether to split traffic by using either an IP address or HTTP cookie. 
It's easier to set up an IP address split, but a cookie split is more precise.
https://cloud.google.com/sdk/gcloud/reference/app/services/set-traffic
https://cloud.google.com/appengine/docs/standard/java11/scheduling-jobs-with-cron-yaml
https://cloud.google.com/appengine/docs/standard/java11/connecting-vpc
To use Serverless VPC Access, you must first create a Serverless VPC Access connector to handle communication to your VPC network.
To send requests to your VPC network and receive the corresponding responses "without using the public internet", you must use a Serverless VPC Access connector.
https://cloud.google.com/vpc/docs/vpc#app-engine-comm (App Engine does not reside in VPC)

Cloud IOT:
https://cloud.google.com/iot-core (SEE this for IOT flow in GCP)

Cloud SQL:
https://cloud.google.com/sql/docs/mysql/create-instance
If the primary and secondary zones are specified (for high availability), they must belong to the same region.
https://cloud.google.com/sql/docs/mysql/clone-instance
https://cloud.google.com/sql/docs/mysql/monitor-instance
https://cloud.google.com/sql/docs/mysql/delete-instance
You cannot delete an instance that has any replicas. You must delete all replicas first.
https://cloud.google.com/sql/docs/mysql/connect-overview
https://cloud.google.com/sql/docs/mysql/connect-overview#private_ip
You can use this address to connect from other resources with access to the VPC.
"Optionally", you can require that all connections use either the Cloud SQL proxy or self-managed SSL certificates.
https://cloud.google.com/sql/docs/mysql/connect-overview#public_ip
How to authorize - which connections are authorized and allowed to connect to your Cloud SQL instance:
1. Cloud SQL Auth proxy and Cloud SQL language connectors - these provide access based on IAM.
2. Self-managed SSL/TLS certificates - these only allow connections based on specific public keys.
3. Authorized networks - a list of IP addresses allowed to connect.
In order to help keep your instance secure, 
any connections to a Cloud SQL instance using a public IP "must be" authorized using either the Cloud SQL Auth proxy "or" authorized networks.
Unless using the Cloud SQL Auth proxy, connections to the public IP address of an instance are only allowed if the connection come from from an authorized network.
When your application does not reside in the same VPC connected network and region as your Cloud SQL instance,
use a cloud sql proxy to secure its external connection.
https://cloud.google.com/sql/docs/mysql/connect-overview#cloud_sql_proxy
The Cloud SQL Auth proxy lets you authorize and secure your connections by using Identity and Access Management (IAM) permissions.
The Cloud SQL Auth proxy validates connections using credentials for a user or service account
and wrapping the connection in a SSL/TLS layer that's authorized for a Cloud SQL instance
https://cloud.google.com/sql/docs/mysql/connect-overview#cloud_sql_connector_libraries_for_java_and_python
Cloud SQL offers client libraries that provide encryption and IAM-based authorization when connecting to a Cloud SQL instance by using Java and Python connectors.
They provide the same authentication as the Cloud SQL Auth proxy without requiring an external process.
https://cloud.google.com/sql/docs/mysql/connect-overview#ssl_tls
It's strongly recommended to use self-managed SSL/TLS certificates to provide encryption when not using the Cloud SQL Auth proxy.
Failing to do so means your data is being transmitted insecurely and might be intercepted or inspected by a third party.
https://cloud.google.com/sql/docs/mysql/connect-overview#authorized_networks
Unless using the Cloud SQL Auth proxy, connections to the public IP address of an instance are allowed only if the connection come from an authorized network.
https://cloud.google.com/sql/docs/mysql/configure-ip
https://cloud.google.com/sql/docs/mysql/configure-private-ip
Note: private services access and private google access, both are different.
https://cloud.google.com/sql/docs/mysql/authorize-networks
Even if your instance has only a public IP address, you can connect to it securely by using the Cloud SQL Auth proxy.
All traffic between the Cloud SQL Auth proxy and your Cloud SQL instance is encrypted.
If you don't use the proxy, and you are connecting your client from your own public IP address,
you need to add your client's public address as an authorized network.
Your client application's IP address or address range must be configured as authorized networks for the following conditions:
1. Your client application is connecting directly to a Cloud SQL instance on its public IP address.
2. Your client application is connecting directly to a Cloud SQL instance on its private IP address, and your client's IP address is a non-RFC 1918 address
https://cloud.google.com/sql/docs/mysql/connect-admin-proxy
Using the Cloud SQL Auth proxy is the recommended method for connecting to a Cloud SQL instance. The Cloud SQL Auth proxy:
1)Works with both public and private IP endpoints
2)Validates connections using credentials for a user or service account
3)Wraps the connection in a SSL/TLS layer that's authorized for a Cloud SQL instance
https://cloud.google.com/sql/docs/mysql/connect-admin-proxy#authentication-options
If you are connecting to Cloud SQL from a Compute Engine instance, the Cloud SQL Auth proxy can use the service account associated with the Compute Engine instance.
If the service account has the required permissions for the Cloud SQL instance, the Cloud SQL Auth proxy authenticates successfully.
https://cloud.google.com/sql/docs/mysql/connect-admin-proxy#private-ip
The Cloud SQL Auth proxy uses IP to establish a connection with your Cloud SQL instance.
By default, the Cloud SQL Auth proxy attempts to connect using a public IPv4 address.
If your Cloud SQL instance has only private IP, the Cloud SQL Auth proxy uses the private IP address to connect.
Note: If a cloud sql instance has a private IP, then any resource can connect directly to the sql instance if it is residing in the same vpc network
as the sql instance or is connected to the vpc network of the sql instance. For example, an app engine standard application can connect directly to
the private IP of a sql instance, if the app engine is connected to the vpc network of the sql instance using serverless vpc access connector.
Although cloud sql auth proxy is not mandatory for connection through private IP, but it is recommended.
https://cloud.google.com/sql/docs/mysql/connect-admin-ip
The gcloud sql connect command does not support connecting to a Cloud SQL instance using private IP.
https://cloud.google.com/sql/docs/mysql/connect-app-engine-standard
https://cloud.google.com/sql/docs/mysql/connect-kubernetes-engine
To access a Cloud SQL instance from an application running in Google Kubernetes Engine, you can use either the Cloud SQL Auth proxy (with public or private IP), 
or connect directly using a private IP address.
The Cloud SQL Auth proxy is the recommended way to connect to Cloud SQL, even when using private IP. 
This is because the Cloud SQL Auth proxy provides strong encryption and authentication using IAM, which can help keep your database secure.
When you connect using the Cloud SQL Auth proxy, the Cloud SQL Auth proxy is added to your pod using the sidecar container pattern.
https://cloud.google.com/sql/docs/mysql/configure-ha
https://cloud.google.com/sql/docs/mysql/backup-recovery/backing-up
https://cloud.google.com/sql/docs/mysql/backup-recovery/restoring
https://cloud.google.com/sql/docs/mysql/backup-recovery/pitr
You must have binary logging and backups enabled for the instance,
with continuous binary logs since the last backup before the event you want to recover from.
https://cloud.google.com/sql/docs/mysql/replication/create-replica
Before creating a read replica:
1. Check the status of the primary instance
   gcloud sql instances describe PRIMARY_INSTANCE_NAME
   If the databaseReplicationEnabled property is true, the instance is a replica; you cannot create a replica of a replica.
2. If the enabled property under backupConfiguration is false, enable backups for the primary instance now:
   gcloud sql instances patch PRIMARY_INSTANCE_NAME --backup-start-time >HH:MM
3. If the binaryLogEnabled property is false, enable binary logs on the primary instance:
   	gcloud sql instances patch --enable-bin-log PRIMARY_INSTANCE_NAME
4. Create the read replica:
   gcloud sql instances create REPLICA_NAME --master-instance-name=PRIMARY_INSTANCE_NAME
https://cloud.google.com/sql/docs/mysql/replication/manage-replicas#gcloud
Disabling replication does not stop the replica instance; it becomes a read-only instance that is no longer replicating from its primary instance. 
You continue to be charged for the instance. You can re-enable replication on the disabled replica, delete the replica, or promote the replica to a stand-alone instance. 
You cannot stop the replica.
https://cloud.google.com/sql/docs/mysql/replication/cross-region-replicas
There are two common scenarios for promoting cross-region replicas:
1. Regional migration: Perform a planned migration of a database to a different region.
2. Disaster recovery: Fail over a database to another region in the event that the primary's region becomes unavailable.
Note: Promoting a replica is done manually and intentionally.
It is not the same as high availability,
where a standby instance (which is not a replica) automatically becomes the primary in case of a failure or zonal outage.
Note: A highly available failover replica must belong to the same region as the primary instance.
Whereas a read replica can belong to the same or different region. So in case of a regional outage, read replica (if configured as cross-regional) is
the only option to failover to.
https://cloud.google.com/sql/docs/mysql/replication/cross-region-replicas#recreating_additional_replicas
When you promote a read replica to a standalone sql instance, you can delete the former primary instance(and any other replicas) and recreate the
read replicas from the newly promoted standalone sql instance.
https://cloud.google.com/sql/docs/mysql/import-export/importing
https://cloud.google.com/sql/docs/mysql/import-export/exporting
https://cloud.google.com/sql/docs/mysql/create-manage-databases
https://cloud.google.com/sql/docs/mysql/create-manage-users
Additional material: https://cloud.google.com/sql/docs/mysql/high-availability
Cloud SQL high availability is backed up by Regional persistent disk.
Through synchronous replication to each zone's persistent disk, all writes made to the primary instance are replicated to disks in both zones before a transaction is reported as committed.
In the event of an instance or zone failure, the persistent disk is attached to the standby instance, and it becomes the new primary instance.
Additional material: https://cloud.google.com/sql/docs/mysql/replication
You connect to a read replica directly using its connection name and IP address.


PCA (professional cloud architect) case studies link:
https://services.google.com/fh/files/blogs/master_case_study_ehr_healthcare.pdf
https://services.google.com/fh/files/blogs/master_case_study_helicopter_racing_league.pdf
https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdf
https://services.google.com/fh/files/blogs/master_case_study_terramearth.pdf
See google cloud best practices.

Important link regarding GCP storage solutions scaling : https://googlecourses.qwiklabs.com/course_sessions/443099/video/101607
Important link regarding microservice fault-tolerant methods: https://googlecourses.qwiklabs.com/course_sessions/443099/video/101635
Important link regarding different type of deployments: https://googlecourses.qwiklabs.com/course_sessions/443099/video/101654
Important qwiklabs course for GKE and containers: https://googlecourses.qwiklabs.com/course_templates/32
Important link regarding containers concepts: https://googlecourses.qwiklabs.com/course_sessions/468188/video/101815
                                              https://www.terriblecode.com/blog/how-docker-images-work-union-file-systems-for-dummies/
Important link kubernetes(GKE) blue/green deployment: https://googlecourses.qwiklabs.com/course_sessions/471006/video/101871
Important qwiklabs course for GKE and kubernetes: https://googlecourses.qwiklabs.com/course_templates/34
                                                https://googlecourses.qwiklabs.com/course_templates/33
Important link about kubernetes resource and limits:
https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits