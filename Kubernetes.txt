REVIEW THIS QWIKLABS COURSE FOR AN IN-DEPTH KNOWLEDGE ON KUBERNETES AND GKE:
Architecting with Google Kubernetes Engine:Foundations
https://partner.skills.google/course_templates/32
Architecting with Google Kubernetes Engine:Workloads
https://partner.skills.google/course_templates/34
---------------------------------------------------------------------------------------------------
Difference between GKE and Cloud Run:

Google Kubernetes Engine (GKE) is a managed environment for deploying, managing, and scaling containerized applications using Google infrastructure. GKE is based on Kubernetes, an open-source container orchestration system that automates the deployment, scaling, and management of containerized applications.
"Running a single-container workload on something as large and complex as Kubernetes is probably overkill."
"Better for steady, high traffic."

Cloud Run is a fully managed compute platform that automatically scales your stateless containers. Cloud Run abstracts away all infrastructure management, allowing you to focus solely on writing code. Cloud Run is built on Knative, an open-source project that extends Kubernetes to provide a serverless experience.
Generally speaking, Cloud Run tends to be really useful for synchronous, event-driven applications and microservices.
Use Case: Deploy stateless containers that listen for HTTP requests or events. Better for spiky, unpredictable traffic.
---------------------------------------------------------------------------------------------------
Kubernetes Architecture:
https://partner.skills.google/course_templates/32/video/450908
The key components of Kubernetes architecture are:
1) Master Node (Control Plane): The master node is responsible for managing the Kubernetes cluster. It consists of several components, including:
   - kube-apiserver : The API server is the front-end of the Kubernetes control plane. It exposes the Kubernetes API and serves as the entry point for all administrative tasks. Kubectl communicates with the API server to manage the cluster.
   - etcd: etcd is a distributed key-value store that stores all cluster data, including configuration data, state information, and metadata.
   - Controller Manager: The controller manager is responsible for managing various controllers that ensure the desired state of the cluster is maintained. Controllers tries to move the current cluster state closer to the desired state with the help of control loops and kube-apiserver. Examples of built-in controllers include the deployment controller and job controller.
   - Scheduler: The scheduler is responsible for assigning Pods to nodes based on resource availability and other constraints. Constraints can include things like node affinity, taints, and tolerations. The job of the scheduler to assign Pods to nodes, but it does not actually create the Pods on the nodes. That is the job of the kubelet running on each node.
   - kube-cloud-manager: The kube-cloud-manager is responsible for managing the integration between Kubernetes and the underlying cloud provider. It handles tasks such as load balancing, storage provisioning, and networking.
2) Worker Nodes: Worker nodes are the machines that run the containerized applications. Each worker node contains several components, including:
    - Kubelet: The kubelet is an agent that runs on each worker node and communicates with the master node. It ensures that the containers are running in the desired state. Kubelet uses container runtime to run the containers. GKE uses containerd as the default container runtime.
    - Kube-proxy: The kube-proxy is responsible for managing network communication between Pods and services within the cluster.
    - Container Runtime: The container runtime is responsible for running the containers on the worker nodes. Kubernetes supports several container runtimes, including Docker, containerd, and CRI-O.
--------------------------------------------------------------------------------------------------
Controllers:
https://kubernetes.io/docs/concepts/architecture/controller/
In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.
Kubernetes comes with a set of built-in controllers that run inside the kube-controller-manager. These built-in controllers provide important core behaviors.
The Deployment controller and Job controller are examples of controllers that come as part of Kubernetes itself ("built-in" controllers).
-------------------------------------------------------------------------------------------------
ReplicaSet:
https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/
A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. Usually, you define a Deployment and let that Deployment manage ReplicaSets automatically.
A ReplicaSet ensures that a specified number of pod replicas are running at any given time. However, a Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features.

Check chatgpt "ReplicaSet vs Deployment"
-------------------------------------------------------------------------------------------------
Horizontal Pod Autoscaler:
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
In Kubernetes, a HorizontalPodAutoscaler automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of automatically scaling the workload to match demand.
If the load decreases, and the number of Pods is above the configured minimum, the HorizontalPodAutoscaler instructs the workload resource (the Deployment, StatefulSet, or other similar resource) to scale back down.
The horizontal pod autoscaling controller, running within the Kubernetes control plane, periodically adjusts the desired scale of its target (for example, a Deployment) to match observed metrics such as average CPU utilization, average memory utilization, or any other custom metric you specify.
Kubernetes implements horizontal pod autoscaling as a control loop that runs intermittently (it is not a continuous process).

-------------------------------------------------------------------------------------------------
Horizontal Pod Autoscaler Example:
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/

Create an autoscaler for a deployment called php-apache, with a target CPU utilization of 50 percent, a minimum of 1 pod, and a maximum of 10 pods:
kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10

The yaml definition of the HorizontalPodAutoscaler created would look like this:
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
---
You can check the current status of the newly-made HorizontalPodAutoscaler, by running:
kubectl get hpa
-------------------------------------------------------------------------------------------------
Deployment:
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
Kubernetes deployments are used to manage stateless applications.
While updating a deployment, if the image tag is not changed, Kubernetes will not pull the new image by default. To force Kubernetes to pull the new image, you can set the imagePullPolicy to Always in the deployment manifest.

Rollout a Deployment:
A Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, .spec.template) is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.

Rollback a Deployment:
Sometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping. By default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want (you can change that by modifying revision history limit).
A Deployment's revision is created when a Deployment's rollout is triggered. This means that the new revision is created if and only if the Deployment's Pod template (.spec.template) is changed.
kubectl rollout undo deployment/nginx-deployment
kubectl rollout undo deployment/nginx-deployment --to-revision=2

Rolling update strategy:
By default, a Deployment uses the RollingUpdate strategy to replace old Pods with new ones. This strategy ensures that some Pods are always available during the update process.
You can customize the rolling update behavior by specifying the maxUnavailable and maxSurge parameters in the rollingUpdate section of the Deployment spec.
Advantage: No downtime during the update process.
Disadvantage: Some users may still be accessing the old version of the application while others are accessing the new version.

---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: nginx-deployment
 labels:
   app: nginx
spec:
 replicas: 3
 selector:
   matchLabels:
     app: nginx
 template:
   metadata:
     labels:
       app: nginx
   spec:
     containers:
     - name: nginx
       image: nginx:1.14.2
       ports:
       - containerPort: 80
 strategy:
   type: RollingUpdate
   rollingUpdate:
     maxSurge: 1
     maxUnavailable: 1
---

Recreate strategy:
All existing Pods are killed before new ones are created when .spec.strategy.type==Recreate.
Advantage: All users will gain access to the new version of the application at the same time.
Disadvantage: There will be a downtime during the update process.

Blue-Green Deployment:
Blue-green deployments are a deployment strategy that reduces downtime and risk by running two identical production environments.
Blue environment: the live environment that serves all production traffic.
Green environment: the idle environment where the new version of the application is deployed and tested.
Once the green environment is verified to be working correctly, the traffic is switched from the blue environment to the green environment.
Advantage: Minimal downtime during the switch.
Disadvantage: Requires double the resources during the deployment process.

Canary Deployment:
Canary deployments are a deployment strategy that releases an application to a small subset of users before rolling it out to the entire infrastructure and user base.
Kubernetes does not have built-in support for canary deployments, but you can implement them using a combination of Deployments, Services, and labels.
https://kubernetes.io/docs/concepts/workloads/management/#canary-deployments
Advantage: Allows for testing the new version with a small group of users before a full rollout.
Disadvantage: It is slower and may require additional tooling, such as service mesh, to manage traffic routing.

In a canary deployment, using a single loadbalancer service does not ensure that all requests from a single client will always connect to the same Pod. Each request is treated separately and can connect to either the normal deployment or to the canary deployment. This potential to switch between different versions may cause problems if there are significant changes in functionality in the canary release. To prevent this we can set the sessionAffinity field to ClientIP in the specification of the service if we need a client's first request to determine which Pod will be used for all subsequent connections.

---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  type: LoadBalancer
  sessionAffinity: ClientIP
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 60000
    targetPort: 80
-------------------------------------------------------------------------------------------------
Pod requests and limits:
https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
Requests: Minimum amount of CPU or memory that a container requires. The scheduler uses this information to decide on which node to place the Pod. If you specify a CPU or memory value that is larger than the node's available resources, the Pod will not be scheduled.
Limits: Sets the upper bound on the amount of CPU or memory that a container can use. Prevents a container from consuming too many resources and affecting other containers on the same node.
Kube-scheduler uses requests to schedule pods onto nodes, while the kubelet enforces limits at runtime.
-------------------------------------------------------------------------------------------------
Pod Affinity and Anti-Affinity:
Affinity rules can be used to ensure that two or more pods are placed together on the same node, and anti-affinity rules can be used to spread Pods across different nodes.
Unlike nodeSelectors, which only schedule Pods if they meet requirements, node affinity lets you define rules as preferences instead of requirements.
Affinity and anti-affinity rules can be expressed by using two specific keywords: 'requiredDuringSchedulingIgnoredDuringExecution' and ‘preferredDuringSchedulingIgnoredDuringExecution’.
'RequiredDuringScheduling' enforces a strict requirement that must be met when scheduling Pods, whereas ‘PreferredDuringScheduling’ indicates a flexible preference that isn’t mandatory.
You can set a weight for this preference, where 1 is the weakest and 100 is the strongest.
'IgnoredDuringExecution' indicates that changes to labels will not affect Pods that are already running.

Check chatgpt "Pod affinity example"

Pod Affinity places pods together for performance
Pod Anti-Affinity separates pods for availability
Required = hard constraint
Preferred = best-effort
topologyKey defines the level (node, zone, region)
-------------------------------------------------------------------------------------------------
Services:
https://docs.cloud.google.com/kubernetes-engine/docs/concepts/service
ClusterIP: When you create a Service of type ClusterIP, Kubernetes creates a stable IP address that is accessible from nodes in the cluster. This is the default ServiceType.

NodePort: When you create a Service of type NodePort, Kubernetes allocates a port from a range (default: 30000-32767) on each node to the Service. You can access the Service using <NodeIP>:<NodePort>.
---
apiVersion: v1
kind: Service
metadata:
  name: my-np-service
spec:
  selector:
    app: products
    department: sales
  # Kubernetes allocates a nodePort between 30000 and 32767 for external traffic
  # that reaches the node IP address. Additionally, Kubernetes dynamically
  # allocates a clusterIP IP address for internal traffic.
  type: NodePort
  ports:
  - protocol: TCP
    port: 80 # Route internal traffic that reaches the clusterIP IP address on this port.
    targetPort: 8080
---
For example, suppose the external IP address of one of the cluster nodes is 203.0.113.2. Then the external client calls the Service at 203.0.113.2 on TCP port 32675(node port assigned by kubernetes). The request is forwarded to one of the member Pods on TCP port 8080(using clusterIP). The member Pod must have a container listening on TCP port 8080.
----------------------------------------------------------------------------
LoadBalancer Service in GKE:
https://docs.cloud.google.com/kubernetes-engine/docs/concepts/service-load-balancer
LoadBalancer: When you create a Service of type LoadBalancer, Kubernetes provisions a network load balancer for your Service in the cloud provider. The load balancer routes external traffic to the Service's NodePort and ClusterIP addresses.
In GKE, LoadBalancer Services are implemented by using passthrough Network Load Balancers, which are layer 4 regional load balancers.
When you create a LoadBalancer Service in GKE, you specify whether the load balancer has an internal or external address:
1) External LoadBalancer Services are implemented by using external passthrough Network Load Balancers. Clients located outside your VPC network and Google Cloud VMs with internet access can access an external LoadBalancer Service.
2) Internal LoadBalancer Services are implemented by using internal passthrough Network Load Balancers. Clients located in the same VPC network or in a network connected to the cluster's VPC network can access an internal LoadBalancer Service.
-----------------------------------------------------------------------------------
Passthrough Network Load Balancer:
https://docs.cloud.google.com/load-balancing/docs/passthrough-network-load-balancer
Passthrough Network Load Balancers are Layer 4 regional, passthrough load balancers. These load balancers distribute traffic among backends in the same region as the load balancer. As the name suggests, passthrough Network Load Balancers are not proxies. Load-balanced packets are received by backend VMs with the packet's source and destination IP addresses, protocol, and, if the protocol is port-based, the source and destination ports unchanged. Load-balanced connections are terminated at the backends. Responses from the backend VMs go directly to the clients, not back through the load balancer. The industry term for this is direct server return (DSR).
----------------------------------------------------------------------------
Optional Read: https://docs.cloud.google.com/kubernetes-engine/docs/concepts/explore-gke-networking-docs-use-cases
-----------------------------------------------------------------------------------
Ingress:
https://kubernetes.io/docs/concepts/services-networking/ingress/
Make your HTTP (or HTTPS) network service available using a protocol-aware configuration mechanism, that understands web concepts like URIs, hostnames, paths, and more. The Ingress concept lets you map traffic to different backends based on rules you define via the Kubernetes API.
Ingress may provide load balancing, SSL termination and name-based virtual hosting.
You must have an Ingress controller to satisfy an Ingress. Only creating an Ingress resource has no effect.
An Ingress with no rules sends all traffic to a single default backend and .spec.defaultBackend is the backend that should handle requests in that case.

Ingress in GKE:
https://docs.cloud.google.com/kubernetes-engine/docs/concepts/ingress#multiple_backend_services
Google Kubernetes Engine (GKE) provides a built-in and managed Ingress controller called GKE Ingress. When you create an Ingress resource in GKE, the controller automatically configures an HTTPS load balancer that allows HTTP or HTTPS traffic to reach your Services.
An Ingress object is associated with one or more Service objects, each of which is associated with a set of Pods.
When you create an Ingress object, the GKE Ingress controller creates a Google Cloud HTTP(S) Load Balancer. This internet-facing load balancer is deployed globally across Google's edge network as a managed and scalable pool of load balancing resources.
You must use the kubernetes.io/ingress.class annotation to what type ingress controller to use(for example, "gce" for external application load balancer, "gce-internal" for internal application load balancer).
You can configure the load balancer to route requests to different backend services depending on the URL path. Requests sent to your-store.example could be routed to a backend service that displays full-price items, and requests sent to your-store.example/discounted could be routed to a backend service that displays discounted items.
You can also configure the load balancer to route requests according to the hostname. Requests sent to your-store.example could go to one backend service, and requests sent to your-experimental-store.example could go to another backend service.
If you want to configure GKE Ingress with multiple backends for the same host, you must have a single rule with a single host and multiple paths.
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
 rules:
  - host: your-store.example
    http:
      paths:
      - path: /products
        backend:
          service:
            name: my-products
            port:
              number: 60000
      - path: /discounted-products
        backend:
          service:
            name: my-discounted-products
            port:
              number: 80
---
In the preceding example, assume you have associated the load balancer's IP address with the domain name your-store.example. Requests sent to your-store.example/products are routed to the my-products Service, and requests sent to your-store.example/discounted-products are routed to the my-discounted-products Service.

https://docs.cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer#optional_configuring_a_static_ip_address
You can reserve a static external IP address and configure your Ingress resource to use that static IP address. Reserving a static IP address ensures that the IP address of your load balancer does not change if you delete and recreate the Ingress resource.
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: basic-ingress
  annotations:
    kubernetes.io/ingress.global-static-ip-name: "web-static-ip"
spec:
  defaultBackend:
    service:
      name: web
      port:
        number: 8080
---
Here the annotation kubernetes.io/ingress.global-static-ip-name is set to the name of the reserved static IP address(web-static-ip).

(Optional Read for WebSocket support in GKE Ingress)
https://docs.cloud.google.com/kubernetes-engine/docs/concepts/ingress-xlb#support_for_websocket
----------------------------------------------------------------------------
Application Load Balancer(Http/Https):
https://docs.cloud.google.com/load-balancing/docs/application-load-balancer
The Application Load Balancer is a proxy-based Layer 7 load balancer that lets you run and scale your services. The Application Load Balancer distributes HTTP and HTTPS traffic to backends hosted on a variety of Google Cloud platforms—such as Compute Engine, Google Kubernetes Engine (GKE), Cloud Storage, and Cloud Run.
------------------------------------------------------------------------------------
Difference between L4 and L7 Load Balancer(Taken from Google search AI):

L4 (Transport Layer) load balancers distribute traffic based on IP/Port (TCP/UDP), focusing on speed and efficiency by forwarding packets without deep inspection, ideal for basic, high-volume tasks. L7 (Application Layer) load balancers inspect request content (HTTP headers, URLs, cookies) for intelligent, context-aware routing, offering advanced features like SSL termination, content caching, and security but with more overhead. Think L4 as fast mail delivery (address/port) and L7 as a smart concierge reading your request (URL/cookie) to direct you perfectly.
Layer 4 Load Balancer (L4)

• OSI Layer: Transport Layer (Layer 4).
• Decision Basis: IP addresses, port numbers, and protocols (TCP/UDP).
• How it Works: Forwards packets without opening them, making it fast and simple.
• Pros: High speed, low latency, efficient for simple distribution.
• Cons: No content awareness, limited routing intelligence.
• Use Case: Basic traffic distribution, simple high-performance needs, database sharding.

Layer 7 Load Balancer (L7)

• OSI Layer: Application Layer (Layer 7).
• Decision Basis: Application-level data like URLs, HTTP headers, cookies, user agents, content.
• How it Works: Terminates the connection, reads the message, makes smart decisions, then creates a new connection to the server.
• Pros: Content-based routing, SSL offloading, caching, WAF integration, user persistence via cookies.
• Cons: More complex, higher resource usage, potential for increased latency due to inspection.
• Use Case: Microservices, personalized content delivery, complex web applications.

Key Difference Summary

• Awareness: L4 is network-focused (IP/Port); L7 is content-focused (HTTP/URL).
• Performance: L4 is faster; L7 offers more features but with more processing.
• Functionality: L4 is basic forwarding; L7 provides intelligent routing and security.

AI responses may include mistakes.

https://www.f5.com/glossary/layer-7-load-balancing
https://www.linkedin.com/posts/prachi-agarwal-01_l4-vs-l7-load-balancing-whats-the-difference-activity-7351864440394338304-lf_i
https://freeloadbalancer.com/load-balancing-layer-4-and-layer-7
https://medium.com/@harishramkumar/difference-between-layer-4-vs-layer-7-load-balancing-57464e29ed9f
https://www.a10networks.com/glossary/how-do-layer-4-and-layer-7-load-balancing-differ/
https://www.linkedin.com/pulse/understanding-layer-4-vs-7-why-matters-hosting-michiel-grotenhuis-6whpe
https://www.youtube.com/shorts/Zt4ItDjFibQ
https://www.geeksforgeeks.org/system-design/layer-4-load-balancing-vs-layer-7-load-balancing/

----------------------------------------------------------------------------------------
Container native load balancing:
Without container-native load balancing, load balancer traffic travels to the node instance groups and gets routed using iptables rules to Pods which might or might not be in the same node. With container-native load balancing, load balancer traffic is distributed directly to the Pods which should receive the traffic, eliminating the extra network hop.
For container-native load balancing, GKE clusters should be in VPC-native mode and use alias IPs.
Also use the annotation cloud.google.com/neg: '{"ingress": true}' in the Service manifest to enable container-native load balancing for the Service.
-----------------------------------------------------------------------------------
Istio(Just a read):
https://kubernetes.io/blog/2017/05/managing-microservices-with-istio-service-mesh/

Kubernetes supports a microservices architecture through the Service construct. It allows developers to abstract away the functionality of a set of Pods, and expose it to other developers through a well-defined API. It allows adding a name to this level of abstraction and perform rudimentary L4 load balancing. But it doesn’t help with higher-level problems, such as L7 metrics, traffic splitting, rate limiting, circuit breaking, etc.

Istio, announced last week at GlueCon 2017, addresses these problems in a fundamental way through a service mesh framework. With Istio, developers can implement the core logic for the microservices, and let the framework take care of the rest – traffic management, discovery, service identity and security, and policy enforcement. Better yet, this can be also done for existing microservices without rewriting or recompiling any of their parts. Istio uses Envoy as its runtime proxy component and provides an extensible intermediation layer which allows global cross-cutting policy enforcement and telemetry collection.
