REVIEW THIS QWIKLABS COURSE FOR AN IN-DEPTH KNOWLEDGE ON KUBERNETES AND GKE: https://googlecourses.qwiklabs.com/course_templates/34
--------------------------------------------------------------------------------------------------
Controllers:
https://kubernetes.io/docs/concepts/architecture/controller/
In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.
Kubernetes comes with a set of built-in controllers that run inside the kube-controller-manager. These built-in controllers provide important core behaviors.
The Deployment controller and Job controller are examples of controllers that come as part of Kubernetes itself ("built-in" controllers).
-------------------------------------------------------------------------------------------------
ReplicaSet:
https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/
A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. Usually, you define a Deployment and let that Deployment manage ReplicaSets automatically.
A ReplicaSet ensures that a specified number of pod replicas are running at any given time. However, a Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features.

Check chatgpt "ReplicaSet vs Deployment"
-------------------------------------------------------------------------------------------------
Horizontal Pod Autoscaler:
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
In Kubernetes, a HorizontalPodAutoscaler automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of automatically scaling the workload to match demand.
If the load decreases, and the number of Pods is above the configured minimum, the HorizontalPodAutoscaler instructs the workload resource (the Deployment, StatefulSet, or other similar resource) to scale back down.
The horizontal pod autoscaling controller, running within the Kubernetes control plane, periodically adjusts the desired scale of its target (for example, a Deployment) to match observed metrics such as average CPU utilization, average memory utilization, or any other custom metric you specify.
Kubernetes implements horizontal pod autoscaling as a control loop that runs intermittently (it is not a continuous process).

-------------------------------------------------------------------------------------------------
Horizontal Pod Autoscaler Example:
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/

Create an autoscaler for a deployment called php-apache, with a target CPU utilization of 50 percent, a minimum of 1 pod, and a maximum of 10 pods:
kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10

The yaml definition of the HorizontalPodAutoscaler created would look like this:
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
---
You can check the current status of the newly-made HorizontalPodAutoscaler, by running:
kubectl get hpa
-------------------------------------------------------------------------------------------------
Deployment:
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
Kubernetes deployments are used to manage stateless applications.
While updating a deployment, if the image tag is not changed, Kubernetes will not pull the new image by default. To force Kubernetes to pull the new image, you can set the imagePullPolicy to Always in the deployment manifest.

Rollout a Deployment:
A Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, .spec.template) is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.

Rollback a Deployment:
Sometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping. By default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want (you can change that by modifying revision history limit).
A Deployment's revision is created when a Deployment's rollout is triggered. This means that the new revision is created if and only if the Deployment's Pod template (.spec.template) is changed.
kubectl rollout undo deployment/nginx-deployment
kubectl rollout undo deployment/nginx-deployment --to-revision=2

Rolling update strategy:
By default, a Deployment uses the RollingUpdate strategy to replace old Pods with new ones. This strategy ensures that some Pods are always available during the update process.
You can customize the rolling update behavior by specifying the maxUnavailable and maxSurge parameters in the rollingUpdate section of the Deployment spec.
Advantage: No downtime during the update process.
Disadvantage: Some users may still be accessing the old version of the application while others are accessing the new version.

---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: nginx-deployment
 labels:
   app: nginx
spec:
 replicas: 3
 selector:
   matchLabels:
     app: nginx
 template:
   metadata:
     labels:
       app: nginx
   spec:
     containers:
     - name: nginx
       image: nginx:1.14.2
       ports:
       - containerPort: 80
 strategy:
   type: RollingUpdate
   rollingUpdate:
     maxSurge: 1
     maxUnavailable: 1
---

Recreate strategy:
All existing Pods are killed before new ones are created when .spec.strategy.type==Recreate.
Advantage: All users will gain access to the new version of the application at the same time.
Disadvantage: There will be a downtime during the update process.

Blue-Green Deployment:
Blue-green deployments are a deployment strategy that reduces downtime and risk by running two identical production environments.
Blue environment: the live environment that serves all production traffic.
Green environment: the idle environment where the new version of the application is deployed and tested.
Once the green environment is verified to be working correctly, the traffic is switched from the blue environment to the green environment.
Advantage: Minimal downtime during the switch.
Disadvantage: Requires double the resources during the deployment process.

Canary Deployment:
Canary deployments are a deployment strategy that releases an application to a small subset of users before rolling it out to the entire infrastructure and user base.
Kubernetes does not have built-in support for canary deployments, but you can implement them using a combination of Deployments, Services, and labels.
https://kubernetes.io/docs/concepts/workloads/management/#canary-deployments
Advantage: Allows for testing the new version with a small group of users before a full rollout.
Disadvantage: It is slower and may require additional tooling, such as service mesh, to manage traffic routing.

In a canary deployment, using a single loadbalancer service does not ensure that all requests from a single client will always connect to the same Pod. Each request is treated separately and can connect to either the normal deployment or to the canary deployment. This potential to switch between different versions may cause problems if there are significant changes in functionality in the canary release. To prevent this we can set the sessionAffinity field to ClientIP in the specification of the service if we need a client's first request to determine which Pod will be used for all subsequent connections.

---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  type: LoadBalancer
  sessionAffinity: ClientIP
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 60000
    targetPort: 80
-------------------------------------------------------------------------------------------------
Pod requests and limits:
https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
Requests: Minimum amount of CPU or memory that a container requires. The scheduler uses this information to decide on which node to place the Pod. If you specify a CPU or memory value that is larger than the node's available resources, the Pod will not be scheduled.
Limits: Sets the upper bound on the amount of CPU or memory that a container can use. Prevents a container from consuming too many resources and affecting other containers on the same node.
Kube-scheduler uses requests to schedule pods onto nodes, while the kubelet enforces limits at runtime.
-------------------------------------------------------------------------------------------------
Pod Affinity and Anti-Affinity:
Affinity rules can be used to ensure that two or more pods are placed together on the same node, and anti-affinity rules can be used to spread Pods across different nodes.
Unlike nodeSelectors, which only schedule Pods if they meet requirements, node affinity lets you define rules as preferences instead of requirements.
Affinity and anti-affinity rules can be expressed by using two specific keywords: 'requiredDuringSchedulingIgnoredDuringExecution' and ‘preferredDuringSchedulingIgnoredDuringExecution’.
'RequiredDuringScheduling' enforces a strict requirement that must be met when scheduling Pods, whereas ‘PreferredDuringScheduling’ indicates a flexible preference that isn’t mandatory.
You can set a weight for this preference, where 1 is the weakest and 100 is the strongest.
'IgnoredDuringExecution' indicates that changes to labels will not affect Pods that are already running.

Check chatgpt "Pod affinity example"

Pod Affinity places pods together for performance
Pod Anti-Affinity separates pods for availability
Required = hard constraint
Preferred = best-effort
topologyKey defines the level (node, zone, region)
-------------------------------------------------------------------------------------------------
Services:
https://docs.cloud.google.com/kubernetes-engine/docs/concepts/service
ClusterIP: When you create a Service of type ClusterIP, Kubernetes creates a stable IP address that is accessible from nodes in the cluster. This is the default ServiceType.

NodePort: When you create a Service of type NodePort, Kubernetes allocates a port from a range (default: 30000-32767) on each node to the Service. You can access the Service using <NodeIP>:<NodePort>.
---
apiVersion: v1
kind: Service
metadata:
  name: my-np-service
spec:
  selector:
    app: products
    department: sales
  # Kubernetes allocates a nodePort between 30000 and 32767 for external traffic
  # that reaches the node IP address. Additionally, Kubernetes dynamically
  # allocates a clusterIP IP address for internal traffic.
  type: NodePort
  ports:
  - protocol: TCP
    port: 80 # Route internal traffic that reaches the clusterIP IP address on this port.
    targetPort: 8080
---
For example, suppose the external IP address of one of the cluster nodes is 203.0.113.2. Then the external client calls the Service at 203.0.113.2 on TCP port 32675(node port assigned by kubernetes). The request is forwarded to one of the member Pods on TCP port 8080(using clusterIP). The member Pod must have a container listening on TCP port 8080.
----------------------------------------------------------------------------
LoadBalancer Service in GKE:
https://docs.cloud.google.com/kubernetes-engine/docs/concepts/service-load-balancer
LoadBalancer: When you create a Service of type LoadBalancer, Kubernetes provisions a network load balancer for your Service in the cloud provider. The load balancer routes external traffic to the Service's NodePort and ClusterIP addresses.
When you create a LoadBalancer Service in GKE, you specify whether the load balancer has an internal or external address:
1) External LoadBalancer Services are implemented by using external passthrough Network Load Balancers. Clients located outside your VPC network and Google Cloud VMs with internet access can access an external LoadBalancer Service.
2) Internal LoadBalancer Services are implemented by using internal passthrough Network Load Balancers. Clients located in the same VPC network or in a network connected to the cluster's VPC network can access an internal LoadBalancer Service.
-----------------------------------------------------------------------------------
Optional Read: https://docs.cloud.google.com/kubernetes-engine/docs/concepts/explore-gke-networking-docs-use-cases
-----------------------------------------------------------------------------------
Ingress:
https://kubernetes.io/docs/concepts/services-networking/ingress/
Make your HTTP (or HTTPS) network service available using a protocol-aware configuration mechanism, that understands web concepts like URIs, hostnames, paths, and more. The Ingress concept lets you map traffic to different backends based on rules you define via the Kubernetes API.
Ingress may provide load balancing, SSL termination and name-based virtual hosting.
You must have an Ingress controller to satisfy an Ingress. Only creating an Ingress resource has no effect.
An Ingress with no rules sends all traffic to a single default backend and .spec.defaultBackend is the backend that should handle requests in that case.

Ingress in GKE:
https://docs.cloud.google.com/kubernetes-engine/docs/concepts/ingress#multiple_backend_services
Google Kubernetes Engine (GKE) provides a built-in and managed Ingress controller called GKE Ingress. When you create an Ingress resource in GKE, the controller automatically configures an HTTPS load balancer that allows HTTP or HTTPS traffic to reach your Services.
An Ingress object is associated with one or more Service objects, each of which is associated with a set of Pods.
When you create an Ingress object, the GKE Ingress controller creates a Google Cloud HTTP(S) Load Balancer. This internet-facing load balancer is deployed globally across Google's edge network as a managed and scalable pool of load balancing resources.
You must use the kubernetes.io/ingress.class annotation to what type ingress controller to use(for example, "gce" for external application load balancer, "gce-internal" for internal application load balancer).
You can configure the load balancer to route requests to different backend services depending on the URL path. Requests sent to your-store.example could be routed to a backend service that displays full-price items, and requests sent to your-store.example/discounted could be routed to a backend service that displays discounted items.
You can also configure the load balancer to route requests according to the hostname. Requests sent to your-store.example could go to one backend service, and requests sent to your-experimental-store.example could go to another backend service.
If you want to configure GKE Ingress with multiple backends for the same host, you must have a single rule with a single host and multiple paths.
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
 rules:
  - host: your-store.example
    http:
      paths:
      - path: /products
        backend:
          service:
            name: my-products
            port:
              number: 60000
      - path: /discounted-products
        backend:
          service:
            name: my-discounted-products
            port:
              number: 80
---
In the preceding example, assume you have associated the load balancer's IP address with the domain name your-store.example. Requests sent to your-store.example/products are routed to the my-products Service, and requests sent to your-store.example/discounted-products are routed to the my-discounted-products Service.

https://docs.cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer#optional_configuring_a_static_ip_address
You can reserve a static external IP address and configure your Ingress resource to use that static IP address. Reserving a static IP address ensures that the IP address of your load balancer does not change if you delete and recreate the Ingress resource.
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: basic-ingress
  annotations:
    kubernetes.io/ingress.global-static-ip-name: "web-static-ip"
spec:
  defaultBackend:
    service:
      name: web
      port:
        number: 8080
---
Here the annotation kubernetes.io/ingress.global-static-ip-name is set to the name of the reserved static IP address(web-static-ip).

(Optional Read for WebSocket support in GKE Ingress)
https://docs.cloud.google.com/kubernetes-engine/docs/concepts/ingress-xlb#support_for_websocket
------------------------------------------------------------------------------------
Container native load balancing:
Without container-native load balancing, load balancer traffic travels to the node instance groups and gets routed using iptables rules to Pods which might or might not be in the same node. With container-native load balancing, load balancer traffic is distributed directly to the Pods which should receive the traffic, eliminating the extra network hop.
For container-native load balancing, GKE clusters should be in VPC-native mode and use alias IPs.
Also use the annotation cloud.google.com/neg: '{"ingress": true}' in the Service manifest to enable container-native load balancing for the Service.
