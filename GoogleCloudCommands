Cheat Sheet
https://cloud.google.com/sdk/gcloud/reference/cheat-sheet
-------------------------------------
To enable gcloud interactive mode:
gcloud beta interactive

When using the interactive mode, press TAB to complete file path and resource arguments. If a dropdown menu appears, press TAB to move through the list, and press the spacebar to select your choice.
When using the interactive mode, press TAB to complete file path and resource arguments. If a dropdown menu appears, press TAB to move through the list, and press the spacebar to select your choice.
gcloud compute instances describe <your_vm>

To install a component using gcloud. For example to install the cbt(bigtable) component:
gcloud components update
gcloud components install cbt

To authorize access and perform other common Cloud SDK setup steps:
gcloud init

To prevent the command from automatically opening a web browser:
gcloud init --console-only

To authorize access without performing other setup steps:
gcloud auth login
OR
gcloud auth login --no-launch-browser

To authorize gcloud to access Google Cloud Platform using an existing service account:
gcloud auth activate-service-account test-service-account@google.com  --key-file=/path/key.json --project=testproject

---------------------------------------
Compute engine default config:

To View the list of configurations in your environment:
gcloud config list

To list all the properties:
gcloud config list --all

To list the default zone of compute in the config of your env:
gcloud config list compute/zone

To set your default compute zone to us-central1-a:
gcloud config set compute/zone us-central1-a

To see what your default region and zone settings are:
gcloud config get-value compute/zone
gcloud config get-value compute/region

To list the default configuration(for compute engine) of a GCP project:
gcloud compute project-info describe --project <your_project_ID>

To get all the gcloud commands:
gcloud -h OR gcloud --help

--------------------------------------
Compute Engine VM instance:

To create VM instance:
gcloud compute instances create privatenet-us-vm --zone=us-central1-c --machine-type=f1-micro \
 --subnet=privatesubnet-us --image-family=debian-10 --image-project=debian-cloud \
 --boot-disk-size=10GB --boot-disk-type=pd-standard --boot-disk-device-name=privatenet-us-vm

To list all the VM instances (sorted by zone):
gcloud compute instances list --sort-by=ZONE

SSH to a VM instance:
gcloud compute ssh vm-internal --zone us-central1-c
When you connect to VMs using the gcloud tool, Compute Engine creates a persistent SSH key for you.

SSH to a private VM instance using IAP tunnel:
gcloud compute ssh vm-internal --zone us-central1-c --tunnel-through-iap

Move a VM instance from one zone to another:
gcloud compute instances move example-instance-1  --zone us-central1-b --destination-zone us-central1-f

To create a VM instance boot disk from a snapshot:
gcloud compute instances create VM_NAME \
    --source-snapshot=BOOT_SNAPSHOT_NAME \
    --boot-disk-size=BOOT_DISK_SIZE \
    --boot-disk-type=BOOT_DISK_TYPE \
    --boot-disk-device-name=BOOT_DISK_NAME
To quickly create more than one VM with the same boot disk, create a custom image and, then create VMs from that image rather than from the snapshot.
SEE: https://cloud.google.com/compute/docs/instances/create-start-instance#createsnapshot

To create a preemptible instance:
gcloud compute instances create [INSTANCE_NAME] --preemptible

To create a standalone boot persistent disk from an image:
gcloud compute disks create example-disk --image example-image

To create a boot persistent disk from a snapshot:
gcloud compute disks create example-disk --source-snapshot example-snapshot
It is only possible to apply data from a snapshot when you first create a persistent disk.
You cannot apply a snapshot to an existing persistent disk, or apply a snapshot to persistent disks that belong to a different project than that snapshot.

To detach a boot disk:
gcloud compute instances detach-disk INSTANCE_NAME --disk=DISK_NAME
You can attach or detach a boot disk only from a stopped VM instance.
You can attach only one boot disk per VM instance.

To reattach a boot disk:
gcloud compute instances attach-disk INSTANCE_NAME \
    --disk=DISK_NAME \
    --boot
Any disk can be reattached as a boot disk to an instance as long as the instance does not already have a boot disk attached
and the boot disk is in the same zone as the VM instance.

To update a boot disk for an instance:
You can use only the Cloud Console to update the boot disk for a VM in one step. Specifically, you can detach an existing boot disk and attach a new boot disk as one action.
SEE: https://cloud.google.com/compute/docs/disks/detach-reattach-boot-disk#update_a_boot_disk_for_an_instance

To list all the windows server images:
gcloud compute images list --project windows-cloud --no-standard-images

To determine whether an image supports Shielded VM features, run the following command, and check for UEFI_COMPATIBLE in the output:
gcloud compute images describe [IMAGE_NAME] --project windows-cloud

If you chose an image that supports Shielded VM, you can optionally change the instance's Shielded VM settings using one of the following flags:
--no-shielded-secure-boot: Disable Secure Boot. Secure Boot helps protect your VM instances against boot-level and kernel-level malware and rootkits.
Example:
gcloud compute instances create my-instance \
--image-family windows-2012-r2 --image-project windows-cloud \
--no-shielded-secure-boot

To check if your windows server instance has successfully started and is ready to be used:
gcloud compute instances get-serial-port-output [INSTANCE_NAME]

To create a new account and password for windows server instance:
gcloud compute reset-windows-password [INSTANCE_NAME]

To create a vm instance from a custom machine type:
gcloud compute instances create example-instance --custom-cpu=4 --custom-memory=5

To create a vm instance from an instance template:
gcloud compute instances create [INSTANCE_NAME] --source-instance-template [INSTANCE_TEMPLATE_NAME]

You can create an instance from an existing instance by going to the VM instance details page of the Google Cloud Console.
When you create an instance by clicking Create similar,
the Cloud Console creates an instance with the same configuration as the source instance but does not copy any data stored on the persistent disks or the existing instance.
You can only use this feature in the Cloud Console; this feature is not supported in the gcloud tool or the API.
SEE: https://cloud.google.com/compute/docs/instances/create-vm-from-similar-instance

To create a VM with a custom hostname:
gcloud compute instances create VM_NAME \
       --hostname=HOST_NAME

Sole-tenant nodes are physical servers that run VMs only from a single project.

To add or remove project-wide public SSH keys:
gcloud compute project-info add-metadata --metadata-from-file ssh-keys=[LIST_PATH]

To add or remove instance-level public SSH keys:
gcloud compute instances add-metadata [INSTANCE_NAME] --metadata-from-file ssh-keys=[LIST_PATH]

To set the auto-delete state of a zonal persistent disk:
gcloud compute instances set-disk-auto-delete example-instance \
  [--auto-delete|--no-auto-delete] \
  --disk DISK_NAME

To create target pools:
gcloud compute target-pools create www-pool --region us-central1 --http-health-check basic-check
Target pool-based load balancers can only use legacy HTTP health checks.

To add instances in the target pool:
gcloud compute target-pools add-instances www-pool --instances www1,www2,www3

To create compute instance group from template:
gcloud compute instance-groups managed create lb-backend-group --template=lb-backend-template --size=2 --zone=us-central1-a

To create a non-boot disk (notice there is no image):
gcloud compute disks create [DISK_NAME] \
  --size [DISK_SIZE] \
  --type [DISK_TYPE]

To create a regional persistent disk:
gcloud compute disks create DISK_NAME \
    --size=DISK_SIZE \
    --type=DISK_TYPE \
    --region=REGION \
    --replica-zones=ZONE1,ZONE2

To attach the regional disk to a vm instance:
gcloud compute instances attach-disk INSTANCE_NAME \
    --disk=DISK_NAME \
    --disk-scope=regional

To attach a disk to an instance:
gcloud compute instances attach-disk [INSTANCE_NAME] \
  --disk [DISK_NAME]

To resize a disk:
gcloud compute disks resize [DISK_NAME] --size [DISK_SIZE]
OR (to resize a regional disk)
gcloud compute disks resize [DISK_NAME] --size [DISK_SIZE] --region [REGION]
You can only resize a persistent disk to increase its size. You cannot reduce the size of a persistent disk.
You can resize disks at any time, whether or not the disk is attached to a running VM (even regional disks).
After resizing your disk, you might need to resize its file system and partitions.
1) Boot disk: VMs using public images automatically resize the root partition and file system after you've resized the boot disk on the VM and restarted the VM.
If you are using an image that does not support this functionality, you must manually resize the root partition and file system.
2) Non-boot disk: After resizing the disk, you must extend the file system on the disk to use the added space.

To list snapshots:
gcloud compute snapshots list

To get details of a snapshot:
gcloud compute snapshots describe [SNAPSHOT_NAME]

To creating a snapshot of a zonal persistent disk:
gcloud compute disks snapshot [DISK_NAME] --storage-location [STORAGE_LOCATION]

To creating a snapshot of a regional persistent disk:
gcloud compute disks snapshot [DISK_NAME] --region [REGION] --storage-location [STORAGE_LOCATION]

To create a disk from a snapshot:
gcloud compute disks create DISK_NAME \
    --size=DISK_SIZE \
    --source-snapshot=SNAPSHOT_NAME \
    --type=DISK_TYPE

To create a disk clone:
gcloud compute disks create projects/TARGET_PROJECT_ID/zones/ZONE/disks/TARGET_DISK_NAME \
    --description="cloned disk" \
    --source-disk=projects/SOURCE_PROJECT_ID/zones/ZONE/disks/SOURCE_DISK_NAME

To create a snapshot schedule:
  gcloud compute resource-policies create snapshot-schedule [SCHEDULE_NAME] \
      --description "[SCHEDULE_DESCRIPTION]" \
      --max-retention-days [MAX_RETENTION_DAYS] \
      --start-time [START_TIME] \
      --hourly-schedule [SNAPSHOT_INTERVAL] \
      --daily-schedule \
      --weekly-schedule [SNAPSHOT_INTERVAL] \
      --weekly-schedule-from-file [FILE_NAME] \
      --on-source-disk-delete [DELETION_OPTION]
[DELETION_OPTION] determines what happens to your snapshots if the source disk is deleted.
Choose either the default keep-auto-snapshots by omitting this flag, or use apply-retention-policy to apply a retention policy.

To attach a snapshot schedule to an existing disk:
gcloud compute disks add-resource-policies [DISK_NAME] \
    --resource-policies [SCHEDULE_NAME] \
    --zone [ZONE]

To create a new disk with a snapshot schedule:
gcloud compute disks create [DISK_NAME] \
    --resource-policies [SCHEDULE_NAME] \
    --zone [ZONE]

To view snapshot schedules:
gcloud compute resource-policies list

https://cloud.google.com/compute/docs/disks/scheduled-snapshots#change_snapshot_schedule
To remove a snapshot schedule:
gcloud compute disks remove-resource-policies [DISK_NAME] \
    --resource-policies [SCHEDULE_NAME] \
    --region [REGION] \
    --zone [ZONE]

To delete a snapshot schedule:
gcloud compute resource-policies delete [SCHEDULE_NAME] \
    --region [REGION]

To create an image from a disk:
gcloud compute images create IMAGE_NAME \
    --source-disk=SOURCE_DISK \
    --source-disk-zone=ZONE \
    [--family=IMAGE_FAMILY] \
    [--force]
The --force flag is an optional flag that lets you create the image from a running instance. By default, you cannot create images from running instances.

To create an image from snapshot:
gcloud compute images create IMAGE_NAME \
  --source-snapshot SOURCE_SNAPSHOT

To export images to cloud storage:
gcloud compute images export \
    --destination-uri DESTINATION_URI \
    --image IMAGE_NAME

To pass startup script:
gcloud compute instances create VM_NAME \
  --image-project=debian-cloud \
  --image-family=debian-10 \
  --metadata=startup-script='#! /bin/bash
  apt update
  apt -y install apache2
  cat <<EOF > /var/www/html/index.html
  <html><body><p>Linux startup script added directly.</p></body></html>'

To pass startup script from local file:
gcloud compute instances create VM_NAME \
  --image-project=debian-cloud \
  --image-family=debian-10 \
  --metadata-from-file=startup-script=FILE_PATH

To pass startup script from clous storage:
gcloud compute instances create VM_NAME \
  --image-project=debian-cloud \
  --image-family=debian-10 \
  --scopes=storage-ro \
  --metadata=startup-script-url=CLOUD_STORAGE_URL

To pass startup script to an existing VM:
gcloud compute instances add-metadata [VM_NAME] --metadata startup-script-url=[CLOUD_STORAGE_URL]

To move a VM instance between zones:
gcloud compute instances move example-instance-1 \
    --zone us-central1-b --destination-zone us-central1-f

You can change the machine type of a stopped instance if it is not part of a managed instance group.
gcloud compute instances set-machine-type INSTANCE_NAME \
    --machine-type NEW_MACHINE_TYPE

A virtual machine instance can only have one service account identity.
To change the service account of an VM instance:
gcloud compute instances set-service-account example-instance \
   --service-account my-sa-123@my-project-123.iam.gserviceaccount.com \
   --scopes compute-rw,storage-ro

To set maintenance policy during VM creation:
gcloud compute instances create INSTANCE_NAME \
    --maintenance-policy MAINTENANCE_POLICY \
    [--no-restart-on-failure]
MAINTENANCE_POLICY: the policy for this instance, either TERMINATE or MIGRATE

To update the maintenance policy of an existing VM:
gcloud compute instances set-scheduling INSTANCE_NAME \
    --maintenance-policy MAINTENANCE_POLICY \
    [--no-restart-on-failure | --restart-on-failure]

To list instances of managed instance group:
gcloud compute instance-groups managed list-instances [INSTANCE_GROUP]

To roll out updates in an MIG(via instance template):
gcloud compute instance-groups managed rolling-action start-update INSTANCE_GROUP_NAME \
    --version=template=INSTANCE_TEMPLATE \
    --type=opportunistic|proactive \
    [--zone=ZONE | --region=REGION]
SEE: https://cloud.google.com/compute/docs/instance-groups/updating-migs#setting_up_an_opportunistic_or_proactive_update

To promote an ephemeral external IP address to static external IP address:
gcloud compute addresses create ADDRESS_NAME --addresses=IP_ADDRESS \
  [--region=REGION | --global]
IP_ADDRESS: the IP address you want to promote

To enable autoscaling based on CPU utilization:
gcloud compute instance-groups managed set-autoscaling example-managed-instance-group \
  --max-num-replicas 20 \
  --target-cpu-utilization 0.60 \
  --cool-down-period 90

To enable autoscaling based on load balancing serving capacity:
gcloud compute instance-groups managed set-autoscaling example-managed-instance-group \
    --max-num-replicas 20 \
    --target-load-balancing-utilization 0.6 \
    --cool-down-period 90

-------------------------------------------------------------------------
VPC Network:

To create a VPC network(custom mode):
gcloud compute networks create privatenet --subnet-mode=custom

To create subnet in a VPC network(say the VPC network name is "privatenet"):
gcloud compute networks subnets create privatesubnet-us --network=privatenet --region=us-central1 --range=172.16.0.0/24

To list VPC networks:
gcloud compute networks list

To list the available VPC subnets (sorted by VPC network):
gcloud compute networks subnets list --sort-by=NETWORK

To enable vpc flow logs in a subnet:
gcloud compute networks subnets create SUBNET_NAME \
    --enable-flow-logs \
    [other flags as needed]

To enable vpc flow logs for existing subnets:
gcloud compute networks subnets update SUBNET_NAME \
    --enable-flow-logs \
    [other flags as needed]

To create a firewall rule:
gcloud compute firewall-rules create privatenet-allow-icmp-ssh-rdp --direction=INGRESS --priority=1000 --network=privatenet --action=ALLOW --rules=icmp,tcp:22,tcp:3389 --source-ranges=0.0.0.0/0

To list all the firewall rules (sorted by VPC network):
gcloud compute firewall-rules list --sort-by=NETWORK

To enable firewall logging:
gcloud compute firewall-rules update NAME \
    --enable-logging
    --logging-metadata=LOGGING_METADATA

To disable firewall logging:
gcloud compute firewall-rules update NAME \
    --no-enable-logging

To create a regional static external IP address:
gcloud compute addresses create network-lb-ip-1 --region us-central1

To create a global static external IP address:
gcloud compute addresses create lb-ipv4-1 --ip-version=IPV4 --global

To create firewall rule for health check rule for global http(s) load balancer. This is an ingress rule that allows traffic from the Google Cloud health checking systems (130.211.0.0/22 and 35.191.0.0/16).
gcloud compute firewall-rules create fw-allow-health-check \
    --network=default \
    --action=allow \
    --direction=ingress \
    --source-ranges=130.211.0.0/22,35.191.0.0/16 \
    --target-tags=allow-health-check \
    --rules=tcp:80

To create health check:
gcloud compute health-checks create http http-basic-check --port 80

-------------------------------------------------------------------------
Google Cloud Storage:

To create a bucket:
gsutil mb gs://BUCKET_NAME
OR
gsutil mb -p [PROJECT_ID] -c [STORAGE_CLASS] -l [BUCKET_LOCATION] -b on gs://BUCKET_NAME
WHERE -p: Specify the project with which your bucket will be associated. For example, my-project.
      -c: Specify the default storage class of your bucket. For example, NEARLINE.
      -l: Specify the location of your bucket. For example, US-EAST1.
      -b: Enable uniform bucket-level access for your bucket.

To list items in a bucket:
gsutil ls gs://[YOUR_BUCKET_NAME]

To upload a file in a bucket:
gsutil cp setup.html gs://$BUCKET_NAME_1

To get the acl list for an object in a bucket:
gsutil acl get gs://$BUCKET_NAME_1/setup.html  > acl.txt

To set the access list to private:
gsutil acl set private gs://$BUCKET_NAME_1/setup.html

To update the access list to make the file publicly readable:
gsutil acl ch -u AllUsers:R gs://$BUCKET_NAME_1/setup.html

To make all objects in a bucket readable to everyone on the public internet (For uniform bucket level access enabled bucket):
gsutil iam ch allUsers:objectViewer gs://BUCKET_NAME

To view the current lifecycle policy of a bucket:
gsutil lifecycle get gs://$BUCKET_NAME_1

To set the lifecycle policy of a bucket(using json):
gsutil lifecycle set life.json gs://$BUCKET_NAME_1

To view the current versioning status for the bucket:
gsutil versioning get gs://$BUCKET_NAME_1

To enable versioning of a bucket:
gsutil versioning set on gs://$BUCKET_NAME_1

To copy a file to the bucket with the versioning:
gsutil cp -v setup.html gs://$BUCKET_NAME_1

To list all versions of the file:
gsutil ls -a gs://$BUCKET_NAME_1/setup.html

To download noncurrent version of an object:
gsutil cp gs://[SOURCE_BUCKET_NAME]/[SOURCE_OBJECT_NAME#GENERATION_NUMBER] gs://[DESTINATION_BUCKET_NAME]/[DESTINATION_OBJECT_NAME]
WHERE "GENERATION_NUMBER" is the generation number for the noncurrent version you want to copy. For example,1560468815691234.

To delete noncurrent version of an object:
gsutil rm gs://[BUCKET_NAME]/[OBJECT_NAME#GENERATION_NUMBER]

To sync a local folder with cloud storage bucket:
gsutil rsync -r ./firstlevel gs://$BUCKET_NAME_1/firstlevel
(Where firstlevel is the local folder, whose contents will be copied into the bucket recursively)

To change the storage class of object:
gsutil rewrite -s STORAGE_CLASS gs://PATH_TO_OBJECT

To change the default storage class of a bucket:
gsutil defstorageclass set STORAGE_CLASS gs://BUCKET_NAME

To enable uniform bucket level access(which disables object level ACLs):
gsutil uniformbucketlevelaccess set on gs://BUCKET_NAME

To get uniform bucket level access status:
gsutil uniformbucketlevelaccess get gs://BUCKET_NAME

To disable uniform bucket level access:
gsutil uniformbucketlevelaccess set off gs://BUCKET_NAME

To get the size of a bucket:
gsutil du -s gs://BUCKET_NAME

To get metadata of a bucket:
gsutil ls -L -b gs://BUCKET_NAME

To get metadata of an object:
gsutil stat gs://BUCKET_NAME/OBJECT_NAME

To delete a bucket:
gsutil rm -r gs://BUCKET_NAME

To apply notification configuration in your bucket:
gsutil notification create -t TOPIC_NAME -f json gs://BUCKET_NAME
If you use a TOPIC_NAME that doesn't exist in your project, gsutil creates one for you.

To list notification configuration of a bucket:
gsutil notification list gs://BUCKET_NAME

-------------------------------------------------------------------

Hybrid Connectivity:

Cloud VPN facilitates the connection of GCP resources present in a particular VPC network with other networks (either VPC network or external network),
using a secured tunnel and internal IP addresses.

Dedicated and Partner Interconnect also facilitates the connection of on-premise resource with GCP resources using internal IP addresses.

Direct and Carrier peering facilitates the connection of on-premise resources with GCP resources using Google's edge point of presence (PoP)
using public IP address.

See: https://callcenterstudio.com/google-cloud-connection-blogs/

Shared VPC and VPC peering is a way to establish direct connection between resources residing on seperate VPC networks using internal IP address.
Cloud VPN is another way to achieve the same,
but Shared VPC/VPC peering is more suitable for this type of scenario and comes with low network latency and low cost.

-------------------------------------------------------------------
Understanding the difference between TCP and HTTP:

https://stackoverflow.com/questions/23157817/http-vs-tcp-ip-send-data-to-a-web-server

Understanding forwarding rule in GCP:
https://cloud.google.com/load-balancing/docs/forwarding-rule-concepts

-------------------------------------------------------------------
Deployment Manager:

To create a deployment. The --preview flag gives a preview without actually starting the deployment.
gcloud deployment-manager deployments create dminfra --config=config.yaml --preview

To actually start a deployment which has been created using the command as shown above, use the below command. The update command commits the preview.
gcloud deployment-manager deployments update dminfra

To create and start the deployment at the same time, without previewing it, run the command.
gcloud deployment-manager deployments create dminfra --config=config.yaml

To delete a deployment.
gcloud deployment-manager deployments delete dminfra

-------------------------------------------------------------------
Google Kubernetes Engine:

To create a kubernetes cluster:
gcloud container clusters create [CLUSTER-NAME]

To create multi zonal cluster with autoscaling enabled:
gcloud container clusters create [CLUSTER-NAME] \
  --num-nodes 2 \
  --zone us-central1-a \
  --node-locations us-central1-a,us-central1-b,us-central1-f \
  --enable-autoscaling --min-nodes 1 --max-nodes 4

This will create six nodes across three zones initially, with a minimum of one node per zone and a maximum of four nodes per zone.
In this example, the total size of the cluster can be between three and twelve nodes, spread across the three zones.
If one of the zones fails, the total size of the cluster can be between two and eight nodes.

To create a regional cluster:
gcloud container clusters create CLUSTER_NAME \
    --cluster-version VERSION
    --region COMPUTE_REGION

SEE: https://cloud.google.com/kubernetes-engine/docs/concepts/types-of-clusters
     https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-regional-cluster
A multi-zonal cluster has a single replica of the control plane running in a single zone, and has nodes running in multiple zones.
A regional cluster has multiple replicas of the control plane, running in multiple zones within a given region.

To enable autoscaling for an existing node pool, run the following command:
gcloud container clusters update [cluster-name] --enable-autoscaling \
    --min-nodes 1 --max-nodes 10 --region [compute-region] || --zone [compute-zone] --node-pool default-pool

To disable autoscaling for a specific node pool, run the following command:
gcloud container clusters update [cluster-name] --no-enable-autoscaling \
    --node-pool pool-name [--zone compute-zone --project project-id]

To authenticate the cluster:
gcloud container clusters get-credentials [CLUSTER-NAME]

To create a deployment (deploy a containerized application):
kubectl create deployment hello-server --image=gcr.io/google-samples/hello-app:1.0

To create a deployment using a deployment descriptor (Pod template):
kubectl apply -f [DEPLOYMENT_DESCRIPTOR]

To create a Kubernetes Service, which is a Kubernetes resource that lets you expose your application to external traffic:
kubectl expose deployment hello-server --type=LoadBalancer --port 8080

To get the list of pods (filtered by key-value pair):
kubectl get pods -l [KEY=VALUE]

To get information about a specific Pod:
kubectl describe pod [POD_NAME]

To view a Deployment's manifest, run the following command:
kubectl get deployments [DEPLOYMENT_NAME] -o yaml

To get the list of kubernetes services:
kubectl get service

To update a deployment, with new version of app:
kubectl apply -f [DEPLOYMENT_FILE]
or
kubectl set image deployment nginx nginx=nginx:1.9.1
See: https://cloud.google.com/kubernetes-engine/docs/how-to/stateless-apps#update

To roll back an update:
kubectl rollout undo deployment my-deployment
or to rollback to specific version
kubectl rollout undo deployment my-deployment --to-revision=3

To scale a deployment (other than auto-scaling):
kubectl scale deployment [DEPLOYMENT_NAME] --replicas [NUMBER_OF_REPLICAS]

Tp apply autoscaling for a deployment:
kubectl autoscale deployment my-app --max 6 --min 4 --cpu-percent 50

To delete a cluster:
gcloud container clusters delete [CLUSTER-NAME]

To get details of deployment:
kubectl describe deployments

To disable cloud operations for GKE (cloud monitoring and logging),while creating a cluster:
gcloud container clusters create [CLUSTER_NAME] \
  --zone=[ZONE] \
  --project=[PROJECT_ID] \
  --cluster-version=[CLUSTER_VERSION] \
  --no-enable-stackdriver-kubernetes
Otherwise cloud Operations for GKE is enabled by default.

To enable Cloud Operations for GKE, in an existing cluster:
gcloud container clusters update [CLUSTER_NAME] \
  --zone=[ZONE]  \
  --enable-stackdriver-kubernetes

--------------------------------------------------------------------
How to create managed load-balanced instance group:

1. First, create the instance template:

gcloud compute instance-templates create lb-backend-template \
   --region=us-central1 \
   --network=default \
   --subnet=default \
   --tags=allow-health-check \
   --image-family=debian-9 \
   --image-project=debian-cloud \
   --metadata=startup-script='#! /bin/bash
     apt-get update
     apt-get install apache2 -y
     a2ensite default-ssl
     a2enmod ssl
     vm_hostname="$(curl -H "Metadata-Flavor:Google" \
     http://169.254.169.254/computeMetadata/v1/instance/name)"
     echo "Page served from: $vm_hostname" | \
     tee /var/www/html/index.html
     systemctl restart apache2'

2. Create a managed instance group based on the template:

gcloud compute instance-groups managed create lb-backend-group \
   --template=lb-backend-template --size=2 --zone=us-central1-a
OR to create a regional MIG:
gcloud compute instance-groups managed create example-rmig \
    --template example-template  \
    --size 30 \
    --region us-east1
OR to choose specific zones in a regional MIG:
gcloud compute instance-groups managed create example-rmig \
    --template example-template \
    --size 30 \
    --zones us-east1-b,us-east1-c

3. Create the "fw-allow-health-check" firewall rule.
This is an ingress rule that allows traffic from the Google Cloud health checking systems (130.211.0.0/22 and 35.191.0.0/16).
This lab uses the target tag allow-health-check to identify the VMs.

gcloud compute firewall-rules create fw-allow-health-check \
    --network=default \
    --action=allow \
    --direction=ingress \
    --source-ranges=130.211.0.0/22,35.191.0.0/16 \
    --target-tags=allow-health-check \
    --rules=tcp:80

4. Now that the instances are up and running, set up a global static external IP address that your customers use to reach your load balancer.

gcloud compute addresses create lb-ipv4-1 \
    --ip-version=[IPV4 | IPV6] \
    --global
(OR to create a regional external IP address:
gcloud compute addresses create ADDRESS_NAME  \
    --region=REGION
All regional IP addresses are IPv4)

5. Note the IPv4 address that was reserved:

gcloud compute addresses describe lb-ipv4-1 \
    --format="get(address)" \
    --global

6. Create a healthcheck for the load balancer:

    gcloud compute health-checks create http http-basic-check \
        --port 80

7. Create a backend service:

    gcloud compute backend-services create web-backend-service \
        --protocol=HTTP \
        --port-name=http \
        --health-checks=http-basic-check \
        --global
(OR a health-check can be associated with an instance group:
gcloud compute instance-groups managed update my-mig \
        --health-check example-check \
        --initial-delay 300 \
        --zone us-east1-b)
8. Add your instance group as the backend to the backend service:

    gcloud compute backend-services add-backend web-backend-service \
        --instance-group=lb-backend-group \
        --instance-group-zone=us-central1-a [--instance-group-region=INSTANCE_GROUP_REGION]\  (for zonal or regional MIGs)
        --global

9. Create a URL map to route the incoming requests to the default backend service:

    gcloud compute url-maps create web-map-http \
        --default-service web-backend-service

10. Create a target HTTP proxy to route requests to your URL map:

    gcloud compute target-http-proxies create http-lb-proxy \
        --url-map web-map-http

11. Create a global forwarding rule to route incoming requests to the proxy (Consider the forwarding rule as load balancer):

    gcloud compute forwarding-rules create http-content-rule \
        --address=lb-ipv4-1\
        --global \
        --target-http-proxy=http-lb-proxy \
        --ports=80

-------------------------------------------------------------------------------------------
Cloud SQL:

TO create a mysql instance:
gcloud sql instances create [INSTANCE_NAME] --cpu=[NUMBER_CPUS] --memory=[MEMORY_SIZE] --region=[REGION]
OR
gcloud sql instances create [INSTANCE_NAME] --tier=[API_TIER_STRING] --region=[REGION]
For example, the following string creates an instance with two vCPUs and 7,680 MB of memory:
gcloud sql instances create myinstance --database-version=MYSQL_8_0 --cpu=2 --memory=7680MB --region=us-central1

To set the password for the "root@%" MySQL user:
gcloud sql users set-password root --host=% --instance INSTANCE_NAME --password PASSWORD

To connect to a cloud sql instance:
gcloud sql connect [INSTANCE_NAME]] --user=root

To view summary information about your Cloud SQL instances:
gcloud sql instances describe [INSTANCE_NAME]

To edit an instance:
gcloud sql instances patch [INSTANCE_NAME] --backup-start-time 16:00

To clone an instance:
gcloud sql instances clone [SOURCE_INSTANCE_NAME] [TARGET_INSTANCE_NAME]

To start a stopped instance:
gcloud sql instances patch [INSTANCE_NAME] --activation-policy ALWAYS

Activation policy options are:
ALWAYS - The instance is always up and running.
NEVER- The instance is not restarted.
If you are using a MySQL instance, you generally set your activation policy to ALWAYS to accept connection requests.
If you are not using your instance, you can set its activation policy to NEVER to avoid instance charges.

To stop an instance:
gcloud sql instances patch [INSTANCE_NAME] --activation-policy NEVER

To restart an instance:
gcloud sql instances restart [INSTANCE_NAME]

To delete an instance:
gcloud sql instances delete [INSTANCE_NAME]

To filter instance searches using labels:
gcloud beta sql instances list --filter='labels.billing-code:34802'

To add an IPv4 address (public ip) to the instance:
gcloud sql instances patch [INSTANCE_NAME] --assign-ip

TO show all existing authorized addresses by describing the instance:
gcloud sql instances describe [INSTANCE_NAME]

To update the authorized network list, including all addresses you want included:
gcloud sql instances patch [INSTANCE_NAME] --authorized-networks=[IP_ADDR1],[IP_ADDR2]...

To remove all authorized networks:
gcloud sql instances patch [INSTANCE_NAME] --clear-authorized-networks

To configure an instance to refuse all public IP connections, Clear the authorized address list:
gcloud sql instances patch [INSTANCE_NAME] --clear-authorized-networks

To disable public IP:
gcloud sql instances patch [INSTANCE_NAME] --no-assign-ip

To create sql instance with high availability:
gcloud sql instances create [REGIONAL_INSTANCE_NAME] \
                     --availability-type=REGIONAL \
                     --database-version=[DATABASE_VERSION] \
                     --tier=[MACHINE_TYPE] \
                     --enable-bin-log
You can specify both the primary and secondary zones, using the --zone and --secondary-zone parameters.
The following restrictions apply when the secondary zone is used during instance creation or edit:
1. The zones must be valid zones.
2. If the secondary zone is specified, the primary must also be specified.
3. If the primary and secondary zones are specified, they must be distinct zones.
4. If the primary and secondary zones are specified, they must belong to the same region.

To update an existing sql instance to be highly available:
gcloud sql instances patch [INSTANCE_NAME] --availability-type REGIONAL --enable-bin-log --backup-start-time=[HH:MM]

To initiate a failover:
gcloud sql instances failover [PRIMARY_INSTANCE_NAME]

To disable high availability:
gcloud sql instances patch [INSTANCE_NAME] --availability-type ZONAL

To create an on-demand backup:
gcloud sql backups create --async --instance [INSTANCE_NAME]

To schedule automated backups:
gcloud sql instances patch [INSTANCE_NAME] --backup-start-time [HH:MM]

To view backups of an instance:
gcloud sql backups list --instance [INSTANCE_NAME]

To list the details of one backup, use the ID from the output of the backups list command:
gcloud sql backups describe [BACKUP_ID] --instance [INSTANCE_NAME]

TO set the number of automated backups to retain:
gcloud sql instances patch [instance-name] --retained-backups-count num-to-retain

To disable automated backups:
gcloud sql instances patch [INSTANCE_NAME] --no-backup

To restore the same instance from a backup:
gcloud sql backups restore [BACKUP_ID] --restore-instance=[INSTANCE_NAME]

To enable point-in-time recovery:
gcloud sql instances patch [INSTANCE_NAME] --enable-bin-log

To disable point-in-time recovery:
gcloud sql instances patch [INSTANCE_NAME] --no-enable-bin-log

To create a read replica:
gcloud sql instances create [REPLICA_NAME] --master-instance-name=PRIMARY_INSTANCE_NAME
You can specify a different tier size using the --tier parameter, if needed.
You can specify a different region using the --region parameter.
If the primary instance has a private IP address only, add the --no-assign-ip parameter to the command.
You must create the replica in the same VPC network as the primary instance.

Note: A read replica can be in a different region whereas a failover instance should be in the same region as the primary instance,
but maybe in a different zone.

To promote a replica to a standalone instance:
gcloud sql instances promote-replica [REPLICA_NAME]

To import data from sql dump file (uploaded in cloud storage) into cloud sql:
gcloud sql import sql [INSTANCE_NAME] gs://[BUCKET_NAME]/[IMPORT_FILE_NAME] --database=[DATABASE_NAME]

To import data from csv file (uploaded in cloud storage) into cloud sql:
gcloud sql import csv [INSTANCE_NAME] gs://[BUCKET_NAME]/[FILE_NAME] --database=[DATABASE_NAME] --table=[TABLE_NAME]

To export data from cloud sql instance to a sqldump (stored in cloud storage):
gcloud sql export sql [INSTANCE_NAME] gs://[BUCKET_NAME]/sqldumpfile.gz \
                              --database=[DATABASE_NAME] --offload
Note: Use the --offload flag if you want to use serverless export. Otherwise, remove it from the following command.

To export data from cloud sql instance to a csv file (stored in cloud storage):
gcloud sql export csv [INSTANCE_NAME] gs://[BUCKET_NAME]/[FILE_NAME] \
                            --database=[DATABASE_NAME] \
                            --offload \
                            --query=[SELECT_QUERY]
The query is used to select the data to be exported in csv file.

To create a database in a cloud sql instance:
gcloud sql databases create [DATABASE_NAME] --instance=[INSTANCE_NAME]
[--charset=CHARSET] [--collation=COLLATION]

To list databases:
gcloud sql databases list --instance=[INSTANCE_NAME]

To delete a database:
gcloud sql databases delete [DATABASE_NAME] --instance=[INSTANCE_NAME]

To create a user:
gcloud sql users create [user_name] \
   --host=[HOST] --instance=[INSTANCE_NAME] --password=[PASSWORD]

To change a user password:
gcloud sql users set-password [USER_NAME] \
   --host=[HOST] --instance=[INSTANCE_NAME] --prompt-for-password
-------------------------------------------------------------------------------------------
IAM:

To grant the Viewer role to the user my-user@example.com for the project my-project:
gcloud projects add-iam-policy-binding my-project \
    --member=user:my-user@example.com --role=roles/viewer

To grant role at organization level:
gcloud organizations add-iam-policy-binding organization-id \
    --member='member' \
    --role='roles/compute.storageAdmin'

To quickly revoke a role from a user:
gcloud projects remove-iam-policy-binding my-project \
    --member=user:my-user@example.com --role=roles/viewer

To get the all iam policies of a project:
gcloud projects get-iam-policy my-project --format json > ~/policy.json

To set the iam policies of a project:
gcloud projects set-iam-policy [project-id] [filepath]

To list grantable roles for a resource identified via full resource name (for example a compute engine instance):
gcloud iam list-grantable-roles  //compute.googleapis.com/projects/example-project/zones/us-central1-f/instances/example-instance

-------------------------------------------------------------------------------------------
BigTable:
To create a bigtable instance with a single cluster:
gcloud bigtable instances create INSTANCE_ID \
    --display-name=DISPLAY_NAME \
    [--cluster-config=id=CLUSTER_ID,zone=CLUSTER_ZONE,[nodes=CLUSTER_NUM_NODES,kms-key=KMS_KEY]] \
    [--cluster-storage-type=CLUSTER_STORAGE_TYPE]
INSTANCE_ID: The permanent identifier for the instance.
DISPLAY_NAME: A human-readable name that identifies the instance in the Cloud Console.
CLUSTER_ID: The permanent identifier for the cluster.
CLUSTER_ZONE: The zone where the cluster runs.
The command accepts the following optional flags:
--cluster-config=nodes=CLUSTER_NUM_NODES: The number of nodes in the cluster.
Each cluster in an instance must have 1 or more nodes. The default value is 1.
If you aren't sure how many nodes you need, use the default. You can add more nodes later. Learn more.
--cluster-config=kms-key=KMS_KEY: A CMEK key ID. Use this if you want the instance to be CMEK-protected. You cannot add this later. Learn more.
--cluster-storage-type=CLUSTER_STORAGE_TYPE: The type of storage to use for the cluster.
Each cluster in an instance must use the same storage type. Accepts the values SSD and HDD. The default value is SSD.

To enable replication for a production instance, create another cluster:
gcloud bigtable clusters create CLUSTER_ID \
    --instance=INSTANCE_ID \
    --zone=ZONE \
    [--num-nodes=NUM_NODES] \
    [--kms-key=KMS_KEY] \
    [--project=PROJECT]
See: https://cloud.google.com/bigtable/docs/creating-instance
ZONE: The zone where the cluster runs.
An instance's clusters must each be in unique zones.
You can create an additional cluster in any zone where Bigtable is available.
For example, if the first cluster is in us-east1-b, you can choose a different zone in the same region, such as us-east1-c,
or a zone in a separate region, such as europe-west2-a.

Common uses of labels:
We do not recommend creating large numbers of unique labels, such as for timestamps or individual values for every API call.
Here are some common use cases for labels:
1. Team or cost center labels: Add labels based on team or cost center to distinguish instances owned by different teams (for example, team:research and team:analytics). You can use this type of label for cost accounting or budgeting.
2. Component labels: For example, component:redis, component:frontend, component:ingest, and component:dashboard.
3. Environment or stage labels: For example, environment:production and environment:test.
4. State labels: For example, state:active, state:readytodelete, and state:archive.

To list bigtable instances:
gcloud bigtable instances list
OR
cbt listinstances

To list the clusters in a particular instance:
gcloud bigtable clusters list --instances=INSTANCE_ID
OR
cbt -instance=INSTANCE_ID listclusters

To list tables in an instance:
gcloud bigtable instances tables list --instance=INSTANCE_ID

To create backup from a table:
gcloud bigtable backups create BACKUP_ID --instance=INSTANCE_ID --cluster=CLUSTER_ID --table=TABLE_ID \
--async --expiration-date=EXPIRATION_DATE | --retention-period=RETENTION_PERIOD

To view list of backups:
gcloud bigtable backups list --instance=INSTANCE_ID --cluster=CLUSTER_ID

To restore the backup to a new table:
gcloud bigtable instances tables restore
--source-instance=INSTANCE_ID_SOURCE --source-cluster=CLUSTER_ID
--source=BACKUP_ID --destination-instance=INSTANCE_ID_DESTINATION
--destination=NEW_TABLE_ID --async
Note: You cannot restore from a backup to an existing table. The table has to be a new table.

To update a backup:
gcloud bigtable backups update BACKUP_ID --instance=INSTANCE_ID
--cluster=CLUSTER_ID --expiration-date=EXPIRATION_DATE |
--retention-period=RETENTION_PERIOD

To get metadata about backup:
gcloud bigtable backups describe BACKUP_ID --instance=INSTANCE_ID --cluster=CLUSTER_ID

TO delete a backup:
gcloud bigtable backups delete BACKUP_ID  --instance=INSTANCE_ID --cluster=CLUSTER_ID

To change the number of nodes:
gcloud bigtable clusters update CLUSTER_ID --instance=INSTANCE_ID --num-nodes=NUM_NODES
OR
cbt -instance=INSTANCE_ID updatecluster CLUSTER_ID NUM_NODES

To create a cluster using cbt command:
cbt -instance=INSTANCE_ID createcluster CLUSTER_ID ZONE NUM_NODES STORAGE_TYPE

To delete a cluster:
gcloud bigtable clusters delete CLUSTER_ID --instance=INSTANCE_ID
OR
cbt -instance=INSTANCE_ID deletecluster CLUSTER_ID

To delete an instance:
gcloud bigtable instances delete INSTANCE_ID
OR
cbt deleteinstance INSTANCE_ID

To use the cbt tool, configure a .cbtrc file in your home directory with default project id and bigtable instance id:
echo -e "project = [PROJECT_ID]\ninstance = [INSTANCE_ID]" > ~/.cbtrc
https://cloud.google.com/bigtable/docs/managing-tables#configuring_the_cbt_tool

To create a table:
cbt createtable my-table

To list tables:
cbt ls

To create a column family named cf1:
cbt createfamily my-table cf1

To list information about a table:
cbt ls my-table

To put the value "test-value" in the row r1, using the column family cf1 and the column qualifier c1:
cbt set my-table r1 cf1:c1=test-value

To read the data in a table:
cbt read my-table

To delete the table:
cbt deletetable my-table

To change garbage collection policy
(Retaining multiple versions of each value):
cbt setgcpolicy [TABLE_NAME] [FAMILY_NAME] maxversions=[VERSIONS]
(Setting an expiration time for values):
cbt setgcpolicy [TABLE_NAME] [FAMILY_NAME] maxage=[DAYS]d

-------------------------------------------------------------------------------
LOGGING:

To read your Google Cloud project-level audit log entries:
gcloud logging read "logName : projects/PROJECT_ID/logs/cloudaudit.googleapis.com" --project=PROJECT_ID

To read your folder-level audit log entries:
gcloud logging read "logName : folders/FOLDER_ID/logs/cloudaudit.googleapis.com" --folder=FOLDER_ID

To read your organization-level audit log entries:
gcloud logging read "logName : organizations/ORGANIZATION_ID/logs/cloudaudit.googleapis.com" --organization=ORGANIZATION_ID

To create an aggregated sink for a folder:
gcloud logging sinks create SINK_NAME \
storage.googleapis.com/BUCKET_NAME --include-children \
--folder=FOLDER_ID --log-filter="logName:activity"
To create a sink on the organization level, replace --folder=[FOLDER_ID] with --organization=[ORGANIZATION_ID].
For a billing account, replace with --billing-account=[BILLING_ACCOUNT_ID].

To create a log sink (not aggregated):
To create a sink to a Cloud Logging log bucket
gcloud logging sinks create my-sink logging.googleapis.com/projects/myproject123/locations/global/buckets/my-bucket \
  --log-filter='logName="projects/myproject123/logs/matched"' --description="My first sink"
-------------------------------------------------------------------------------
APP engine:

To select a region and create an App Engine application:
gcloud app create --project=[YOUR_PROJECT_ID]
Each Cloud project can contain only a single App Engine application, and once created you cannot change the location of your App Engine application.

To check if an App Engine application exists in your Cloud project:
gcloud app describe

To retrieve the IDs of your app's services and versions:
gcloud app instances list

To list all services and versions:
gcloud app versions list

To list all versions for a specific service:
gcloud app versions list --service=service1

To list only versions that are receiving traffic:
gcloud app versions list --hide-no-traffic

To display all data about an existing version:
gcloud app versions describe [VERSION_ID] --service=default

To deploy the dispatch configuration file without otherwise altering the currently serving version:
gcloud app deploy dispatch.yaml
SEE: https://cloud.google.com/appengine/docs/standard/java11/how-requests-are-routed

To view the ingress setting for a service:
gcloud app services describe [SERVICE]

To update the ingress setting for a service:
gcloud app services update [SERVICE] --ingress=[INGRESS]
INGRESS: The ingress control you want to apply. One of all, internal-only, or internal-and-cloud-load-balancing.

To migrate traffic immediately:
gcloud app services set-traffic [MY_SERVICE] --splits [MY_VERSION]=1

To migrate traffic gradually:
gcloud app services set-traffic [MY_SERVICE] --splits [MY_VERSION]=1 --migrate

To split traffic between versions of a service:
gcloud app services set-traffic [MY_SERVICE] --splits [MY_VERSION1]=[VERSION1_WEIGHT],[MY_VERSION2]=[VERSION2_WEIGHT] --split-by [IP_OR_COOKIE]
For example:
To send all traffic to 'v2' of service 's1', run:
gcloud app services set-traffic s1 --splits=v2=1
To split traffic evenly between 'v1' and 'v2' of service 's1', run:
gcloud app services set-traffic s1 --splits=v2=.5,v1=.5
-------------------------------------------------------------------------------
BigQuery:

To view information of a particular job:
bq --location=location show -j job_id
1. location is optional. Location is the name of the location where the job runs.
For example, if you are using BigQuery in the Tokyo region, set the flag's value to asia-northeast1.
You can set a default value for the location using the .bigqueryrc file.
2. job_id is the ID of the job.

To list jobs in a project:
bq ls -j -a \
--min_creation_time integer1 \
--max_creation_time integer2 \
-n integer3 \
project_id
1) -j is used to identify jobs as the resource to list.
2) --all or -a lists jobs from all users. To see full (unredacted) details for all jobs, you must have bigquery.jobs.listAll permissions.
3) --min_creation_time is used to list jobs after a supplied timestamp value.
4) --max_creation_time is used to list jobs before a supplied timestamp value.
5) -n limits the results. By default, you are limited to 100,000 results.

To cancel a PENDING or RUNNING job:
bq --location=location cancel job_id
OR
bq --location=location --nosync cancel job_id (--nosync flag to return immediately)

To create a dataset:
bq --location=location mk \
--dataset \
--default_table_expiration [integer1] \
--default_partition_expiration [integer2] \
--description [description] \
[project_id]:[dataset]

For example, the following command creates a dataset named mydataset with data location set to US,
a default table expiration of 3600 seconds (1 hour), and a description of This is my dataset.
Instead of using the --dataset flag, the command uses the -d shortcut. If you omit -d and --dataset, the command defaults to creating a dataset.

bq --location=US mk -d \
--default_table_expiration 3600 \
--description "This is my dataset." \
mydataset

Dataset copying uses features of the BigQuery Data Transfer Service.
To create a dataset copy:
bq mk --transfer_config --project_id=PROJECT_ID --data_source=DATA_SOURCE --target_dataset=DATASET --display_name=NAME --params='PARAMETERS'

Replace the following:
1. PROJECT_ID: your Google Cloud project ID. If --project_id isn't specified, the default project is used.
2. DATA_SOURCE: the data source: cross_region_copy.
3. DATASET: the BigQuery target dataset for the transfer configuration.
4. NAME: the display name for the copy job or transfer configuration.
The transfer name can be any value that lets you easily identify the transfer if you need to modify it later.
5. PARAMETERS: contains the parameters for the created transfer configuration in JSON format.
For example: --params='{"param":"param_value"}'.
For dataset copying, you must supply the source_dataset_id, the source_project_id, and optionally the overwrite_destination_table parameters.

For example, the following command creates a dataset copy configuration named My Transfer with a target dataset named mydataset
and a project with the ID of myproject.
bq mk --transfer_config --project_id=myproject --data_source=cross_region_copy --target_dataset=mydataset --display_name='My Dataset Copy'
--params='{"source_dataset_id":"123_demo_eu","source_project_id":"mysourceproject","overwrite_destination_table":"true"}'

To get the existing dataset information (and output to a JSON file):
bq show --format=prettyjson [project_id]:[dataset] > [path_to_file]
Example:
Enter the following command to write the access controls for mydataset to a JSON file. mydataset is in your default project.
bq show --format=prettyjson mydataset > /tmp/mydataset.json
Enter the following command to write the access controls for mydataset to a JSON file. mydataset is in myotherproject.
bq show --format=prettyjson myotherproject:mydataset > /tmp/mydataset.json

To update a dataset (suppose updating the access controls to the dataset):
bq update --source [path_to_file] [project_id]:[dataset]
Example:
Enter the following command to update the access controls for mydataset. mydataset is in myotherproject.
bq update --source /tmp/mydataset.json myotherproject:mydataset

To list datasets in a project:
bq ls --filter labels.key:value \
--max_results integer \
--format=prettyjson \
--project_id project_id
Examples:
Enter the following command to list datasets in your default project.
bq ls --format=pretty
Enter the following command to list datasets in myotherproject.
bq ls --format=prettyjson --project_id myotherproject
Enter the following command to list all datasets including anonymous datasets in your default project.
bq ls -a
Enter the following command to list datasets in your default project with the label org:dev.
bq ls --filter labels.org:dev

The following example retrieves all columns from the INFORMATION_SCHEMA.SCHEMATA view except for schema_owner which is reserved for future use.
The metadata returned is for all datasets in the default project — myproject.
To run the query against a project other than your default project,
add the project ID to the dataset in the following format: `project_id`.INFORMATION_SCHEMA.view for example, `myproject`.INFORMATION_SCHEMA.SCHEMATA.
bq query --nouse_legacy_sql \
'SELECT
   * EXCEPT(schema_owner)
 FROM
   INFORMATION_SCHEMA.SCHEMATA'
SEE: https://cloud.google.com/bigquery/docs/dataset-metadata#information_schema

To update an existing dataset:
bq update --description "string" [project_id]:[dataset] (to update description)
OR
bq update --default_table_expiration=[integer] [project_id]:[dataset] (to update default table expiration time)
You can set a default table expiration time at the dataset level, or you can set a table's expiration time when the table is created.
If you set the expiration when the table is created, the dataset's default table expiration is ignored.
If you do not set a default table expiration at the dataset level, and you do not set a table expiration when the table is created,
the table never expires and you must delete the table manually.
OR
bq update --default_partition_expiration [integer] [project_id]:[dataset] (to update default partition expiration time)
OR
bq update --source [path_to_file] [project_id]:[dataset] (to update the access controls as mentioned previously)

To delete a dataset:
bq rm -r -d project_id:dataset
In addition, if the dataset contains tables, you must use the -r flag to remove all tables in the dataset.

To create an empty table in an existing dataset with a schema definition:
bq mk \
--table \
--expiration integer \
--description description \
--label key:value, key:value \
project_id:dataset.table \
[schema]
Use the bq mk command with the --table or -t flag. You can supply table schema information inline or via a JSON schema file.

To create a permanent table from a query result:
bq --location=location query \
--destination_table project_id:dataset.table \
--use_legacy_sql=false 'query'

To get information about table:
bq show --format=prettyjson --schema [project_id]:[dataset].[table_id]
Use the --schema flag to display only table schema information.
When you query the INFORMATION_SCHEMA.TABLES view, the query results contain one row for each table or view in a dataset.

To display all information of a table:
bq show --format=prettyjson [project_id]:[dataset].[table_id]

To create an access policy on a table using IAM:
bq get-iam-policy \
 project-id:dataset.table_or_view \
 > policy.json

To update a policy:
bq set-iam-policy \
 project-id:dataset.table_or_view \
 policy.json

To update a table:
bq update --description=[description] [project_id]:[dataset].[table]
OR
bq update --expiration=[integer] [project_id]:[dataset].[table]

To copy a single source table:
bq --location=location cp \
-a -f -n \
project_id:dataset.source_table \
project_id:dataset.destination_table
1) -a or --append_table appends the data from the source table to an existing table in the destination dataset.
2) -f or --force overwrites an existing table in the destination dataset and doesn't prompt you for confirmation.
3) -n or --no_clobber returns the following error message if the table exists in the destination dataset: Table 'project_id:dataset.table' already exists, skipping.
If -n is not specified, the default behavior is to prompt you to choose whether to replace the destination table.

To copy multiple source tables:
bq --location=location cp \
-a -f -n \
project_id:dataset.source_table,project_id:dataset.source_table \
project_id:dataset.destination_table
Note: All source tables must have identical schemas, and only one destination table is allowed.

To delete a table:
bq rm -t -f [project_id]:[dataset].[table]
1) --table or -t should be specified to delete a table.
2) --force or -f to skip confirmation.

To export data from a BigQuery table:
bq --location=location extract \
--destination_format format \
--compression compression_type \
--field_delimiter delimiter \
--print_header=boolean \
project_id:dataset.table \
gs://bucket/filename.ext

To create an empty partitioned table:
The steps to create a partitioned table in BigQuery are similar to creating a standard table, except that you specify the partitioning options,
along with any other table options.
bq mk --table \
  --schema SCHEMA \
  --time_partitioning_field COLUMN \
  --time_partitioning_type UNIT_TIME \
  --time_partitioning_expiration EXPIRATION_TIME \
  --require_partition_filter=BOOLEAN
  PROJECT_ID:DATASET.TABLE

To load data in partitioned table is same as normal table:
[USING QUERY]
bq query \
--use_legacy_sql=false \
--replace \
--destination_table 'mydataset.table1$20160301' \
'SELECT
  column1,
  column2
FROM
  mydataset.mytable'
SEE: https://cloud.google.com/bigquery/docs/managing-partitioned-table-data#append-overwrite

To create a view:
bq mk \
--use_legacy_sql=false \
--view_udf_resource=PATH_TO_FILE \
--expiration INTEGER \
--description "DESCRIPTION" \
--label KEY:VALUE \
--view 'QUERY' \
--project_id PROJECT_ID \
DATASET.VIEW

To list views or tables in a dataset:
bq ls --format=prettyjson [projectid]:[dataset]

To get information about views:
bq show --format=prettyjson [project_id]:[dataset].[view]
When you query the INFORMATION_SCHEMA.VIEWS view, the query results contain one row for each view in a dataset.

To update a view:
bq update --expiration [integer] [project_id]:[dataset].[view] (to update expiration time)

To delete a view:
bq rm -t project_id:dataset.view

To add label to an existing dataset:
bq update --set_label [KEY]:[VALUE] [PROJECT_ID]:[DATASET]

To add label to an existing table/view:
bq update --set_label [KEY]:[VALUE] [PROJECT_ID]:[DATASET].[TABLE_OR_VIEW]

To filter using labels:
bq ls --filter "labels.department:shipping" (to list datasets in your default project that have a department:shipping label)

To remove a label from a dataset:
bq update --clear_label [key] [project_id]:[dataset]

To remove a label from a table or view:
bq update --clear_label [key] [project_id]:[dataset].[table_or_view]

To load data from a local data source:
bq --location=LOCATION load \
--source_format=FORMAT \
PROJECT_ID:DATASET.TABLE \
PATH_TO_SOURCE \
SCHEMA
1) LOCATION: your location. The --location flag is optional.
For example, if you are using BigQuery in the Tokyo region, set the flag's value to asia-northeast1.
You can set a default value for the location by using the .bigqueryrc file.
2) FORMAT: CSV, AVRO, PARQUET, ORC, or NEWLINE_DELIMITED_JSON.
3) project_id: your project ID.
4) dataset: an existing dataset.
5) table: the name of the table into which you're loading data.
6) path_to_source: the path to the local file.
7) schema: a valid schema. The schema can be a local JSON file, or it can be typed inline as part of the command.
You can also use the --autodetect flag instead of supplying a schema definition.

To run batch query:
bq --location=location query \
--batch \
--use_legacy_sql=false \
'query'
Specify the:
1) --destination_table flag to create a permanent table based on the query results.
2) --append_table flag to append the query results to a destination table.
3) --use_legacy_sql=false flag to use standard SQL syntax. You can set a default syntax for the bq command-line tool using the .bigqueryrc file.
4) --label flag to apply a label to the query job in the form key:value. Repeat this flag to specify multiple labels.
5) --max_rows or -n flag to specify the number of rows to return in the query results.
6) --maximum_bytes_billed flag to limit the bytes billed for the query.
If the query goes beyond the limit, it fails (without incurring a charge). If not specified, the bytes billed is set to the project default.

Note: --maximum_bytes_billed flag to limit the bytes billed for the query. If the query goes beyond the limit, it fails (without incurring a charge).
If not specified, the bytes billed is set to the project default.

To dry run a query:
bq query --use_legacy_sql=false --dry_run [QUERY]
